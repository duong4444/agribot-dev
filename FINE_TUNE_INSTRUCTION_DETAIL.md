# üéì H∆∞·ªõng d·∫´n chi ti·∫øt Fine-Tune PhoBERT (Local) - D√†nh cho ng∆∞·ªùi m·ªõi

> **M·ª•c ti√™u**: Hu·∫•n luy·ªán PhoBERT cho Intent Classification v√† NER. Gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc chi ti·∫øt.

---

## üìã T·ªïng quan lu·ªìng x·ª≠ l√Ω

```
C√ÄI ƒê·∫∂T ‚Üí CHU·∫®N B·ªä DATA ‚Üí TRAINING ‚Üí TEST ‚Üí T√çCH H·ª¢P
   ‚Üì            ‚Üì             ‚Üì         ‚Üì        ‚Üì
Python      CSV/JSON     PhoBERT    Script   Service
 venv        labels       epochs    verify   restart
```

---

## 1Ô∏è‚É£ C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng (Windows)

### B∆∞·ªõc 1.1: Ki·ªÉm tra Python

```powershell
# M·ªü PowerShell, ki·ªÉm tra phi√™n b·∫£n
python --version
```
**C·∫ßn:** Python 3.10+ 
**N·∫øu ch∆∞a c√≥:** t·∫£i t·ª´ python.org, tick "Add to PATH" khi c√†i.

### B∆∞·ªõc 1.2: Di chuy·ªÉn v√†o th∆∞ m·ª•c

```powershell
cd C:\Users\ADMIN\Desktop\ex\apps\python-ai-service
```

### B∆∞·ªõc 1.3: T·∫°o virtual environment

```powershell
# T·∫°o venv
python -m venv venv

# K√≠ch ho·∫°t
venv\Scripts\activate
```
**Ki·ªÉm tra:** D√≤ng l·ªánh c√≥ `(venv)` ·ªü ƒë·∫ßu.

### B∆∞·ªõc 1.4: C√†i th∆∞ vi·ªán

```powershell
# C·∫≠p nh·∫≠t pip
python -m pip install --upgrade pip

# C√†i dependencies
pip install transformers datasets torch pandas scikit-learn numpy accelerate evaluate
```
**Th·ªùi gian:** 5-15 ph√∫t. Torch ~2GB.

---

## 2Ô∏è‚É£ Chu·∫©n b·ªã d·ªØ li·ªáu

### B∆∞·ªõc 2.1: T·∫°o th∆∞ m·ª•c

```powershell
mkdir train\data
mkdir train\scripts
mkdir models\intent_classifier
```

### B∆∞·ªõc 2.2: T·∫°o file d·ªØ li·ªáu Intent

**File:** `train/data/intent_data.csv`

```csv
text,label
doanh thu th√°ng n√†y l√† bao nhi√™u,0
chi ph√≠ t∆∞·ªõi ti√™u th√°ng 3,0
l·ª£i nhu·∫≠n t·ª´ c√† chua,0
c√°ch tr·ªìng c√† chua,1
th·ªùi gian thu ho·∫°ch rau,1
gi·ªëng c√¢y n√†o t·ªët,1
b·∫≠t h·ªá th·ªëng t∆∞·ªõi,2
t·∫Øt m√°y b∆°m n∆∞·ªõc,2
ƒëi·ªÅu khi·ªÉn c·∫£m bi·∫øn,2
t∆∞·ªõi n∆∞·ªõc cho rau,3
b√≥n ph√¢n cho c√¢y,3
thu ho·∫°ch s·∫£n ph·∫©m,3
ph√¢n t√≠ch d·ªØ li·ªáu farm,4
th·ªëng k√™ s·∫£n l∆∞·ª£ng,4
b√°o c√°o t√†i ch√≠nh,4
```

**Label mapping:**
- 0 = financial_query
- 1 = crop_query  
- 2 = device_control
- 3 = activity_query
- 4 = analytics_query
- 5 = farm_query
- 6 = sensor_query
- 7 = create_record
- 8 = update_record
- 9 = delete_record

**L∆∞u √Ω:** C·∫ßn √≠t nh·∫•t 10-20 c√¢u/intent. C√†ng nhi·ªÅu c√†ng t·ªët.

---

## 3Ô∏è‚É£ Script Training Intent (chi ti·∫øt)

### File: `train/scripts/train_intent.py`

T·∫°o file n√†y v·ªõi n·ªôi dung ƒë·∫ßy ƒë·ªß c√≥ gi·∫£i th√≠ch:

```python
"""Training Intent Classification - C√≥ gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc"""

import pandas as pd
from datasets import Dataset
from sklearn.model_selection import train_test_split
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import numpy as np

# ============ C·∫§U H√åNH ============
DATA_PATH = "../data/intent_data.csv"
OUTPUT_DIR = "../../models/intent_classifier"
MODEL_NAME = "vinai/phobert-base"
NUM_EPOCHS = 4
BATCH_SIZE = 8  # Gi·∫£m xu·ªëng 4 n·∫øu m√°y y·∫øu

print("üöÄ B·∫Øt ƒë·∫ßu training...")

# ============ LOAD D·ªÆ LI·ªÜU ============
print(f"üìä Load d·ªØ li·ªáu t·ª´ {DATA_PATH}")
df = pd.read_csv(DATA_PATH)
print(f"   T·ªïng: {len(df)} c√¢u, {df['label'].nunique()} intents")

# ============ CHIA TRAIN/VAL ============
print("‚úÇÔ∏è  Chia train (80%) v√† validation (20%)")
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(),
    df['label'].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=df['label']
)
print(f"   Train: {len(train_texts)}, Val: {len(val_texts)}")

# ============ LOAD MODEL ============
print(f"ü§ñ Load {MODEL_NAME}...")
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=df['label'].nunique()
)

# ============ TOKENIZE ============
print("üî§ Tokenize d·ªØ li·ªáu...")
def tokenize_fn(texts):
    return tokenizer(texts, truncation=True, padding='max_length', max_length=256)

train_enc = tokenize_fn(train_texts)
val_enc = tokenize_fn(val_texts)

# ============ T·∫†O DATASET ============
class IntentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels
    
    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item
    
    def __len__(self):
        return len(self.labels)

train_dataset = IntentDataset(train_enc, train_labels)
val_dataset = IntentDataset(val_enc, val_labels)

# ============ C·∫§U H√åNH TRAINING ============
training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=NUM_EPOCHS,
    weight_decay=0.01,
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    accuracy = (predictions == labels).mean()
    return {"accuracy": accuracy}

# ============ TRAINING ============
print(f"\nüèãÔ∏è  Training {NUM_EPOCHS} epochs...")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

trainer.train()

# ============ L∆ØU MODEL ============
print(f"\nüíæ L∆∞u model v√†o {OUTPUT_DIR}")
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

# ============ ƒê√ÅNH GI√Å ============
eval_results = trainer.evaluate()
print(f"\n‚úÖ Ho√†n th√†nh! Accuracy: {eval_results['eval_accuracy']:.2%}")
print(f"üìÅ Model t·∫°i: {OUTPUT_DIR}")
```

---

## 4Ô∏è‚É£ Ch·∫°y Training

### B∆∞·ªõc 4.1: Di chuy·ªÉn v√†o th∆∞ m·ª•c scripts

```powershell
cd train\scripts
```

### B∆∞·ªõc 4.2: Ch·∫°y script

```powershell
python train_intent.py
```

### B∆∞·ªõc 4.3: Theo d√µi qu√° tr√¨nh

**Output m·∫´u:**
```
üöÄ B·∫Øt ƒë·∫ßu training...
üìä Load d·ªØ li·ªáu t·ª´ ../data/intent_data.csv
   T·ªïng: 50 c√¢u, 10 intents
‚úÇÔ∏è  Chia train (80%) v√† validation (20%)
   Train: 40, Val: 10
ü§ñ Load vinai/phobert-base...
Downloading... 100%
üî§ Tokenize d·ªØ li·ªáu...

üèãÔ∏è  Training 4 epochs...
Epoch 1/4: [‚ñà‚ñà‚ñà‚ñà] Loss: 2.12, Acc: 0.45
Epoch 2/4: [‚ñà‚ñà‚ñà‚ñà] Loss: 1.56, Acc: 0.70
Epoch 3/4: [‚ñà‚ñà‚ñà‚ñà] Loss: 0.89, Acc: 0.85
Epoch 4/4: [‚ñà‚ñà‚ñà‚ñà] Loss: 0.45, Acc: 0.95

üíæ L∆∞u model v√†o ../../models/intent_classifier
‚úÖ Ho√†n th√†nh! Accuracy: 95.00%
```

**Th·ªùi gian d·ª± ki·∫øn:**
- CPU: 20-60 ph√∫t (t√πy m√°y)
- GPU: 5-15 ph√∫t

---

## 5Ô∏è‚É£ Ki·ªÉm th·ª≠ Model

### File: `train/scripts/test_intent.py`

```python
"""Test fine-tuned Intent model"""

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

MODEL_PATH = "../../models/intent_classifier"

# Label names
INTENT_LABELS = [
    "financial_query", "crop_query", "device_control",
    "activity_query", "analytics_query", "farm_query",
    "sensor_query", "create_record", "update_record",
    "delete_record"
]

# Load model
print(f"üì¶ Load model t·ª´ {MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)

def predict(text):
    """D·ª± ƒëo√°n intent c·ªßa c√¢u"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=-1)
        idx = torch.argmax(probs).item()
        confidence = probs[0][idx].item()
    return INTENT_LABELS[idx], confidence

# Test cases
test_cases = [
    "Doanh thu th√°ng n√†y l√† bao nhi√™u",
    "C√°ch tr·ªìng c√† chua",
    "B·∫≠t h·ªá th·ªëng t∆∞·ªõi",
    "T∆∞·ªõi n∆∞·ªõc cho rau",
    "Ph√¢n t√≠ch d·ªØ li·ªáu farm"
]

print("\nüß™ Testing model:\n")
for text in test_cases:
    intent, conf = predict(text)
    print(f"'{text}'")
    print(f"  ‚Üí {intent} ({conf:.2%})\n")
```

### Ch·∫°y test:

```powershell
python test_intent.py
```

**Output mong ƒë·ª£i:**
```
üì¶ Load model t·ª´ ../../models/intent_classifier
üß™ Testing model:

'Doanh thu th√°ng n√†y l√† bao nhi√™u'
  ‚Üí financial_query (95.23%)

'C√°ch tr·ªìng c√† chua'
  ‚Üí crop_query (92.45%)

'B·∫≠t h·ªá th·ªëng t∆∞·ªõi'
  ‚Üí device_control (89.67%)
```

---

## 6Ô∏è‚É£ T√≠ch h·ª£p v√†o Service

### B∆∞·ªõc 6.1: Ki·ªÉm tra checkpoint

```powershell
cd ..\..
dir models\intent_classifier
```

**Ph·∫£i th·∫•y:**
- `config.json`
- `pytorch_model.bin`
- `tokenizer.json`
- `vocab.txt`

### B∆∞·ªõc 6.2: Kh·ªüi ƒë·ªông l·∫°i service

```powershell
cd src
python main.py
```

**Log mong ƒë·ª£i:**
```
üì¶ Loading Intent Classifier (PhoBERT)...
Loading fine-tuned model from ./models/intent_classifier...
‚úÖ Intent Classifier loaded successfully
üéâ Python AI Service ready!
```

### B∆∞·ªõc 6.3: Test API

M·ªü PowerShell m·ªõi:

```powershell
curl -X POST http://localhost:8000/intent/classify `
  -H "Content-Type: application/json" `
  -d '{\"text\": \"Doanh thu th√°ng n√†y bao nhi√™u\"}'
```

**Response:**
```json
{
  "intent": "financial_query",
  "confidence": 0.95,
  "all_intents": [
    {"intent": "financial_query", "confidence": 0.95},
    {"intent": "analytics_query", "confidence": 0.03}
  ],
  "processing_time_ms": 45.2
}
```

---

## 7Ô∏è‚É£ X·ª≠ l√Ω l·ªói th∆∞·ªùng g·∫∑p

### L·ªói 1: "CUDA out of memory"

**Nguy√™n nh√¢n:** GPU kh√¥ng ƒë·ªß RAM.

**Gi·∫£i ph√°p:**
1. Gi·∫£m `BATCH_SIZE` xu·ªëng 4 ho·∫∑c 2
2. Ho·∫∑c t·∫Øt GPU, d√πng CPU (th√™m v√†o script):
   ```python
   import os
   os.environ["CUDA_VISIBLE_DEVICES"] = ""
   ```

### L·ªói 2: "ModuleNotFoundError: No module named 'transformers'"

**Nguy√™n nh√¢n:** Ch∆∞a c√†i th∆∞ vi·ªán ho·∫∑c ch·∫°y ngo√†i venv.

**Gi·∫£i ph√°p:**
```powershell
# K√≠ch ho·∫°t venv
venv\Scripts\activate

# C√†i l·∫°i
pip install transformers datasets torch
```

### L·ªói 3: "No such file or directory: intent_data.csv"

**Nguy√™n nh√¢n:** File CSV kh√¥ng ƒë√∫ng v·ªã tr√≠ ho·∫∑c ƒë∆∞·ªùng d·∫´n sai.

**Gi·∫£i ph√°p:**
1. Ki·ªÉm tra file t·ªìn t·∫°i: `dir ..\data\intent_data.csv`
2. Ch·ªânh `DATA_PATH` trong script n·∫øu c·∫ßn

### L·ªói 4: Training qu√° ch·∫≠m

**Nguy√™n nh√¢n:** M√°y y·∫øu ho·∫∑c d·ªØ li·ªáu l·ªõn.

**Gi·∫£i ph√°p:**
1. Gi·∫£m `NUM_EPOCHS` xu·ªëng 2-3
2. Gi·∫£m `BATCH_SIZE` xu·ªëng 4
3. Gi·∫£m `max_length` xu·ªëng 128

### L·ªói 5: Accuracy th·∫•p (<70%)

**Nguy√™n nh√¢n:** D·ªØ li·ªáu √≠t ho·∫∑c kh√¥ng ƒë·ªß ƒëa d·∫°ng.

**Gi·∫£i ph√°p:**
1. Th√™m nhi·ªÅu c√¢u h∆°n (m·ª•c ti√™u 50-100 c√¢u/intent)
2. ƒêa d·∫°ng c√°ch di·ªÖn ƒë·∫°t
3. TƒÉng `NUM_EPOCHS` l√™n 5-6

---

## 8Ô∏è‚É£ Lu·ªìng ho·∫°t ƒë·ªông chi ti·∫øt Training

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TRAINING LOOP (m·ªói epoch)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

1. LOAD BATCH DATA
   ‚Üì
   [Batch: 8 c√¢u] ‚Üí ["doanh thu...", "b·∫≠t m√°y...", ...]
   
2. TOKENIZE
   ‚Üì
   Text ‚Üí Numbers: [101, 5432, 234, ..., 102]
   
3. FORWARD PASS
   ‚Üì
   Input ‚Üí PhoBERT ‚Üí Logits [0.1, 0.8, 0.05, ...]
                              ‚Üì
                         Predictions [1]
   
4. CALCULATE LOSS
   ‚Üì
   Compare: Prediction vs True Label
   Loss = CrossEntropyLoss(pred, label)
   
5. BACKWARD PASS
   ‚Üì
   T√≠nh gradient (ƒë·∫°o h√†m)
   Update tr·ªçng s·ªë model
   
6. REPEAT
   ‚Üì
   Next batch ‚Üí Quay l·∫°i b∆∞·ªõc 1

SAU M·ªñI EPOCH:
   ‚Üì
   Ch·∫°y VALIDATION ƒë·ªÉ ƒë√°nh gi√°
   L∆∞u checkpoint n·∫øu accuracy t·ªët h∆°n
```

---

## 9Ô∏è‚É£ NER Training (N√¢ng cao - T√πy ch·ªçn)

NER ph·ª©c t·∫°p h∆°n Intent. N·∫øu b·∫°n c·∫ßn, tham kh·∫£o:
- Notebook HuggingFace: https://huggingface.co/docs/transformers/tasks/token_classification
- Ho·∫∑c xem `FINE_TUNING_GUIDE.md` ƒë·ªÉ c√≥ script m·∫´u ƒë·∫ßy ƒë·ªß

**L∆∞u √Ω:** ∆Øu ti√™n l√†m Intent tr∆∞·ªõc. Sau khi quen r·ªìi m·ªõi l√†m NER.

---

## ‚úÖ Checklist ho√†n th√†nh

- [ ] C√†i Python 3.10+
- [ ] T·∫°o v√† k√≠ch ho·∫°t virtualenv
- [ ] C√†i transformers, torch, datasets
- [ ] Chu·∫©n b·ªã `intent_data.csv` (√≠t nh·∫•t 50 c√¢u)
- [ ] T·∫°o script `train_intent.py`
- [ ] Ch·∫°y training th√†nh c√¥ng
- [ ] Test v·ªõi `test_intent.py`
- [ ] Checkpoint l∆∞u t·∫°i `models/intent_classifier/`
- [ ] Restart service v√† th·∫•y log "fine-tuned model"
- [ ] Test API tr·∫£ v·ªÅ confidence >0.8

---

## üìö T√†i li·ªáu tham kh·∫£o

- PhoBERT: https://github.com/VinAIResearch/PhoBERT
- Transformers docs: https://huggingface.co/docs/transformers
- PyTorch tutorial: https://pytorch.org/tutorials/

---

**Ch√∫c b·∫°n fine-tune th√†nh c√¥ng! üöÄ**

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ, ki·ªÉm tra l·∫°i t·ª´ng b∆∞·ªõc ho·∫∑c tham kh·∫£o ph·∫ßn x·ª≠ l√Ω l·ªói.
