# ğŸ“ HÆ°á»›ng dáº«n Fine-tuning PhoBERT cho Intent Classification

## ğŸ“‹ Tá»•ng quan

**Fine-tuning** = Tinh chá»‰nh model PhoBERT Ä‘á»ƒ hiá»ƒu rÃµ hÆ¡n vá» lÄ©nh vá»±c nÃ´ng nghiá»‡p.

### **Táº¡i sao cáº§n Fine-tuning?**
- âœ… **Base Model**: Hiá»ƒu tiáº¿ng Viá»‡t chung
- âœ… **Fine-tuned Model**: Hiá»ƒu tiáº¿ng Viá»‡t + NÃ´ng nghiá»‡p
- âœ… **Accuracy**: TÄƒng tá»« 60% â†’ 90%+

## ğŸ¯ **Káº¿t quáº£ mong Ä‘á»£i:**

### **TrÆ°á»›c Fine-tuning:**
```
"doanh thu thÃ¡ng nÃ y" â†’ "device_control" (0.104) âŒ
```

### **Sau Fine-tuning:**
```
"doanh thu thÃ¡ng nÃ y" â†’ "financial_query" (0.95) âœ…
```

## ğŸ› ï¸ **BÆ°á»›c 1: Chuáº©n bá»‹ Data**

### **1.1: Táº¡o file data**
Táº¡o file `intent_data.csv` vá»›i ná»™i dung:

```csv
text,label
"doanh thu thÃ¡ng nÃ y lÃ  bao nhiÃªu",0
"chi phÃ­ tÆ°á»›i tiÃªu thÃ¡ng 3",0
"lá»£i nhuáº­n tá»« cÃ  chua",0
"tá»•ng tiá»n thu Ä‘Æ°á»£c",0
"giÃ¡ trá»‹ sáº£n pháº©m",0
"cÃ¡ch trá»“ng cÃ  chua",1
"thá»i gian thu hoáº¡ch rau",1
"giá»‘ng cÃ¢y nÃ o tá»‘t",1
"ká»¹ thuáº­t trá»“ng lÃºa",1
"chÄƒm sÃ³c cÃ¢y trá»“ng",1
"báº­t há»‡ thá»‘ng tÆ°á»›i",2
"táº¯t mÃ¡y bÆ¡m nÆ°á»›c",2
"Ä‘iá»u khiá»ƒn cáº£m biáº¿n",2
"kiá»ƒm tra thiáº¿t bá»‹",2
"báº­t Ä‘Ã¨n chiáº¿u sÃ¡ng",2
"tÆ°á»›i nÆ°á»›c cho rau",3
"bÃ³n phÃ¢n cho cÃ¢y",3
"thu hoáº¡ch sáº£n pháº©m",3
"chÄƒm sÃ³c cÃ¢y trá»“ng",3
"hoáº¡t Ä‘á»™ng nÃ´ng nghiá»‡p",3
"phÃ¢n tÃ­ch dá»¯ liá»‡u farm",4
"thá»‘ng kÃª sáº£n lÆ°á»£ng",4
"bÃ¡o cÃ¡o tÃ i chÃ­nh",4
"biá»ƒu Ä‘á»“ tÄƒng trÆ°á»Ÿng",4
"analytics nÃ´ng nghiá»‡p",4
"thÃ´ng tin vá» farm",5
"dá»¯ liá»‡u trang tráº¡i",5
"quáº£n lÃ½ nÃ´ng tráº¡i",5
"thÃ´ng tin Ä‘áº¥t Ä‘ai",5
"dá»¯ liá»‡u mÃ´i trÆ°á»ng",5
"báº­t mÃ¡y tÆ°á»›i",6
"táº¯t há»‡ thá»‘ng",6
"Ä‘iá»u khiá»ƒn thiáº¿t bá»‹",6
"kiá»ƒm tra cáº£m biáº¿n",6
"báº­t Ä‘Ã¨n LED",6
"dá»¯ liá»‡u cáº£m biáº¿n",7
"thÃ´ng tin nhiá»‡t Ä‘á»™",7
"Ä‘á»™ áº©m khÃ´ng khÃ­",7
"dá»¯ liá»‡u mÃ´i trÆ°á»ng",7
"thÃ´ng tin thá»i tiáº¿t",7
"táº¡o báº£n ghi má»›i",8
"thÃªm dá»¯ liá»‡u",8
"ghi nháº­n hoáº¡t Ä‘á»™ng",8
"táº¡o report",8
"thÃªm thÃ´ng tin",8
"cáº­p nháº­t dá»¯ liá»‡u",9
"sá»­a thÃ´ng tin",9
"chá»‰nh sá»­a record",9
"update thÃ´ng tin",9
"thay Ä‘á»•i dá»¯ liá»‡u",9
"xÃ³a báº£n ghi",10
"xÃ³a dá»¯ liá»‡u",10
"remove record",10
"xÃ³a thÃ´ng tin",10
"delete data",10
```

### **1.2: Giáº£i thÃ­ch Labels:**
```
0 = financial_query    (doanh thu, chi phÃ­, tiá»n)
1 = crop_query        (trá»“ng cÃ¢y, giá»‘ng, thu hoáº¡ch)
2 = device_control    (báº­t, táº¯t, Ä‘iá»u khiá»ƒn)
3 = activity_query    (tÆ°á»›i, bÃ³n phÃ¢n, chÄƒm sÃ³c)
4 = analytics_query   (phÃ¢n tÃ­ch, thá»‘ng kÃª, bÃ¡o cÃ¡o)
5 = farm_query        (thÃ´ng tin farm, dá»¯ liá»‡u)
6 = device_control    (thiáº¿t bá»‹, mÃ¡y mÃ³c)
7 = sensor_query      (cáº£m biáº¿n, nhiá»‡t Ä‘á»™, Ä‘á»™ áº©m)
8 = create_record     (táº¡o má»›i, thÃªm dá»¯ liá»‡u)
9 = update_record     (cáº­p nháº­t, sá»­a Ä‘á»•i)
10 = delete_record    (xÃ³a, remove)
```

## ğŸš€ **BÆ°á»›c 2: Setup Environment**

### **2.1: CÃ i Ä‘áº·t Google Colab (Khuyáº¿n nghá»‹)**
1. Truy cáº­p: https://colab.research.google.com
2. ÄÄƒng nháº­p báº±ng Google account
3. Táº¡o notebook má»›i

### **2.2: Hoáº·c cÃ i Ä‘áº·t local**
```bash
# CÃ i Ä‘áº·t Python 3.8+
# Download tá»«: https://python.org

# CÃ i Ä‘áº·t dependencies
pip install transformers datasets torch pandas scikit-learn
```

## ğŸ“ **BÆ°á»›c 3: Training Script**

### **3.1: Táº¡o file `train_intent.py`**

```python
# train_intent.py
import pandas as pd
import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments, 
    Trainer
)
from datasets import Dataset
from sklearn.model_selection import train_test_split
import numpy as np

# 1. Load data
print("ğŸ“Š Loading data...")
df = pd.read_csv('intent_data.csv')
print(f"Total examples: {len(df)}")

# 2. Split data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(), 
    df['label'].tolist(), 
    test_size=0.2, 
    random_state=42
)

print(f"Train examples: {len(train_texts)}")
print(f"Validation examples: {len(val_texts)}")

# 3. Load model and tokenizer
print("ğŸ¤– Loading PhoBERT model...")
model_name = "vinai/phobert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=11  # 11 intent classes
)

# 4. Tokenize data
print("ğŸ”¤ Tokenizing data...")
def tokenize_function(examples):
    return tokenizer(
        examples, 
        truncation=True, 
        padding=True, 
        max_length=256
    )

train_encodings = tokenize_function(train_texts)
val_encodings = tokenize_function(val_texts)

# 5. Create datasets
class IntentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = IntentDataset(train_encodings, train_labels)
val_dataset = IntentDataset(val_encodings, val_labels)

# 6. Training arguments
training_args = TrainingArguments(
    output_dir='./models/intent_classifier',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
)

# 7. Create trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
)

# 8. Train model
print("ğŸš€ Starting training...")
trainer.train()

# 9. Save model
print("ğŸ’¾ Saving model...")
trainer.save_model()
tokenizer.save_pretrained('./models/intent_classifier')

print("âœ… Training completed!")
print("Model saved to: ./models/intent_classifier")
```

## ğŸ¯ **BÆ°á»›c 4: Cháº¡y Training**

### **4.1: TrÃªn Google Colab (Khuyáº¿n nghá»‹)**
```python
# 1. Upload file intent_data.csv vÃ o Colab
# 2. Cháº¡y script training
!python train_intent.py
```

### **4.2: TrÃªn Local**
```bash
# 1. Äáº·t file intent_data.csv cÃ¹ng thÆ° má»¥c
# 2. Cháº¡y training
python train_intent.py
```

## â±ï¸ **Thá»i gian Training:**

### **Google Colab (Free GPU):**
- **Data**: 100 examples
- **Time**: 10-15 phÃºt
- **Cost**: Free

### **Local CPU:**
- **Data**: 100 examples
- **Time**: 1-2 giá»
- **Cost**: Free

### **Local GPU:**
- **Data**: 100 examples
- **Time**: 5-10 phÃºt
- **Cost**: Free

## ğŸ“Š **BÆ°á»›c 5: Test Model**

### **5.1: Táº¡o test script**
```python
# test_model.py
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Load fine-tuned model
model_path = "./models/intent_classifier"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# Test function
def test_intent(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(predictions, dim=-1).item()
        confidence = predictions[0][predicted_class].item()
    
    intent_labels = [
        "financial_query", "crop_query", "device_control", 
        "activity_query", "analytics_query", "farm_query",
        "device_control", "sensor_query", "create_record",
        "update_record", "delete_record"
    ]
    
    return intent_labels[predicted_class], confidence

# Test cases
test_cases = [
    "doanh thu thÃ¡ng nÃ y lÃ  bao nhiÃªu",
    "cÃ¡ch trá»“ng cÃ  chua",
    "báº­t há»‡ thá»‘ng tÆ°á»›i",
    "tÆ°á»›i nÆ°á»›c cho rau",
    "phÃ¢n tÃ­ch dá»¯ liá»‡u farm"
]

print("ğŸ§ª Testing fine-tuned model:")
for text in test_cases:
    intent, confidence = test_intent(text)
    print(f"'{text}' â†’ {intent} ({confidence:.3f})")
```

### **5.2: Cháº¡y test**
```bash
python test_model.py
```

## ğŸ‰ **Káº¿t quáº£ mong Ä‘á»£i:**

```
ğŸ§ª Testing fine-tuned model:
'doanh thu thÃ¡ng nÃ y lÃ  bao nhiÃªu' â†’ financial_query (0.95)
'cÃ¡ch trá»“ng cÃ  chua' â†’ crop_query (0.92)
'báº­t há»‡ thá»‘ng tÆ°á»›i' â†’ device_control (0.89)
'tÆ°á»›i nÆ°á»›c cho rau' â†’ activity_query (0.91)
'phÃ¢n tÃ­ch dá»¯ liá»‡u farm' â†’ analytics_query (0.88)
```

## ğŸ”§ **BÆ°á»›c 6: TÃ­ch há»£p vÃ o Python Service**

### **6.1: Update IntentClassifier**
```python
# Trong intent_classifier.py
def __init__(self, model_name: str = "vinai/phobert-base"):
    self.model_name = model_name
    self.fine_tuned_path = "./models/intent_classifier"
    self.use_finetuned = os.path.exists(self.fine_tuned_path)
    
    if self.use_finetuned:
        logger.info("Using fine-tuned model")
    else:
        logger.info("Using base model with rule-based fallback")
```

### **6.2: Load fine-tuned model**
```python
async def load_model(self):
    if self.use_finetuned:
        # Load fine-tuned model
        self.tokenizer = AutoTokenizer.from_pretrained(self.fine_tuned_path)
        self.model = AutoModelForSequenceClassification.from_pretrained(self.fine_tuned_path)
    else:
        # Load base model
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=len(self.INTENT_LABELS)
        )
```

## ğŸ“ˆ **So sÃ¡nh káº¿t quáº£:**

### **Base Model + Rule-based:**
```
"doanh thu thÃ¡ng nÃ y" â†’ financial_query (0.9) âœ…
"cÃ¡ch trá»“ng cÃ  chua" â†’ crop_query (0.9) âœ…
"báº­t há»‡ thá»‘ng tÆ°á»›i" â†’ device_control (0.9) âœ…
```

### **Fine-tuned Model:**
```
"doanh thu thÃ¡ng nÃ y" â†’ financial_query (0.95) âœ…
"cÃ¡ch trá»“ng cÃ  chua" â†’ crop_query (0.92) âœ…
"báº­t há»‡ thá»‘ng tÆ°á»›i" â†’ device_control (0.89) âœ…
```

## ğŸš€ **Quick Start (5 phÃºt):**

### **1. Táº¡o data file:**
```csv
# intent_data.csv
text,label
"doanh thu thÃ¡ng nÃ y",0
"cÃ¡ch trá»“ng cÃ  chua",1
"báº­t há»‡ thá»‘ng tÆ°á»›i",2
```

### **2. Cháº¡y training:**
```bash
python train_intent.py
```

### **3. Test model:**
```bash
python test_model.py
```

### **4. Restart Python service:**
```bash
python src/main.py
```

## ğŸ¯ **TÃ³m táº¯t:**

### **Fine-tuning:**
- **KhÃ³**: Trung bÃ¬nh (cáº§n data + script)
- **Time**: 10-60 phÃºt
- **Cost**: Free (vá»›i Colab)
- **Result**: Accuracy 90%+

### **Rule-based (Hiá»‡n táº¡i):**
- **KhÃ³**: Dá»…
- **Time**: 0 giá»
- **Cost**: Free
- **Result**: Accuracy 80%

**Fine-tuning cho káº¿t quáº£ tá»‘t hÆ¡n nhÆ°ng cáº§n thá»i gian chuáº©n bá»‹!** ğŸ¯

---

**Status**: âœ… Ready to use!
**Setup Time**: ~30 phÃºt
**Data Required**: 100+ examples
**Result**: 90%+ accuracy
