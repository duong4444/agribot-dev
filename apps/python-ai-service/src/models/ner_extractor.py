"""
Named Entity Recognition using PhoBERT
Fine-tuned for Vietnamese Agricultural Domain
"""

import json
import re
import time
from pathlib import Path
from typing import Any, Dict, List, Tuple

import torch
from loguru import logger
# AutoModelForTokenClassification d√πng trong b√†i to√°n g√°n nh√£n cho t·ª´ - ti√™u bi·ªÉu nh·∫•t l√† NER
# m·ªói token trong c√¢u s·∫Ω ƒëc g√°n 1 nh√£n, 
#Token classification: 1 nh√£n cho m·ªói token, intent classification 1 nh√£n cho c·∫£ c√¢u, 
# Intent: 1 Classification Head cho c·∫£ c√¢u.
# NER:    N Classification Heads cho N tokens
from transformers import AutoModelForTokenClassification, AutoTokenizer

# Input text
#    ‚Üì
# Tokenizer (PhoBERT)
#    ‚Üì
# Embedding
#    ‚Üì
# Transformer Encoder (PhoBERT) - hi·ªÉu ng·ªØ c·∫£nh
#    ‚Üì
# Token-wise Classification Head - D·ª± ƒëo√°n nh√£n cho t·ª´ng token
#    ‚Üì
# Softmax (per token) chuy·ªÉn th√†nh x√°c su·∫•t
#    ‚Üì
# NER labels

# | Tag | √ù nghƒ©a |
# | --- | ------- |
# | B   | Begin   |
# | I   | Inside  |
# | O   | Outside |
# | E   | End     |
# | S   | Single  |

class NERExtractor:
    """
    PhoBERT-based NER Extractor for Vietnamese Agricultural IoT Chatbot
    """
    
    # Entity labels (6 types for IoT chatbot)
    # model AI output l√† s·ªë 0,1,2,3,...
    # m·∫£ng n√†y l√† t·ª´ ƒëi·ªÅn ƒë·ªÉ d·ªãch 3 -> "B-CROP"
    ENTITY_LABELS = [
        "O",              # 0 - Outside
        "B-DATE",         # 1 - Begin Date
        "I-DATE",         # 2 - Inside Date
        "B-CROP",         # 3 - Begin Crop Name
        "I-CROP",         # 4 - Inside Crop Name
        "B-AREA",         # 5 - Begin Farm Area
        "I-AREA",         # 6 - Inside Farm Area
        "B-DEVICE",       # 7 - Begin Device Name
        "I-DEVICE",       # 8 - Inside Device Name
        "B-METRIC",       # 9 - Begin Metric
        "I-METRIC",       # 10 - Inside Metric
        "B-DURATION",     # 11 - Begin Duration
        "I-DURATION",     # 12 - Inside Duration
    ]
    
    # Entity type mapping tr·∫£ v·ªÅ cho NESTJS
    ENTITY_TYPE_MAP = {
        "DATE": "date",
        "CROP": "crop_name",
        "AREA": "farm_area",
        "DEVICE": "device_name",
        "METRIC": "metric",
        "DURATION": "duration",
    }
    
    def __init__(self, model_name: str = "vinai/phobert-base"):
        """
        Initialize NER Extractor
        
        Args:
            model_name: HuggingFace model name (default: vinai/phobert-base)
        """
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.entity_labels: List[str] = self.ENTITY_LABELS.copy() #l∆∞u b·∫£n sao
        self.entity_type_map: Dict[str, str] = self.ENTITY_TYPE_MAP.copy() #l∆∞u b·∫£n sao
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        # 10
        logger.info(f"NER Extractor initialized with device: {self.device}")
    
    async def load_model(self):
        """Load PhoBERT model and tokenizer"""
        try:
            # 11
            logger.info(f"Loading tokenizer from {self.model_name}...")
            #load model tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

            model_dir = Path(__file__).resolve().parents[2] / "models" / "ner_extractor"

            # Load label mapping if available
            label_map_path = model_dir / "label_mapping.json"
            if label_map_path.exists():
                try:
                    mapping_data = json.loads(label_map_path.read_text(encoding="utf-8"))
                    label_to_id = mapping_data.get("label_to_id") #obj 
                    if isinstance(label_to_id, dict) and label_to_id:
                        # sx label theo ID
                        # t·∫°i sao c·∫ßn sort
                        # predictions = [0, 2, 8, 1, 7]
                        # entity_labels[0] ‚Üí "O"
                        # Ta c·∫ßn tra c·ª©u:
                        # entity_labels[2] ‚Üí "B-CROP"
                        # entity_labels[8] ‚Üí "I-CROP"
                        # entity_labels[1] ‚Üí "B-AREA"
                        # entity_labels[7] ‚Üí "I-AREA"
                        self.entity_labels = sorted(label_to_id.keys(), key=lambda label: label_to_id[label])
                        logger.info(self.entity_labels)
                        # 12
                        logger.info(f"Loaded NER label mapping with {len(self.entity_labels)} labels")
                        
                        # Update entity type map from loaded labels
                        entity_types = mapping_data.get("entity_types", []) #l√† arr ch·ª©a "DEVICE", "AREA",...
                        logger.info(entity_types)
                        if entity_types:
                            for entity_type in entity_types:
                                self.entity_type_map[entity_type] = entity_type.lower()
                except Exception as mapping_error:
                    logger.warning(f"Unable to read NER label mapping: {mapping_error}. Using default labels")

            num_labels = len(self.entity_labels)

            if model_dir.exists():
                # 13
                logger.info(f"Loading fine-tuned model from {model_dir}...")
                try:
                    self.model = AutoModelForTokenClassification.from_pretrained(
                        model_dir,
                        num_labels=num_labels
                    )
                except Exception as load_error:
                    logger.warning(f"Failed to load fine-tuned NER model: {load_error}. Falling back to base PhoBERT.")
                    self.model = AutoModelForTokenClassification.from_pretrained(
                        self.model_name,
                        num_labels=num_labels
                    )
            else:
                logger.warning("Fine-tuned NER model not found. Falling back to base PhoBERT (rule-based hybrid).")
                self.model = AutoModelForTokenClassification.from_pretrained(
                    self.model_name,
                    num_labels=num_labels
                )
            
            self.model.to(self.device)
            self.model.eval()
            # 14
            logger.info("NER Extractor model loaded successfully")
            
        except Exception as e:
            logger.error(f"Failed to load NER Extractor: {str(e)}")
            raise
    
    async def extract(self, text: str) -> Dict[str, Any]:
        """
        Extract named entities from text
        
        Args:
            text: Input text to extract entities from
            
        Returns:
            Dictionary with entities and processing time
        """
        if self.model is None or self.tokenizer is None:
            raise RuntimeError("Model not loaded. Call load_model() first.")
        
        start_time = time.time()
        
        try:
            # Tokenize input
            try:
                inputs = self.tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=256,
                    padding=True,
                    return_offsets_mapping=True  # L·∫•y v·ªã tr√≠ k√Ω t·ª± c·ªßa m·ªói token
                )
                print(f"1.ner input tokenizer: {input}") # token id , id c·ªßa t·ª´ v·ª±ng trong t·ª´ ƒëi·ªÉn PhoBert
                
                offset_mapping = inputs.pop("offset_mapping")[0]
                print(f"2.ner offset_mapping : {offset_mapping}")
                
            except NotImplementedError:
                # PhoBERT tokenizer kh√¥ng h·ªó tr·ª£ offset mapping
                logger.info(
                    "Tokenizer does not support offset mapping. Using manual token alignment for PhoBERT predictions."
                )

                inputs = self.tokenizer(
                    text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=256,
                    padding=True
                )
                print(f"ner Using manual token alignment_input-tokenizier: {inputs}")
                offset_mapping = None
            
            # Move to device
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            print(f"ner move to device_input-tokenizier: {input}")
            
            # Get predictions
            with torch.no_grad():
                outputs = self.model(**inputs)
                print(f"ner outputs: {outputs}")

                logits = outputs.logits
                print(f"ner logits: {logits}")

                predictions = torch.argmax(logits, dim=-1)[0] #ch·ªçn nh√£n c√≥ ƒëi·ªÉm cao nh·∫•t
                print(f"ner predictions: {predictions}")
                print(f"ner predictions para1-both: {predictions.cpu().numpy()}") #label id
                print(f"ner predictions para2-both: {inputs['input_ids'][0].cpu().numpy()}") #token id

            entities: List[Dict[str, Any]] = []
            if offset_mapping is not None:
                print(f"c√≥ offset_mapping: {predictions}")

                entities = self._convert_predictions_to_entities(
                    text,
                    predictions.cpu().numpy(),
                    offset_mapping.numpy(),
                    inputs["input_ids"][0].cpu().numpy()
                )
            else:
                # PhoBERT doesn't support offset mapping
                print(f"KH√îNG C√ì offset_mapping: {predictions}")
#               numpy ch·ªâ ho·∫°t ƒë·ªông tr√™n cpu
                entities = self._convert_predictions_without_offsets(
                    text,
                    # tensor([1, 3, 0])  ‚Üí  array([1, 3, 0])
                    predictions.cpu().numpy(),
                    inputs["input_ids"][0].cpu().numpy()
                )
            # Apply rule-based post-processing for better accuracy
            # merge v·ªõi regex, filter r√°c, normalize values
            entities = self._post_process_entities(text, entities)
            print("\n" + "="*60)
            print("Final entities g·ª≠i cho NestJS:")
            print(json.dumps(entities, indent=2, ensure_ascii=False))
            print("="*60)

            processing_time = (time.time() - start_time) * 1000
            
            return {
                "entities": entities,
                "processing_time_ms": processing_time
            }
            
        except Exception as e:
            logger.error(f"NER extraction error: {str(e)}")
            raise
    
    def _convert_predictions_to_entities(
        self,
        text: str,
        predictions: List[int],  # [0, 3, 4, 0, 5, 6]
        offset_mapping: List[Tuple[int, int]], # [(0,5), (6,8), ...]
        input_ids: List[int]
    ) -> List[Dict[str, Any]]:
        """Convert model predictions to entity list"""
        entities = []
        current_entity = None
        
        for idx, (pred, (start, end)) in enumerate(zip(predictions, offset_mapping)):
            # Skip special tokens
            if start == end: # Special tokens nh∆∞ <s>, </s>
                continue
            
            label = self.entity_labels[pred]
            print(f"label: {label}")
            
            if label.startswith("B-"):
                # Save previous entity
                if current_entity:
                    entities.append(current_entity)
                
                # Start new entity
                entity_type = label[2:] #c·∫Øt chu·ªói b·ªè "B-"
                current_entity = {
                    "type": self.ENTITY_TYPE_MAP.get(entity_type, entity_type.lower()),
                    "raw": text[start:end],
                    "start": start,
                    "end": end,
                    "confidence": 0.85  # Base confidence
                }
                print(f"current_entity: {current_entity}")
            
            elif label.startswith("I-") and current_entity:
                # Continue current entity
                current_entity["raw"] = text[current_entity["start"]:end]
                current_entity["end"] = end
            
            elif label == "O" and current_entity:
                # End current entity
                entities.append(current_entity)
                current_entity = None
        
        # Add last entity , n·∫øu entity ·ªü cu·ªëi c√¢u
        if current_entity:
            print(f"trong if add last entity")
            entities.append(current_entity)
        
        return entities
    
    def _convert_predictions_without_offsets(
        self,
        text: str,
        predictions: List[int],   # [0, 3, 4, 0, 5, 6]
        input_ids: List[int]    #[1, 5432, 8976, 2]
    ) -> List[Dict[str, Any]]:
        """
        Convert predictions to entities without offset mapping.
        This is a workaround for PhoBERT which doesn't support offset mapping.
        """
        entities = []
        current_entity = None
        # c√°ch tr·ªìng c√† chua
        print(f"---predictions----: {predictions}") #[0 0 0 2 8 0]
        print(f"---input_ids----: {input_ids}") #[    0   139   719 10709  5344     2]
        # B·∫¢N CH·∫§T L√Ä DETOKEN TOKEN ID -> TEXT
        # SAU ƒê√ì T√åM C√ÅI TEXT V·ª™A ƒêC DETOKEN TRONG USER_PROMPT_TEXT
        # Debug: Log ALL tokens and predictions
        all_tokens_debug = []
        for i, (token_id, pred) in enumerate(zip(input_ids, predictions)):
            token = self.tokenizer.decode([token_id], skip_special_tokens=True)
            print(f"token trong loop debug: {token}")
            label = self.entity_labels[pred]
            all_tokens_debug.append(f"~~'{token}'‚Üí{label}")
        logger.info(f"All tokens: {all_tokens_debug}")
        
        pred_labels = [self.entity_labels[p] for p in predictions]
        #pred_labels___: ['O', 'O', 'O', 'B-CROP', 'I-CROP', 'O']
        print(f"pred_labels___: {pred_labels}")
        #non_o_labels: ['B-CROP', 'I-CROP']
        non_o_labels = [l for l in pred_labels if l != 'O']
        print(f"non_o_labels: {non_o_labels}")
        logger.info(f"NER Predictions - Total tokens: {len(predictions)}, Non-O predictions: {len(non_o_labels)}")
        if non_o_labels:
            logger.info(f"Entity predictions found: {non_o_labels}")
        else:
            logger.warning(f"All predictions are 'O' (no entities) - Model may not be trained yet!")
        
        # Normalize text for matching
        text_lower = text.lower()
        current_pos = 0 #v·ªã tr√≠ hi·ªán t·∫°i trong text
        print(f"1.currPOS: {current_pos}")
        for idx, (pred, token_id) in enumerate(zip(predictions, input_ids)):
            # l·∫•y ƒë∆∞·ª£c nh√£n "B-xxxx","I-xxxx","O"
            #[0 0 0 2 8 0] 
            #[0 139 719 10709 5344 2]
            label = self.entity_labels[pred]
            print(f"label trong for_loop ko offset: {label}")
            print(f"tokenID trong for_loop ko offset: {token_id}")

            # Skip special tokens (<s>, </s>, <pad>)
            if token_id in [self.tokenizer.bos_token_id, self.tokenizer.eos_token_id, self.tokenizer.pad_token_id]:
                continue
            
            # Decode token - note PhoBERT adds underscores for word boundaries
            # strip() = trim()
            token_text = self.tokenizer.decode([token_id], skip_special_tokens=True).strip()
            print(f"token_text: {token_text}")
            if not token_text:
                continue
            
            # Remove underscore prefix that PhoBERT uses (e.g., "_c√†" -> "c√†")
            # key
            token_clean = token_text.replace("_", " ").strip()
            print(f"token_clean: {token_clean}")
            
            # Find token in text (case-insensitive search from current position)
            token_lower = token_clean.lower()
            print(f"final_token_clean_lower: {token_lower}")
            # t√¨m token trong prompt_text_g·ªëc , 
            # t√¨m v·ªã tr√≠ ƒë·∫ßu ti√™n token xu·∫•t hi·ªán t√¨m t·ª´ v·ªã tr√≠ current_pos tr·ªü ƒëi
            token_start = text_lower.find(token_lower, current_pos)
            print(f"token_start: {token_start}")
            # n·∫øu ko t√¨m ƒëc token (case kh√≥)
            # vd text g·ªëc kh√°c vs tokenizer "ƒëi·ªán tho·∫°i" - "ƒëi·ªántho·∫°i"
            if token_start == -1:
                # Try exact match without spaces
                # token_clean= "ƒëi·ªán tho·∫°i" -> "ƒëi·ªántho·∫°i"
                token_no_space = token_clean.replace(" ", "")
                token_start = text_lower.find(token_no_space.lower(), current_pos)
                if token_start != -1:
                    print(f"t√¨m th·∫•y token")
                    print(f"token_start trong if vi·∫øtli·ªÅn: {token_start}")

                    token_clean = token_no_space
            
            if token_start == -1:
                # Can't find this token - skip it
                continue
            
            token_end = token_start + len(token_clean)
            current_pos = token_end
            print(f"2.currPOS: {current_pos}")

            # ƒë√£ c√≥ token start,end -> Logic gi·ªëng h√†m c√≥ offset
            if label.startswith("B-"):
                # Save previous entity
                if current_entity:
                    entities.append(current_entity)
                
                # Start new entity
                entity_type = label[2:]
                current_entity = {
                    "type": self.ENTITY_TYPE_MAP.get(entity_type, entity_type.lower()),#key,default
                    "raw": text[token_start:token_end],
                    "start": token_start,
                    "end": token_end,
                    "confidence": 0.85
                }
            
            elif label.startswith("I-"):
                if current_entity:
                    # Continue current entity - extend to include this token
                    current_entity["raw"] = text[current_entity["start"]:token_end]
                    current_entity["end"] = token_end
                else:
                    # Orphaned I- tag (B- was skipped/not found) - treat as new entity
                    logger.debug(f"Orphaned I- tag {label} at position {token_start}, treating as B-")
                    entity_type = label[2:]
                    current_entity = {
                        "type": self.ENTITY_TYPE_MAP.get(entity_type, entity_type.lower()),
                        "raw": text[token_start:token_end],
                        "start": token_start,
                        "end": token_end,
                        "confidence": 0.75  # Lower confidence for orphaned tags
                    }
            
            elif label == "O" and current_entity:
                # End current entity
                print(f"!!!! add v√†o entities")
                entities.append(current_entity)
                current_entity = None
        
        # Add last entity
        if current_entity:
            print(f"!!!! add cu·ªëi")
            entities.append(current_entity)

        print(f"entities_extractNoOffset: {entities}")
        
        return entities
    
    def _post_process_entities(self, text: str, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Apply rule-based post-processing to improve accuracy
        Uses patterns similar to the original rule-based system
        """
        processed = []
        
        # Add rule-based entities for common patterns (high priority)
        rule_entities = self._extract_rule_based_entities(text)
        print("\n" + "="*60)
        print(" B∆Ø·ªöC 1 - Rule-based entities (Regex):")
        print(json.dumps(rule_entities, indent=2, ensure_ascii=False))
        print("="*60)
        # Merge PhoBERT and rule-based entities
        all_entities = entities + rule_entities
        print("\n" + "="*60)
        print(" B∆Ø·ªöC 2 - Merged entities (AI + Regex):")
        print(json.dumps(all_entities, indent=2, ensure_ascii=False))
        print("="*60)
        
        # Remove duplicates and overlaps (prefer rule-based for multi-word)
        all_entities = self._remove_overlapping_entities(all_entities)
        print("\n" + "="*60)
        print(" B∆Ø·ªöC 3 - Sau khi lo·∫°i b·ªè overlap:")
        print(json.dumps(all_entities, indent=2, ensure_ascii=False))
        print("="*60)
        
        # Filter out invalid single-word entities
        all_entities = self._filter_invalid_entities(text, all_entities)
        print("\n" + "="*60)
        print(" B∆Ø·ªöC 4 - Sau khi l·ªçc invalid entities:")
        print(json.dumps(all_entities, indent=2, ensure_ascii=False))
        print("="*60)
        
        # Normalize values
        for entity in all_entities:
            entity["value"] = self._normalize_entity_value(entity)
        print("\n" + "="*60)
        print(" B∆Ø·ªöC 5 - Sau khi normalize values:")
        print(json.dumps(all_entities, indent=2, ensure_ascii=False))
        print("="*60)
        
        return all_entities
    
    def _extract_rule_based_entities(self, text: str) -> List[Dict[str, Any]]:
        """Extract entities using rule-based patterns (fallback)"""
        entities = []
        
        # Date patterns (prioritize longer/more specific patterns first)
        date_patterns = [
            # Month + Year combinations (most specific first)
            (r'th√°ng\s*\d{1,2}\s*nƒÉm\s*\d{4}', 'date'),  # th√°ng 11 nƒÉm 2024
            (r'th√°ng\s*\d{1,2}\s*nƒÉm\s*(?:ngo√°i|tr∆∞·ªõc)', 'date'),  # th√°ng 11 nƒÉm ngo√°i
            # Year patterns
            (r'nƒÉm\s*\d{4}', 'date'),  # nƒÉm 2024
            (r'nƒÉm\s*(?:nay|n√†y)', 'date'),  # nƒÉm nay
            (r'nƒÉm\s*(?:ngo√°i|tr∆∞·ªõc)', 'date'),  # nƒÉm ngo√°i
            # Month patterns
            (r'th√°ng\s*\d{1,2}', 'date'),  # th√°ng 11
            (r'th√°ng\s*(?:n√†y|nay)', 'date'),  # th√°ng n√†y
            (r'th√°ng\s*tr∆∞·ªõc', 'date'),  # th√°ng tr∆∞·ªõc
            # Quarter patterns
            (r'qu√Ω\s*\d', 'date'),  # qu√Ω 1, qu√Ω 2
            # Week patterns
            (r'tu·∫ßn\s*(?:n√†y|nay)', 'date'),  # tu·∫ßn n√†y
            (r'tu·∫ßn\s*tr∆∞·ªõc', 'date'),  # tu·∫ßn tr∆∞·ªõc
            # Day patterns
            (r'h√¥m\s*nay', 'date'),
            (r'h√¥m\s*qua', 'date'),
            # Date format
            (r'\d{1,2}/\d{1,2}/\d{2,4}', 'date'),
        ]
        
        # Multi-word crop patterns (prioritize longer matches)
        crop_patterns = [
            # Common 2-3 word crops
            (r'\bc√†\s+chua\b', 'crop_name'),
            (r'\bc√†\s+ph√™\b', 'crop_name'),
            (r'\bc√†\s+r·ªët\b', 'crop_name'),
            (r'\bc√†\s+t√≠m\b', 'crop_name'),
            (r'\bkhoai\s+lang\b', 'crop_name'),
            (r'\bkhoai\s+t√¢y\b', 'crop_name'),
            (r'\bkhoai\s+m√¨\b', 'crop_name'),
            (r'\bs·∫ßu\s+ri√™ng\b', 'crop_name'),
            (r'\bthanh\s+long\b', 'crop_name'),
            (r'\bh·ªì\s+ti√™u\b', 'crop_name'),
            (r'\bcao\s+su\b', 'crop_name'),
            (r'\bƒë·∫≠u\s+t∆∞∆°ng\b', 'crop_name'),
            (r'\bƒë·∫≠u\s+ph·ªông\b', 'crop_name'),
            (r'\bb·∫Øp\s+c·∫£i\b', 'crop_name'),
            (r'\brau\s+mu·ªëng\b', 'crop_name'),
            (r'\brau\s+d·ªÅn\b', 'crop_name'),
            (r'\bd∆∞a\s+chu·ªôt\b', 'crop_name'),
            (r'\bd∆∞a\s+h·∫•u\b', 'crop_name'),
            (r'\bsu\s+su\b', 'crop_name'),
            (r'\bsu\s+h√†o\b', 'crop_name'),
            (r'\bc·ªß\s+c·∫£i\b', 'crop_name'),
        ]
        
        # Multi-word device patterns
        device_patterns = [
            (r'\bm√°y\s+b∆°m\b', 'device_name'),
            (r'\bm√°y\s+t∆∞·ªõi\b', 'device_name'),
            (r'\bm√°y\s+phun\b', 'device_name'),
            (r'\bvan\s+n∆∞·ªõc\b', 'device_name'),
            (r'\bc·∫£m\s+bi·∫øn\b', 'device_name'),
            (r'\bh·ªá\s+th·ªëng\s+t∆∞·ªõi\b', 'device_name'),
        ]
        
        # Farm area patterns
        area_patterns = [
            (r'\bh√†ng\s+\d+\b', 'farm_area'),  # h√†ng 1, h√†ng 2
            (r'\blu·ªëng\s+\d+\b', 'farm_area'),  # lu·ªëng 1
            (r'\bkhu\s+[A-Z]\b', 'farm_area'),  # khu A
            (r'\bkhu\s+\d+\b', 'farm_area'),  # khu 1
            (r'\bv∆∞·ªùn\s+\w+\b', 'farm_area'),  # v∆∞·ªùn cam
        ]
        
        # Duration patterns
        duration_patterns = [
            (r'\d+\s*ph√∫t', 'duration'),  # 5 ph√∫t, 10 ph√∫t
            (r'\d+\s*gi·ªù', 'duration'),   # 1 gi·ªù, 2 gi·ªù
            (r'\d+\s*ng√†y', 'duration'),  # 1 ng√†y
            (r'n·ª≠a\s+ti·∫øng', 'duration'), # n·ª≠a ti·∫øng
            (r'\d+\s*ti·∫øng\s*r∆∞·ª°i', 'duration'), # 1 ti·∫øng r∆∞·ª°i
        ]

        # Combine all patterns (prioritize multi-word patterns first)
        all_patterns = (
            crop_patterns + 
            device_patterns + 
            area_patterns + 
            duration_patterns +
            date_patterns
        )
        # pattern = regex, entity_type = type
        for pattern, entity_type in all_patterns:
            # finditer t√¨m t·∫•t c·∫£ c√°c ƒëo·∫°n vƒÉn kh·ªõp v·ªõi m·∫´u trong text
            # re l√† th∆∞ vi·ªán built in c·ªßa Python ƒë·ªÉ l√†m vi·ªác v·ªõi regex
            for match in re.finditer(pattern, text, re.IGNORECASE):
                print(f"append trong match")
                entities.append({
                    "type": entity_type,
                    "raw": match.group(0),
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.95  # Very high confidence for rule-based multi-word
                })
                print("\n" + "="*60)
                print("üîç Entities t√¨m ƒë∆∞·ª£c b·∫±ng Regex:")
                print(json.dumps(entities, indent=2, ensure_ascii=False))
                print("="*60)
        
        return entities
    
    def _filter_invalid_entities(self, text: str, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Filter out invalid entities based on context and common sense rules
        """
        filtered = []
        
        # Invalid single words that are often misclassified
        invalid_crop_words = {"nam", "b·∫Øc", "trung", "mi·ªÅn", "chua", "t√¢y", "ƒë√¥ng"}
        invalid_area_words = {"1", "2", "3", "4", "5", "6", "7", "8", "9", "0"}
        
        for entity in entities:
            raw_lower = entity["raw"].lower().strip()
            entity_type = entity["type"]
            
            # Filter invalid crop names
            if entity_type == "crop_name":
                # Reject single invalid words
                if raw_lower in invalid_crop_words:
                    logger.debug(f"Filtered invalid crop: '{entity['raw']}'")
                    continue
                # Reject very short single-character crops (except valid ones)
                if len(raw_lower) == 1:
                    continue
            
            # Filter invalid farm areas
            if entity_type == "farm_area":
                # Reject standalone numbers without context
                if raw_lower in invalid_area_words:
                    logger.debug(f"Filtered invalid area: '{entity['raw']}'")
                    continue
            
            filtered.append(entity)
        
        return filtered
    
    def _remove_overlapping_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Remove overlapping entities, keeping the one with higher confidence and longer span"""
        if not entities:
            return []
        
        # Sort by start position, then by confidence (higher first), then by length (longer first)
        # sort theo v·ªã tr·ªã b·∫Øt ƒë·∫ßu (tƒÉng d·∫ßn),
        # (d·∫•u - ƒë·ªÉ ƒë·∫£o ngc th·ª© t·ª±) n·∫øu c√πng v·ªã tr√≠ ∆∞u ti√™n confidence cao h∆°n
        # n·∫øu c√πng v·ªã tr√≠ v√† confidence ∆∞u ti√™n entity d√†i h∆°n
        sorted_entities = sorted(
            entities, 
            key=lambda x: (x["start"], -x["confidence"], -(x["end"] - x["start"]))
        )
        
        result = []
        last_end = -1
        # last_end v·ªã tr√≠ k·∫øt th√∫c c·ªßa entity cu·ªëi c√πng ƒë∆∞·ª£c ch·ªçn
        for entity in sorted_entities:
            if entity["start"] >= last_end:
                result.append(entity)
                last_end = entity["end"]
        
        return result
    
    def _normalize_entity_value(self, entity: Dict[str, Any]) -> str:
        """Normalize entity value based on type"""
        raw = entity["raw"]
        entity_type = entity["type"]
        
        if entity_type == "date":
            return self._normalize_date(raw)
        elif entity_type == "money":
            return self._normalize_money(raw)
        else:
            return raw.strip()
    
    def _normalize_date(self, date_str: str) -> str:
        """Normalize date to ISO format or relative format"""
        from datetime import datetime, timedelta
        
        normalized = date_str.lower().strip()
        now = datetime.now()
        
        # Keep Vietnamese date expressions as-is for NestJS date parser
        if "h√¥m nay" in normalized:
            return now.strftime("%Y-%m-%d")
        elif "h√¥m qua" in normalized:
            return (now - timedelta(days=1)).strftime("%Y-%m-%d")
        elif "tu·∫ßn n√†y" in normalized:
            return "tu·∫ßn n√†y"  # Keep Vietnamese
        elif "tu·∫ßn tr∆∞·ªõc" in normalized:
            return "tu·∫ßn tr∆∞·ªõc"  # Keep Vietnamese
        elif "th√°ng n√†y" in normalized:
            return "th√°ng n√†y"  # Keep Vietnamese
        elif "th√°ng tr∆∞·ªõc" in normalized:
            return "th√°ng tr∆∞·ªõc"  # Keep Vietnamese
        elif "nƒÉm n√†y" in normalized:
            return "nƒÉm nay"  # Keep Vietnamese
        elif "nƒÉm tr∆∞·ªõc" in normalized or "nƒÉm ngo√°i" in normalized:
            return "nƒÉm tr∆∞·ªõc"  # Keep Vietnamese
        
        # Try to parse dd/mm/yyyy
        # Input: "15/3/24"
        # Output: "2024-03-15"
        date_match = re.match(r'(\d{1,2})/(\d{1,2})/(\d{2,4})', normalized)
        if date_match:
            day, month, year = date_match.groups()
            year = f"20{year}" if len(year) == 2 else year
            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
        
        return date_str
    
    def _normalize_money(self, money_str: str) -> str:
        """Normalize money to number (VND)"""
        normalized = money_str.lower().strip()
        
        # Extract number
        number_match = re.search(r'([\d,\.]+)', normalized)
        if not number_match:
            return "0"
        
        value = float(number_match.group(1).replace(',', '.'))
        
        # Handle units
        if 't·ª∑' in normalized:
            value *= 1000000000
        elif 'tri·ªáu' in normalized or 'tr' in normalized:
            value *= 1000000
        elif 'k' in normalized:
            value *= 1000
        
        return str(int(value))

