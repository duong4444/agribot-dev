{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåæ AgriBot NER Training with PhoBERT (IoT Focus)\n",
    "\n",
    "Train PhoBERT-based NER model for Vietnamese Agricultural IoT Chatbot\n",
    "\n",
    "**Entity Types (6 types):**\n",
    "- `DATE`: th√°ng n√†y, qu√Ω 1, nƒÉm nay, th√°ng 11, ...\n",
    "- `CROP`: cam s√†nh, l√∫a ST25, xo√†i c√°t chu, ...\n",
    "- `AREA`: khu A, khu B, khu 1, ...\n",
    "- `DURATION`: 5 ph√∫t, 10 ph√∫t, 1 gi·ªù, ...\n",
    "- `DEVICE`: m√°y b∆°m, ƒë√®n, t∆∞·ªõi, b∆°m, ...\n",
    "- `METRIC`: nhi·ªát ƒë·ªô, ƒë·ªô ·∫©m, √°nh s√°ng, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch scikit-learn seqeval pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 2: Load Training Data\n",
    "\n",
    "**Upload CSV file** generated by `generate_ner_data_v2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from google.colab import files\n",
    "\n",
    "# Upload CSV file\n",
    "print(\"üì§ Please upload your CSV file (generated by generate_ner_data_v2.py)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded filename\n",
    "csv_filename = list(uploaded.keys())[0]\n",
    "print(f\"\\nüìÇ Loading data from {csv_filename}...\")\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Convert to training format: [(text, [(start, end, label), ...]), ...]\n",
    "training_data = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    entities_json = json.loads(row['entities'])\n",
    "    # Convert to (start, end, type) tuples\n",
    "    entities = [(e['start'], e['end'], e['type']) for e in entities_json]\n",
    "    training_data.append((text, entities))\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(training_data)} training examples from CSV\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìù Sample data:\")\n",
    "for i, (text, entities) in enumerate(training_data[:5]):\n",
    "    print(f\"\\n{i+1}. Text: {text}\")\n",
    "    print(f\"   Entities: {entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 3: Convert to BIO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio_format(data: List[tuple]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Convert annotated data to BIO format\n",
    "    \"\"\"\n",
    "    bio_data = []\n",
    "    \n",
    "    for text, entities in data:\n",
    "        # Tokenize by word\n",
    "        words = text.split()\n",
    "        labels = ['O'] * len(words)\n",
    "        \n",
    "        # Create character to word index mapping\n",
    "        char_to_word = {}\n",
    "        current_pos = 0\n",
    "        for word_idx, word in enumerate(words):\n",
    "            word_start = text.find(word, current_pos)\n",
    "            word_end = word_start + len(word)\n",
    "            for char_idx in range(word_start, word_end):\n",
    "                char_to_word[char_idx] = word_idx\n",
    "            current_pos = word_end\n",
    "        \n",
    "        # Assign BIO labels\n",
    "        for start, end, entity_type in entities:\n",
    "            # Find words that overlap with entity span\n",
    "            entity_words = set()\n",
    "            for char_idx in range(start, end):\n",
    "                if char_idx in char_to_word:\n",
    "                    entity_words.add(char_to_word[char_idx])\n",
    "            \n",
    "            entity_words = sorted(entity_words)\n",
    "            if entity_words:\n",
    "                # First word gets B- tag\n",
    "                labels[entity_words[0]] = f\"B-{entity_type}\"\n",
    "                # Remaining words get I- tag\n",
    "                for word_idx in entity_words[1:]:\n",
    "                    labels[word_idx] = f\"I-{entity_type}\"\n",
    "        \n",
    "        bio_data.append({\n",
    "            \"tokens\": words,\n",
    "            \"ner_tags\": labels\n",
    "        })\n",
    "    \n",
    "    return bio_data\n",
    "\n",
    "bio_dataset = convert_to_bio_format(training_data)\n",
    "\n",
    "# Display first example\n",
    "print(\"\\nüìù Example BIO format:\")\n",
    "example = bio_dataset[0]\n",
    "for token, tag in zip(example['tokens'], example['ner_tags']):\n",
    "    print(f\"{token:20} ‚Üí {tag}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Converted {len(bio_dataset)} examples to BIO format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 4: Create Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique labels\n",
    "all_labels = set()\n",
    "for example in bio_dataset:\n",
    "    all_labels.update(example['ner_tags'])\n",
    "\n",
    "# Sort labels (O first, then B- tags, then I- tags)\n",
    "label_list = sorted(all_labels, key=lambda x: (x != 'O', x))\n",
    "\n",
    "# Create label mappings\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Total labels: {len(label_list)}\")\n",
    "print(\"\\nLabel mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"{idx:2d}: {label}\")\n",
    "\n",
    "# Save label mapping\n",
    "label_mapping = {\n",
    "    \"label_to_id\": label2id,\n",
    "    \"id_to_label\": id2label,\n",
    "    \"entity_types\": list(set([label.split('-')[1] for label in label_list if '-' in label]))\n",
    "}\n",
    "\n",
    "with open('label_mapping.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(label_mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Saved label_mapping.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Step 5: Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "def prepare_dataset(bio_data, label2id):\n",
    "    dataset_dict = {\n",
    "        \"tokens\": [],\n",
    "        \"ner_tags\": []\n",
    "    }\n",
    "    \n",
    "    for example in bio_data:\n",
    "        dataset_dict[\"tokens\"].append(example[\"tokens\"])\n",
    "        # Convert labels to IDs\n",
    "        tag_ids = [label2id[tag] for tag in example[\"ner_tags\"]]\n",
    "        dataset_dict[\"ner_tags\"].append(tag_ids)\n",
    "    \n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Split train/validation (80/20)\n",
    "train_data, val_data = train_test_split(bio_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = prepare_dataset(train_data, label2id)\n",
    "val_dataset = prepare_dataset(val_data, label2id)\n",
    "\n",
    "print(f\"\\nüìä Dataset split:\")\n",
    "print(f\"  Training: {len(train_dataset)} examples\")\n",
    "print(f\"  Validation: {len(val_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 6: Load PhoBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"vinai/phobert-base\"\n",
    "num_labels = len(label_list)\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on {device}\")\n",
    "print(f\"   Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 7: Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 7\n",
    "PhoBERT tokenizer kh√¥ng support word_ids(), c·∫ßn manual alignment\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text and align NER labels with subword tokens\n",
    "    PhoBERT tokenizer doesn't support word_ids(), so we do manual alignment\n",
    "    \"\"\"\n",
    "    tokenized_inputs = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    \n",
    "    for tokens, ner_tags in zip(examples[\"tokens\"], examples[\"ner_tags\"]):\n",
    "        # Join tokens back to text\n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        # Tokenize the full text\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Get token IDs\n",
    "        token_ids = encoding[\"input_ids\"]\n",
    "        attention_mask = encoding[\"attention_mask\"]\n",
    "        \n",
    "        # Initialize labels with -100 (ignore index)\n",
    "        labels = [-100] * len(token_ids)\n",
    "        \n",
    "        # Manual alignment: match each word to its tokens\n",
    "        current_pos = 0\n",
    "        word_idx = 0\n",
    "        \n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            # Skip special tokens\n",
    "            if token_id in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id]:\n",
    "                continue\n",
    "            \n",
    "            # Decode token\n",
    "            token_text = tokenizer.decode([token_id], skip_special_tokens=True).strip()\n",
    "            \n",
    "            # Remove PhoBERT underscore prefix\n",
    "            token_clean = token_text.replace(\"_\", \" \").strip()\n",
    "            \n",
    "            if not token_clean:\n",
    "                continue\n",
    "            \n",
    "            # Try to match this token to a word\n",
    "            if word_idx < len(tokens):\n",
    "                word = tokens[word_idx]\n",
    "                \n",
    "                # Check if this token is part of the current word\n",
    "                if token_clean.lower() in word.lower() or word.lower().startswith(token_clean.lower()):\n",
    "                    # Assign the label for this word\n",
    "                    labels[i] = ner_tags[word_idx]\n",
    "                    \n",
    "                    # Check if we've finished this word\n",
    "                    if token_clean.lower() == word.lower():\n",
    "                        word_idx += 1\n",
    "                else:\n",
    "                    # Move to next word\n",
    "                    word_idx += 1\n",
    "                    if word_idx < len(tokens):\n",
    "                        labels[i] = ner_tags[word_idx]\n",
    "        \n",
    "        tokenized_inputs[\"input_ids\"].append(encoding[\"input_ids\"])\n",
    "        tokenized_inputs[\"attention_mask\"].append(encoding[\"attention_mask\"])\n",
    "        tokenized_inputs[\"labels\"].append(labels)\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing training dataset...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_and_align_labels, \n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenizing validation dataset...\")\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_and_align_labels, \n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Datasets tokenized\")\n",
    "print(f\"   Training samples: {len(tokenized_train)}\")\n",
    "print(f\"   Validation samples: {len(tokenized_val)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 8: Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 8 (thay th·∫ø TrainingArguments)\n",
    "Disable wandb logging\n",
    "\"\"\"\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "# Disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Metric computation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "# Training arguments (with wandb disabled)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",  # Disable all reporting (wandb, tensorboard, etc.)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Logging: Disabled (no wandb)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 9: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüöÄ Starting training...\\n\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 10: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 11: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "output_dir = \"./ner_extractor_final\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Copy label mapping\n",
    "import shutil\n",
    "shutil.copy('label_mapping.json', f'{output_dir}/label_mapping.json')\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved to {output_dir}\")\n",
    "print(\"\\nüì¶ Files to download:\")\n",
    "print(\"  - config.json\")\n",
    "print(\"  - pytorch_model.bin (or model.safetensors)\")\n",
    "print(\"  - label_mapping.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 12: Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new examples\n",
    "test_examples = [\n",
    "    \"B·∫≠t t∆∞·ªõi khu A trong 5 ph√∫t\",\n",
    "    \"ƒê·ªô ·∫©m ·ªü khu B l√† bao nhi√™u\",\n",
    "    \"Chi ph√≠ th√°ng n√†y\",\n",
    "    \"T·∫Øt ƒë√®n khu C\",\n",
    "    \"Nhi·ªát ƒë·ªô khu 1 hi·ªán t·∫°i\",\n",
    "    \"Doanh thu qu√Ω 2\",\n",
    "    \"C√°ch tr·ªìng cam s√†nh\"\n",
    "]\n",
    "\n",
    "def predict_entities(text):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)[0]\n",
    "    \n",
    "    # Decode\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    labels = [id2label[p.item()] for p in predictions]\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token in [\"<s>\", \"</s>\", \"<pad>\"]:\n",
    "            continue\n",
    "            \n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {\"type\": label[2:], \"text\": token.replace(\"_\", \" \").strip()}\n",
    "        elif label.startswith(\"I-\") and current_entity:\n",
    "            current_entity[\"text\"] += \" \" + token.replace(\"_\", \"\").strip()\n",
    "        elif label == \"O\" and current_entity:\n",
    "            entities.append(current_entity)\n",
    "            current_entity = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "print(\"\\nüß™ Testing model on new examples:\\n\")\n",
    "for example in test_examples:\n",
    "    entities = predict_entities(example)\n",
    "    print(f\"Text: {example}\")\n",
    "    print(f\"Entities: {entities}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 13: Download Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip model files for easy download\n",
    "!zip -r ner_model.zip ner_extractor_final/\n",
    "print(\"‚úÖ Model zipped as ner_model.zip\")\n",
    "print(\"\\nüì• Download ner_model.zip from Colab Files panel\")\n",
    "print(\"\\nüìã Deployment instructions:\")\n",
    "print(\"1. Extract ner_model.zip\")\n",
    "print(\"2. Copy files to: C:\\\\Users\\\\ADMIN\\\\Desktop\\\\ex\\\\apps\\\\python-ai-service\\\\models\\\\ner_extractor\\\\\")\n",
    "print(\"3. Restart Python AI service\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
