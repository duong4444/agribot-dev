{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåæ AgriBot NER Training with PhoBERT (IoT Focus)\n",
    "\n",
    "Train PhoBERT-based NER model for Vietnamese Agricultural IoT Chatbot\n",
    "\n",
    "**Entity Types (6 types):**\n",
    "- `DATE`: th√°ng n√†y, qu√Ω 1, nƒÉm nay, th√°ng 11, ...\n",
    "- `CROP`: cam s√†nh, l√∫a ST25, xo√†i c√°t chu, ...\n",
    "- `AREA`: khu A, khu B, khu 1, ...\n",
    "- `DURATION`: 5 ph√∫t, 10 ph√∫t, 1 gi·ªù, ...\n",
    "- `DEVICE`: m√°y b∆°m, ƒë√®n, t∆∞·ªõi, b∆°m, ...\n",
    "- `METRIC`: nhi·ªát ƒë·ªô, ƒë·ªô ·∫©m, √°nh s√°ng, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch scikit-learn seqeval pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 2: Load Training Data\n",
    "\n",
    "**Upload CSV file** generated by `generate_ner_data_v2.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from google.colab import files\n",
    "\n",
    "\"\"\"\n",
    "chuy·ªÉn ƒë·ªãnh d·∫°ng data sang\n",
    " 1. Text: ng√¥ ng·ªçt b·ªã b·ªánh g√¨\n",
    "   Entities: [(0, 8, 'CROP')]\n",
    "\"\"\"\n",
    "\n",
    "# Upload CSV file\n",
    "print(\"üì§ Please upload your CSV file (generated by generate_ner_data_v2.py)\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the uploaded filename\n",
    "csv_filename = list(uploaded.keys())[0]\n",
    "print(f\"\\nüìÇ Loading data from {csv_filename}...\")\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_filename)\n",
    "\n",
    "# Convert to training format: [(text, [(start, end, label), ...]), ...]\n",
    "training_data = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    text = row['text']\n",
    "    entities_json = json.loads(row['entities'])\n",
    "    # Convert to (start, end, type) tuples\n",
    "    entities = [(e['start'], e['end'], e['type']) for e in entities_json]\n",
    "    training_data.append((text, entities))\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(training_data)} training examples from CSV\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nüìù Sample data:\")\n",
    "for i, (text, entities) in enumerate(training_data[:5]):\n",
    "    print(f\"\\n{i+1}. Text: {text}\")\n",
    "    print(f\"   Entities: {entities}\")\n",
    "\"\"\"\n",
    "üìù Sample data:\n",
    "1. Text: ng√¥ ng·ªçt b·ªã b·ªánh g√¨\n",
    "   Entities: [(0, 8, 'CROP')]\n",
    "2. Text: Th√¥ng tin v·ªÅ gi·ªëng h√†nh t√≠m\n",
    "   Entities: [(19, 27, 'CROP')]\n",
    "3. Text: S√¢u ƒë·ª•c th√¢n ·ªü m∆∞·ªõp x·ª≠ l√Ω nh∆∞ th·∫ø n√†o\n",
    "   Entities: [(15, 19, 'CROP')]\n",
    "4. Text: C√°ch tr·ªìng d∆∞a l∆∞·ªõi\n",
    "   Entities: [(11, 19, 'CROP')]\n",
    "5. Text: T√¥i mu·ªën bi·∫øt v·ªÅ gi·ªëng cam Cao Phong\n",
    "   Entities: [(23, 36, 'CROP')]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Step 3: Convert to BIO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio_format(data: List[tuple]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    T·ª©c l√† ƒë·∫ßu v√†o l√†\n",
    "    1. Text: ng√¥ ng·ªçt b·ªã b·ªánh g√¨\n",
    "    Entities: [(0, 8, 'CROP')]\n",
    "    || Entity ƒëang ƒëc g√°n theo v·ªã tr√≠ k√Ω t·ª± , ko ph·∫£i theo word\n",
    "    =>\n",
    "    ƒê·∫ßu ra \n",
    "#   {\n",
    "#     \"tokens\": [\"ng√¥\", \"ng·ªçt\", \"b·ªã\", \"b·ªánh\", \"g√¨\"],\n",
    "#     \"ner_tags\": [\"B-CROP\", \"I-CROP\", \"O\", \"O\", \"O\"]\n",
    "#   }\n",
    "\n",
    "B1. Duy·ªát t·ª´ng m·∫´u d·ªØ li·ªáu\n",
    "B2. Tokenizen c√¢u b·∫±ng split() ->   tokens = [\"ng√¥\", \"ng·ªçt\", \"b·ªã\", \"b·ªánh\", \"g√¨\"]\n",
    "                                    labels = [\"B-CROP\", \"I-CROP\", \"O\", \"O\", \"O\"]\n",
    "B3. T·∫°o mapping char index -> word index\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Convert annotated data to BIO format\n",
    "    Chuy·ªÉn data t·ª´ format v·ªã tr√≠ k√Ω t·ª± sang format BIO\n",
    "    Data g·ªëc ƒë√°nh d·∫•u v·ªã tr√≠ k√Ω t·ª±(start,end)\n",
    "    AI c·∫ßn nh√£n cho t·ª´ng t·ª´ (BIO tags)\n",
    "text = \"ng√¥ ng·ªçt b·ªã b·ªánh g√¨\"\n",
    "entities = [(0, 8, 'CROP')]\n",
    "tokens = [\"ng√¥\", \"ng·ªçt\", \"b·ªã\", \"b·ªánh\", \"g√¨\"]\n",
    "labels = [\"B-CROP\", \"I-CROP\", \"O\", \"O\", \"O\"]\n",
    "\n",
    "    \"\"\"\n",
    "    bio_data = []\n",
    "    # duy·ªát t·ª´ng m·∫´u, l·∫•y text, entities t·ª´ng m·∫´u\n",
    "    for text, entities in data:\n",
    "        # Tokenize by word\n",
    "        words = text.split()\n",
    "        labels = ['O'] * len(words)\n",
    "# text = \"ng√¥ ng·ªçt b·ªã b·ªánh g√¨\"\n",
    "# words = [\"ng√¥\", \"ng·ªçt\", \"b·ªã\", \"b·ªánh\", \"g√¨\"]\n",
    "# labels = [\"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "        \n",
    "        \"\"\"\n",
    "        t·∫°o b·∫£n ƒë·ªì k√Ω t·ª± -> t·ª´\n",
    "        \"\"\"\n",
    "        # Create character to word index mapping\n",
    "        char_to_word = {}\n",
    "        current_pos = 0\n",
    "        # FOR 1 -  CHO WORD \n",
    "        for word_idx, word in enumerate(words):\n",
    "            # word_idx=0, word=\"ng√¥\"\n",
    "            # word_idx=1, word=\"ng·ªçt\"\n",
    "            # word_idx=2, word=\"b·ªã\"\n",
    "            word_start = text.find(word, current_pos)\n",
    "            word_end = word_start + len(word)\n",
    "            \"\"\"\n",
    "            v√≠ d·ª• \"ng√¥\" range(0,3) = [0,1,2]\n",
    "            char_to_word[0] = 0  # K√Ω t·ª± 'n' thu·ªôc t·ª´ 0 (idx) l√† ng√¥\n",
    "            char_to_word[1] = 0  # K√Ω t·ª± 'g' thu·ªôc t·ª´ 0\n",
    "            char_to_word[2] = 0  # K√Ω t·ª± '√¥' thu·ªôc t·ª´ 0\n",
    "\n",
    "            v√≠ d·ª• \"ng·ªçt\" range(4, 8) = [4, 5, 6, 7]\n",
    "            char_to_word[4] = 1  # K√Ω t·ª± 'n' thu·ªôc t·ª´ 1\n",
    "            char_to_word[5] = 1  # K√Ω t·ª± 'g' thu·ªôc t·ª´ 1\n",
    "            char_to_word[6] = 1  # K√Ω t·ª± '·ªç' thu·ªôc t·ª´ 1\n",
    "            char_to_word[7] = 1  # K√Ω t·ª± 't' thu·ªôc t·ª´ 1\n",
    "            \"\"\"\n",
    "            # char_idx 0 -> 3\n",
    "            for char_idx in range(word_start, word_end):\n",
    "                # n = 0\n",
    "                # g = 0\n",
    "                # √¥ = 0\n",
    "                char_to_word[char_idx] = word_idx\n",
    "            current_pos = word_end\n",
    "\n",
    "            \"\"\"\n",
    "             V√ç D·ª§ HO√ÄN CH·ªàNH\n",
    "Input:\n",
    "text = \"B·∫≠t t∆∞·ªõi khu A trong 5 ph√∫t\"\n",
    "entities = [\n",
    "    (4, 8, 'DEVICE'),    # \"t∆∞·ªõi\"\n",
    "    (9, 14, 'AREA'),     # \"khu A\"\n",
    "    (21, 28, 'DURATION') # \"5 ph√∫t\"\n",
    "]\n",
    "B∆∞·ªõc 1: T√°ch t·ª´\n",
    "words = [\"B·∫≠t\", \"t∆∞·ªõi\", \"khu\", \"A\", \"trong\", \"5\", \"ph√∫t\"]\n",
    "labels = [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "B∆∞·ªõc 2: T·∫°o b·∫£n ƒë·ªì\n",
    "text = \"B·∫≠t t∆∞·ªõi khu A trong 5 ph√∫t\"\n",
    "       0123456789...\n",
    "char_to_word = {\n",
    "    0: 0,  # 'B' ‚Üí t·ª´ 0 (\"B·∫≠t\")\n",
    "    1: 0,  # '·∫≠' ‚Üí t·ª´ 0\n",
    "    2: 0,  # 't' ‚Üí t·ª´ 0\n",
    "    4: 1,  # 't' ‚Üí t·ª´ 1 (\"t∆∞·ªõi\")\n",
    "    5: 1,  # '∆∞' ‚Üí t·ª´ 1\n",
    "    6: 1,  # '·ªõ' ‚Üí t·ª´ 1\n",
    "    7: 1,  # 'i' ‚Üí t·ª´ 1\n",
    "    9: 2,  # 'k' ‚Üí t·ª´ 2 (\"khu\")\n",
    "    10: 2, # 'h' ‚Üí t·ª´ 2\n",
    "    11: 2, # 'u' ‚Üí t·ª´ 2\n",
    "    13: 3, # 'A' ‚Üí t·ª´ 3 (\"A\")\n",
    "    ...\n",
    "}\n",
    "B∆∞·ªõc 3: G√°n nh√£n\n",
    "Entity 1: (4, 8, 'DEVICE') ‚Üí \"t∆∞·ªõi\"\n",
    "\n",
    "range(4, 8) = [4, 5, 6, 7]\n",
    "entity_words = {1}  # Ch·ªâ t·ª´ 1 (\"t∆∞·ªõi\")\n",
    "labels[1] = \"B-DEVICE\"\n",
    "K·∫øt qu·∫£: [\"O\", \"B-DEVICE\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "Entity 2: (9, 14, 'AREA') ‚Üí \"khu A\"\n",
    "\n",
    "range(9, 14) = [9, 10, 11, 12, 13]\n",
    "entity_words = {2, 3}  # T·ª´ 2 (\"khu\") v√† t·ª´ 3 (\"A\")\n",
    "labels[2] = \"B-AREA\"\n",
    "labels[3] = \"I-AREA\"\n",
    "K·∫øt qu·∫£: [\"O\", \"B-DEVICE\", \"B-AREA\", \"I-AREA\", \"O\", \"O\", \"O\"]\n",
    "Entity 3: (21, 28, 'DURATION') ‚Üí \"5 ph√∫t\"\n",
    "\n",
    "range(21, 28) = [21, 22, 23, 24, 25, 26, 27]\n",
    "entity_words = {5, 6}  # T·ª´ 5 (\"5\") v√† t·ª´ 6 (\"ph√∫t\")\n",
    "labels[5] = \"B-DURATION\"\n",
    "labels[6] = \"I-DURATION\"\n",
    "K·∫øt qu·∫£: [\"O\", \"B-DEVICE\", \"B-AREA\", \"I-AREA\", \"O\", \"B-DURATION\", \"I-DURATION\"]\n",
    "Output cu·ªëi c√πng:\n",
    "{\n",
    "    \"tokens\": [\"B·∫≠t\", \"t∆∞·ªõi\", \"khu\", \"A\", \"trong\", \"5\", \"ph√∫t\"],\n",
    "    \"ner_tags\": [\"O\", \"B-DEVICE\", \"B-AREA\", \"I-AREA\", \"O\", \"B-DURATION\", \"I-DURATION\"]\n",
    "}\n",
    "    trong c√°i range c·ªßa t·ª´ vd ng√¥ idx=0 th√¨ range(0,3) ƒë√°nh =0 h·∫øt bi·ªÉu th·ªã ƒë√≥ l√† ch·ªó ng√¥\n",
    "    range(4,8) c·ªßa ng·ªçt (idx=1) ƒë√°nh =1 h·∫øt bi·ªÉu th·ªã ƒë√≥ l√† ch·ªó c·ªßa ng·ªçt  \n",
    "    char_to_word = {\n",
    "    0: 0,  # 'n' ‚Üí t·ª´ 0 (\"ng√¥\")\n",
    "    1: 0,  # 'g' ‚Üí t·ª´ 0\n",
    "    2: 0,  # '√¥' ‚Üí t·ª´ 0\n",
    "    3: 0,  # ' ' ‚Üí KH√îNG G√ÅN (kho·∫£ng tr·∫Øng)\n",
    "    4: 1,  # 'n' ‚Üí t·ª´ 1 (\"ng·ªçt\")\n",
    "    5: 1,  # 'g' ‚Üí t·ª´ 1\n",
    "    6: 1,  # '·ªç' ‚Üí t·ª´ 1\n",
    "    7: 1,  # 't' ‚Üí t·ª´ 1\n",
    "    8: 1,  # ' ' ‚Üí KH√îNG G√ÅN\n",
    "    9: 2,  # 'b' ‚Üí t·ª´ 2 (\"b·ªã\")\n",
    "    10: 2, # '·ªã' ‚Üí t·ª´ 2\n",
    "    ...\n",
    "}\n",
    "            \"\"\"\n",
    "        \n",
    "        # Assign BIO labels\n",
    "#           1. Text: ng√¥ ng·ªçt b·ªã b·ªánh g√¨\n",
    "#           Entities: [(0, 8, 'CROP')]\n",
    "        # FOR 2 - CHO ENTITIES\n",
    "        for start, end, entity_type in entities:\n",
    "            # Find words that overlap with entity span\n",
    "            entity_words = set()\n",
    "            # range(0, 8) = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "            for char_idx in range(start, end):\n",
    "                # char_idx 0 -> 8\n",
    "#      char_to_word =\n",
    "#       {\n",
    "#           0:0, 1:0, 2:0,           # ng√¥\n",
    "#           4:1, 5:1, 6:1, 7:1,      # ng·ªçt\n",
    "#           9:2, 10:2,               # b·ªã\n",
    "#           12:3,13:3,14:3,15:3,     # b·ªánh\n",
    "#           17:4,18:4                # g√¨\n",
    "#       }\n",
    "                if char_idx in char_to_word:\n",
    "                    entity_words.add(char_to_word[char_idx])\n",
    "                # 0 -> 8 l√† c√≥ 0,1\n",
    "                # entity_words = {0,1}\n",
    "            \"\"\"\n",
    "char_idx=0 ‚Üí char_to_word[0]=0 ‚Üí entity_words={0}\n",
    "char_idx=1 ‚Üí char_to_word[1]=0 ‚Üí entity_words={0}\n",
    "char_idx=2 ‚Üí char_to_word[2]=0 ‚Üí entity_words={0}\n",
    "char_idx=3 ‚Üí KH√îNG T·ªíN T·∫†I (kho·∫£ng tr·∫Øng)\n",
    "char_idx=4 ‚Üí char_to_word[4]=1 ‚Üí entity_words={0, 1}\n",
    "char_idx=5 ‚Üí char_to_word[5]=1 ‚Üí entity_words={0, 1}\n",
    "char_idx=6 ‚Üí char_to_word[6]=1 ‚Üí entity_words={0, 1}\n",
    "char_idx=7 ‚Üí char_to_word[7]=1 ‚Üí entity_words={0, 1}\n",
    "K·∫øt qu·∫£: entity_words = {0, 1 ||can be 2,3,4,7,9,....}\n",
    "Entity \"ng√¥ ng·ªçt\" (k√Ω t·ª± 0-8) bao g·ªìm t·ª´ 0 (\"ng√¥\") v√† t·ª´ 1 (\"ng·ªçt\")\n",
    "            \"\"\"\n",
    "            entity_words = sorted(entity_words)\n",
    "            if entity_words:\n",
    "                # First word gets B- tag\n",
    "                # nh·ªØng th·∫±ng 0 th√¨ ƒë√°nh l√† B-CROP\n",
    "                labels[entity_words[0]] = f\"B-{entity_type}\"\n",
    "                # Remaining words get I- tag\n",
    "                # c√≤n l·∫°i ƒë√°nh I-CROP\n",
    "                for word_idx in entity_words[1:]:\n",
    "                    labels[word_idx] = f\"I-{entity_type}\"\n",
    "#   BIO_DATA C√ì D·∫†NG N√ÄY:\n",
    "#   {\n",
    "#     \"tokens\": [\"ng√¥\", \"ng·ªçt\", \"b·ªã\", \"b·ªánh\", \"g√¨\"],\n",
    "#     \"ner_tags\": [\"B-CROP\", \"I-CROP\", \"O\", \"O\", \"O\"]\n",
    "#   }\n",
    "\n",
    "        bio_data.append({\n",
    "            \"tokens\": words,\n",
    "            \"ner_tags\": labels\n",
    "        })\n",
    "    \n",
    "    return bio_data\n",
    "\n",
    "bio_dataset = convert_to_bio_format(training_data)\n",
    "\n",
    "# Display first example\n",
    "print(\"\\nüìù Example BIO format:\")\n",
    "example = bio_dataset[0]\n",
    "for token, tag in zip(example['tokens'], example['ner_tags']):\n",
    "    print(f\"{token:20} ‚Üí {tag}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Converted {len(bio_dataset)} examples to BIO format\")\n",
    "\"\"\"\n",
    "üìù Example BIO format:\n",
    "ng√¥                  ‚Üí B-CROP\n",
    "ng·ªçt                 ‚Üí I-CROP\n",
    "b·ªã                   ‚Üí O\n",
    "b·ªánh                 ‚Üí O\n",
    "g√¨                   ‚Üí O\n",
    "‚úÖ Converted 2000 examples to BIO format\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Step 4: Create Label Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all unique labels\n",
    "all_labels = set()\n",
    "for example in bio_dataset:\n",
    "    all_labels.update(example['ner_tags'])\n",
    "\n",
    "# Sort labels (O first, then B- tags, then I- tags)\n",
    "label_list = sorted(all_labels, key=lambda x: (x != 'O', x))\n",
    "\n",
    "# Create label mappings\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Total labels: {len(label_list)}\")\n",
    "print(\"\\nLabel mapping:\")\n",
    "for label, idx in label2id.items():\n",
    "    print(f\"{idx:2d}: {label}\")\n",
    "\n",
    "# Save label mapping\n",
    "label_mapping = {\n",
    "    \"label_to_id\": label2id,\n",
    "    \"id_to_label\": id2label,\n",
    "    \"entity_types\": list(set([label.split('-')[1] for label in label_list if '-' in label]))\n",
    "}\n",
    "\n",
    "with open('label_mapping.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(label_mapping, f, ensure_ascii=False, indent=2)\n",
    "# üè∑Ô∏è Total labels: 13\n",
    "\n",
    "# Label mapping:\n",
    "#  0: O\n",
    "#  1: B-AREA\n",
    "#  2: B-CROP\n",
    "#  3: B-DATE\n",
    "#  4: B-DEVICE\n",
    "#  5: B-DURATION\n",
    "#  6: B-METRIC\n",
    "#  7: I-AREA\n",
    "#  8: I-CROP\n",
    "#  9: I-DATE\n",
    "# 10: I-DEVICE\n",
    "# 11: I-DURATION\n",
    "# 12: I-METRIC\n",
    "\n",
    "print(\"\\n‚úÖ Saved label_mapping.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Step 5: Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "def prepare_dataset(bio_data, label2id):\n",
    "    dataset_dict = {\n",
    "        \"tokens\": [], #danh s√°ch c√°c token\n",
    "        \"ner_tags\": [] #danh s√°ch c√°c nh√£n (s·ªë theo label2id)\n",
    "    }\n",
    "    \n",
    "    for example in bio_data:\n",
    "        dataset_dict[\"tokens\"].append(example[\"tokens\"])\n",
    "        # Convert labels to IDs\n",
    "        tag_ids = [label2id[tag] for tag in example[\"ner_tags\"]]\n",
    "        dataset_dict[\"ner_tags\"].append(tag_ids)\n",
    "    # chuy·ªÉn dict th√†nh dataset obj\n",
    "    #dataset obj l√† ƒë·ªãnh d·∫°ng m√† th∆∞ vi·ªán Transformers y√™u c·∫ßu\n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "#   BIO_DATA C√ì D·∫†NG N√ÄY:\n",
    "#   {\n",
    "#     \"tokens\": [\"ng√¥\", \"ng·ªçt\", \"b·ªã\", \"b·ªánh\", \"g√¨\"],\n",
    "#     \"ner_tags\": [\"B-CROP\", \"I-CROP\", \"O\", \"O\", \"O\"]\n",
    "#   }\n",
    "# Split train/validation (80/20)\n",
    "train_data, val_data = train_test_split(bio_dataset, test_size=0.2, random_state=42)\n",
    "# v√≠ d·ª• v·ªõi 3 m·∫´u \n",
    "# dataset_dict = {\n",
    "#    \"tokens\": [\n",
    "#         [\"ng√¥\", \"ng·ªçt\", \"b·ªã\", \"b·ªánh\", \"g√¨\"],\n",
    "#         [\"B·∫≠t\", \"t∆∞·ªõi\", \"khu\", \"A\"],\n",
    "#         [\"Chi\", \"ph√≠\", \"th√°ng\", \"n√†y\"]\n",
    "#     ],\n",
    "#     \"ner_tags\": [\n",
    "#         [1, 2, 0, 0, 0],           # B-CROP, I-CROP, O, O, O\n",
    "#         [0, 3, 5, 6],              # O, B-DEVICE, B-AREA, I-AREA\n",
    "#         [0, 0, 7, 8]               # O, O, B-DATE, I-DATE\n",
    "#     ]\n",
    "# }\n",
    "train_dataset = prepare_dataset(train_data, label2id)\n",
    "val_dataset = prepare_dataset(val_data, label2id)\n",
    "\n",
    "print(f\"\\nüìä Dataset split:\")\n",
    "print(f\"  Training: {len(train_dataset)} examples\")\n",
    "print(f\"  Validation: {len(val_dataset)} examples\")\n",
    "\"\"\"\n",
    "bio_dataset = [\n",
    "    {\n",
    "        \"tokens\": [\"ng√¥\", \"ng·ªçt\", \"b·ªã\", \"b·ªánh\", \"g√¨\"],\n",
    "        \"ner_tags\": [\"B-CROP\", \"I-CROP\", \"O\", \"O\", \"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"B·∫≠t\", \"t∆∞·ªõi\", \"khu\", \"A\"],\n",
    "        \"ner_tags\": [\"O\", \"B-DEVICE\", \"B-AREA\", \"I-AREA\"]\n",
    "    },\n",
    "    # ... 1998 m·∫´u kh√°c\n",
    "]\n",
    "# M·∫´u 1:\n",
    "tokens: [\"ng√¥\", \"ng·ªçt\", \"b·ªã\", \"b·ªánh\", \"g√¨\"]\n",
    "ner_tags: [\"B-CROP\", \"I-CROP\", \"O\", \"O\", \"O\"]\n",
    "‚Üí tag_ids: [1, 2, 0, 0, 0]\n",
    "# M·∫´u 2:\n",
    "tokens: [\"B·∫≠t\", \"t∆∞·ªõi\", \"khu\", \"A\"]\n",
    "ner_tags: [\"O\", \"B-DEVICE\", \"B-AREA\", \"I-AREA\"]\n",
    "‚Üí tag_ids: [0, 3, 5, 6]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Step 6: Load PhoBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"vinai/phobert-base\"\n",
    "num_labels = len(label_list)\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model loaded on {device}\")\n",
    "print(f\"   Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Step 7: Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PhoBERT tokenizer kh√¥ng support word_ids(), c·∫ßn manual alignment\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Input (word-level):\n",
    "T·ª´:    [\"m√°y\", \"b∆°m\"]\n",
    "Nh√£n:  [B-DEVICE, I-DEVICE]\n",
    "PhoBERT tokenize (subword-level):\n",
    "Token: [\"_m√°y\", \"_b∆°m\"]\n",
    "Nh√£n:  [?, ?]  ‚Üê C·∫ßn g√°n nh√£n\n",
    "\n",
    "Cell 7 th·ª±c hi·ªán 4 b∆∞·ªõc ch√≠nh:\"\n",
    "\n",
    "Gh√©p t·ª´ th√†nh c√¢u ho√†n ch·ªânh\n",
    "\n",
    "Input: [\"B·∫≠t\", \"m√°y\", \"b∆°m\"]\n",
    "Output: \"B·∫≠t m√°y b∆°m\"\n",
    "Tokenization v·ªõi PhoBERT\n",
    "\n",
    "S·ª≠ d·ª•ng PhoBERT tokenizer\n",
    "Tham s·ªë: max_length=128, padding=\"max_length\"\n",
    "Kh·ªüi t·∫°o labels v·ªõi -100\n",
    "\n",
    "-100 = ignore index (b·ªè qua khi t√≠nh loss)\n",
    "D√πng cho special tokens v√† padding\n",
    "Manual Alignment\n",
    "\n",
    "CƒÉn ch·ªânh nh√£n t·ª´ word-level sang token-level\n",
    "\n",
    "V·ªõi m·ªói token:\n",
    "  1. B·ªè qua n·∫øu l√† special token (<s>, </s>, <pad>)\n",
    "  2. Decode token th√†nh text\n",
    "  3. So kh·ªõp v·ªõi t·ª´ hi·ªán t·∫°i (word_idx)\n",
    "  4. N·∫øu kh·ªõp:\n",
    "     - G√°n nh√£n: labels[i] = ner_tags[word_idx]\n",
    "     - N·∫øu h·∫øt t·ª´: word_idx++\n",
    "  5. N·∫øu kh√¥ng kh·ªõp:\n",
    "     - Chuy·ªÉn sang t·ª´ ti·∫øp theo: word_idx++\n",
    "     - Th·ª≠ g√°n nh√£n l·∫°i\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "M·ª•c ƒë√≠ch bi·∫øn d·ªØ li·ªáu NER d·∫°ng word-level(BIO) th√†nh subword-level ƒë·ªÉ PhoBERT c√≥ th·ªÉ train ƒë∆∞·ª£c\n",
    "ƒê·∫ßu v√†o \n",
    "tokens   = [\"ng√¥\", \"ng·ªçt\", \"b·ªã\", \"b·ªánh\", \"g√¨\"]\n",
    "ner_tags = [\"B-CROP\", \"I-CROP\", \"O\", \"O\", \"O\"]\n",
    "PhoBERT ko l√†m vi·ªác vs word m√† v·ªõi subword token \n",
    "<s> _ng√¥ _ng·ªçt _b·ªã _b·ªánh _g√¨ </s> <pad> <pad> ... t·ª©c 1 word c√≥ th·ªÉ b·ªã t√°ch th√†nh nhi·ªÅu subword -> c·∫ßn nh√¢n b·∫£n / cƒÉn ch·ªânh label cho ƒë√∫ng token\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "ƒêang c√≥ data d·∫°ng word-level \n",
    "ng√¥     ‚Üí B-CROP\n",
    "ng·ªçt   ‚Üí I-CROP\n",
    "b·ªã     ‚Üí O\n",
    "b·ªánh   ‚Üí O\n",
    "g√¨     ‚Üí O\n",
    "\n",
    "NH∆ØNG PhoBERT KH√îNG l√†m vi·ªác v·ªõi t·ª´\n",
    "PhoBERT nh√¨n th·∫•y subword token: <s>  _ng√¥  _ng·ªçt  _b·ªã  _b·ªánh  _g√¨  </s> ,\n",
    "Model ch·ªâ bi·∫øt t·ª´ng token,\n",
    "kh√¥ng bi·∫øt ‚Äúng√¥‚Äù l√† m·ªôt t·ª´ ho√†n ch·ªânh.\n",
    "\n",
    "==> N·∫øu 1 t·ª´ b·ªã t√°ch th√†nh 2 token th√¨ label ph·∫£i g√°n cho token n√†o ?\n",
    "\n",
    "TEXT:\n",
    "\"ng√¥ ng·ªçt b·ªã b·ªánh g√¨\"\n",
    "\n",
    "WORD LEVEL:\n",
    "ng√¥     ng·ªçt     b·ªã     b·ªánh     g√¨\n",
    "B-CROP I-CROP   O       O        O\n",
    "\n",
    "SUBWORD LEVEL (PhoBERT):\n",
    "<s> _ng√¥ _ng·ªçt _b·ªã _b  _·ªánh _g√¨ </s>\n",
    "\n",
    "LABEL PH·∫¢I KH·ªöP:\n",
    "-100 B-CROP I-CROP O   O   -100 -100\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"\n",
    "    Tokenize text and align NER labels with subword tokens\n",
    "    PhoBERT tokenizer doesn't support word_ids(), so manual alignment\n",
    "    \"\"\"\n",
    "    # ƒê·ªãnh d·∫°ng chu·∫©n HF Trainer cho NER\n",
    "    tokenized_inputs = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": []\n",
    "    }\n",
    "    # Duy·ªát t·ª´ng sapmle\n",
    "    for tokens, ner_tags in zip(examples[\"tokens\"], examples[\"ner_tags\"]):\n",
    "        # Join tokens back to text v√¨ PhoBERT c·∫ßn c√¢u ho√†n ch·ªânh ƒë·ªÉ tokenize\n",
    "        text = \" \".join(tokens)\n",
    "\n",
    "        # Tokenize the full text\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            truncation=True, #trunc n·∫øu >128 token\n",
    "            max_length=128,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None #tr·∫£ v·ªÅ list python(ko ph·∫£i tensor)\n",
    "        )\n",
    "#         M·∫£nh:    [<s>,  _ng√¥,  _ng,  ·ªçt,  _b·ªã,  </s>]\n",
    "#         S·ªë ID:   [0,    1234,  5678, 9012, 3456, 2]\n",
    "        \"\"\"\n",
    "        encoding = {\n",
    "        # m·ªói s√≥ ƒë·∫°i di·ªán cho 1 token\n",
    "            \"input_ids\": [0, 1234, 5678, 9012, 3456, 7890, 2, 1, 1, 1, ...],  # 128 s·ªë\n",
    "            \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...]             # 128 s·ªë\n",
    "        }   \n",
    "        input_ids: M·ªói s·ªë ƒë·∫°i di·ªán cho 1 token\n",
    "        0: <s> (b·∫Øt ƒë·∫ßu c√¢u)\n",
    "        1234: \"_ng√¥\"\n",
    "        5678: \"_ng\"\n",
    "        9012: \"·ªçt\"\n",
    "        2: </s> (k·∫øt th√∫c c√¢u)\n",
    "        1: <pad> (padding)\n",
    "        attention_mask:\n",
    "        1: Token th·∫≠t\n",
    "        0: Padding (b·ªè qua)\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # Get token IDs\n",
    "        # output ki·ªÉu \n",
    "        #         [\n",
    "        #           [0, 1234, 5678, 9012, 3456, 2, 1, 1, ...]\n",
    "        #         ]\n",
    "        token_ids = encoding[\"input_ids\"]\n",
    "        attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "        # Initialize labels with -100 (ignore index)\n",
    "        labels = [-100] * len(token_ids)\n",
    "\n",
    "        # Manual alignment: match each word to its tokens\n",
    "        current_pos = 0\n",
    "        word_idx = 0\n",
    "        # <s>, _ng√¥, _ng·ªçt, _b·ªã, _b·ªánh, _g√¨, </s>, <pad>, <pad>, ...\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            # Skip special tokens\n",
    "            if token_id in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id]:\n",
    "                continue\n",
    "      \n",
    "            # Decode token\n",
    "            token_text = tokenizer.decode([token_id], skip_special_tokens=True).strip()\n",
    "            \n",
    "            \"\"\"\n",
    "            text = \"ng√¥ ng·ªçt b·ªã\"\n",
    "            tokens = [\"ng√¥\", \"ng·ªçt\", \"b·ªã\"]\n",
    "            ner_tags = [1, 2, 0]  # B-CROP, I-CROP, O\n",
    "            # Sau khi tokenize:\n",
    "            token_ids = [0, 1234, 5678, 9012, 3456, 2, 1, 1, ...]\n",
    "                          ‚Üë   ‚Üë     ‚Üë     ‚Üë     ‚Üë    ‚Üë\n",
    "                        <s> _ng√¥  _ng   ·ªçt   _b·ªã  </s> <pad>\n",
    "            \n",
    "            \"\"\"\n",
    "            # Remove PhoBERT underscore prefix\n",
    "            token_clean = token_text.replace(\"_\", \" \").strip()\n",
    "\n",
    "            if not token_clean:\n",
    "                continue\n",
    "\n",
    "            # Try to match this token to a word\n",
    "            # nh∆∞ for m√† word_idx++ , <len\n",
    "            if word_idx < len(tokens): \n",
    "                word = tokens[word_idx]\n",
    "\n",
    "                # Check if this token is part of the current word\n",
    "                if token_clean.lower() in word.lower() or word.lower().startswith(token_clean.lower()):\n",
    "                    # Assign the label for this word\n",
    "                    labels[i] = ner_tags[word_idx]\n",
    "\n",
    "                    # Check if we've finished this word\n",
    "                    if token_clean.lower() == word.lower():\n",
    "                        word_idx += 1\n",
    "                else:\n",
    "                    # Move to next word\n",
    "                    word_idx += 1\n",
    "                    if word_idx < len(tokens):\n",
    "                        labels[i] = ner_tags[word_idx]\n",
    "\n",
    "        tokenized_inputs[\"input_ids\"].append(encoding[\"input_ids\"])\n",
    "        tokenized_inputs[\"attention_mask\"].append(encoding[\"attention_mask\"])\n",
    "        tokenized_inputs[\"labels\"].append(labels)\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "print(\"Tokenizing training dataset...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenizing validation dataset...\")\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Datasets tokenized\")\n",
    "print(f\"   Training samples: {len(tokenized_train)}\")\n",
    "print(f\"   Validation samples: {len(tokenized_val)}\")\n",
    "\"\"\"\n",
    "tokens = [\"B·∫≠t\", \"m√°y\", \"b∆°m\", \"·ªü\", \"khu\", \"A\", \"trong\", \"30\", \"ph√∫t\"]\n",
    "ner_tags = [0, 1, 2, 0, 3, 4, 0, 5, 6]\n",
    "\n",
    "T·ª´:      B·∫≠t    m√°y    b∆°m    ·ªü    khu    A    trong    30    ph√∫t\n",
    "Tag:     O      B-DEV  I-DEV  O    B-AREA I-AREA O      B-DUR I-DUR\n",
    "S·ªë:      0      1      2      0    3      4      0      5     6\n",
    "\n",
    "text = \" \".join(tokens)\n",
    "# text = \"B·∫≠t m√°y b∆°m ·ªü khu A trong 30 ph√∫t\"\n",
    "\n",
    "token_ids = [\n",
    "    0,      # <s> (b·∫Øt ƒë·∫ßu)\n",
    "    8901,   # _B·∫≠t\n",
    "    2345,   # _m√°y\n",
    "    6789,   # _b∆°m\n",
    "    1234,   # _·ªü\n",
    "    5678,   # _khu\n",
    "    9012,   # _A\n",
    "    3456,   # _trong\n",
    "    7890,   # _30\n",
    "    4567,   # _ph√∫t\n",
    "    2,      # </s> (k·∫øt th√∫c)\n",
    "    1, 1, 1, ...  # <pad> (padding ƒë·∫øn 128)\n",
    "]\n",
    "\n",
    "# Decode l·∫°i th√†nh text:\n",
    "decoded_tokens = [\n",
    "    \"<s>\", \"_B·∫≠t\", \"_m√°y\", \"_b∆°m\", \"_·ªü\", \"_khu\", \"_A\", \n",
    "    \"_trong\", \"_30\", \"_ph√∫t\", \"</s>\", \"<pad>\", \"<pad>\", ...\n",
    "]\n",
    "\n",
    "labels = [-100] * 128  # 128 v·ªã tr√≠, t·∫•t c·∫£ l√† -100 (ignore)\n",
    "\n",
    "word_idx = 0  # B·∫Øt ƒë·∫ßu t·ª´ t·ª´ ƒë·∫ßu ti√™n\n",
    "\n",
    "i = 0\n",
    "token_id = 0\n",
    "# Ki·ªÉm tra special token:\n",
    "if token_id == tokenizer.bos_token_id:  # 0 == 0 ‚Üí TRUE\n",
    "    continue  # B·ªè qua\n",
    "# labels[0] v·∫´n l√† -100\n",
    "\n",
    "token_ids = [\n",
    "    0,      # <s> (b·∫Øt ƒë·∫ßu)\n",
    "    8901,   # _B·∫≠t\n",
    "    2345,   # _m√°y\n",
    "    6789,   # _b∆°m\n",
    "    1234,   # _·ªü\n",
    "    5678,   # _khu\n",
    "    9012,   # _A\n",
    "    3456,   # _trong\n",
    "    7890,   # _30\n",
    "    4567,   # _ph√∫t\n",
    "    2,      # </s> (k·∫øt th√∫c)\n",
    "    1, 1, 1, ...  # <pad> (padding ƒë·∫øn 128)\n",
    "]\n",
    "\n",
    "i = 1\n",
    "token_id = 8901\n",
    "word_idx = 0  # ƒêang x√©t t·ª´ \"B·∫≠t\"\n",
    "# Decode token:\n",
    "token_text = tokenizer.decode([8901]) = \"_B·∫≠t\"\n",
    "token_clean = \"B·∫≠t\"\n",
    "# L·∫•y t·ª´ hi·ªán t·∫°i:\n",
    "word = tokens[0] = \"B·∫≠t\"\n",
    "# Ki·ªÉm tra kh·ªõp:\n",
    "\"B·∫≠t\" in \"B·∫≠t\" ‚Üí TRUE\n",
    "# HO·∫∂C\n",
    "\"B·∫≠t\".startswith(\"B·∫≠t\") ‚Üí TRUE\n",
    "# G√°n nh√£n:\n",
    "labels[1] = ner_tags[0] = 0  # O\n",
    "# Ki·ªÉm tra h·∫øt t·ª´:\n",
    "\"B·∫≠t\" == \"B·∫≠t\" ‚Üí TRUE\n",
    "word_idx = 1  # Chuy·ªÉn sang t·ª´ \"m√°y\"\n",
    "\n",
    "i = 2\n",
    "token_id = 2345\n",
    "word_idx = 1  # ƒêang x√©t t·ª´ \"m√°y\"\n",
    "# Decode token:\n",
    "token_clean = \"m√°y\"\n",
    "# L·∫•y t·ª´ hi·ªán t·∫°i:\n",
    "word = tokens[1] = \"m√°y\"\n",
    "# Ki·ªÉm tra kh·ªõp:\n",
    "\"m√°y\" in \"m√°y\" ‚Üí TRUE\n",
    "# G√°n nh√£n:\n",
    "labels[2] = ner_tags[1] = 1  # B-DEVICE\n",
    "# Ki·ªÉm tra h·∫øt t·ª´:\n",
    "\"m√°y\" == \"m√°y\" ‚Üí TRUE\n",
    "word_idx = 2  # Chuy·ªÉn sang t·ª´ \"b∆°m\"\n",
    "\n",
    "OUTPUT cu·ªëi c√πnG\n",
    "tokenized_inputs = {\n",
    "    \"input_ids\": [\n",
    "        0, 8901, 2345, 6789, 1234, 5678, 9012, 3456, 7890, 4567, 2, 1, 1, ...\n",
    "    ],\n",
    "    \"attention_mask\": [\n",
    "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...\n",
    "    ],\n",
    "    \"labels\": [\n",
    "        -100, 0, 1, 2, 0, 3, 4, 0, 5, 6, -100, -100, -100, ...\n",
    "    ]\n",
    "}\n",
    "\n",
    "ƒê·∫∑c ƒëi·ªÉm:\n",
    "\n",
    "M·ªói token c√≥ 1 nh√£n t∆∞∆°ng ·ª©ng\n",
    "Special tokens v√† padding c√≥ nh√£n -100\n",
    "S·∫µn s√†ng ƒë∆∞a v√†o model ƒë·ªÉ training\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Step 8: Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 8 (thay th·∫ø TrainingArguments)\n",
    "Disable wandb logging\n",
    "\"\"\"\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "# batch = {\n",
    "#     \"input_ids\": [\n",
    "#         [0, 1234, 5678, 2, 1, 1, ...],      # M·∫´u 1\n",
    "#         [0, 9012, 3456, 7890, 2, 1, ...],   # M·∫´u 2\n",
    "#         [0, 2345, 6789, 2, 1, 1, ...]       # M·∫´u 3\n",
    "#     ],  # Shape: (3, 128)\n",
    "    \n",
    "#     \"attention_mask\": [\n",
    "#         [1, 1, 1, 1, 0, 0, ...],\n",
    "#         [1, 1, 1, 1, 1, 0, ...],\n",
    "#         [1, 1, 1, 1, 0, 0, ...]\n",
    "#     ],  # Shape: (3, 128)\n",
    "    \n",
    "#     \"labels\": [\n",
    "#         [-100, 1, 2, -100, -100, ...],\n",
    "#         [-100, 0, 3, 4, -100, ...],\n",
    "#         [-100, 5, 6, -100, -100, ...]\n",
    "#     ]  # Shape: (3, 128)\n",
    "# }\n",
    "import numpy as np\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "# Disable wandb\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# Data collator\n",
    "# Gh√©p nhi·ªÅu sample l·∫°i th√†nh 1 batch sao cho: input_ids, attention_mask, labels\n",
    "# ƒë·ªÅu c√πng chi·ªÅu d√†i\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Metric computation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "# Training arguments (with wandb disabled)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",  # Disable all reporting (wandb, tensorboard, etc.)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Logging: Disabled (no wandb)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Step 9: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nüöÄ Starting training...\\n\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Step 10: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Step 11: Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer\n",
    "output_dir = \"./ner_extractor_final\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# Copy label mapping\n",
    "import shutil\n",
    "shutil.copy('label_mapping.json', f'{output_dir}/label_mapping.json')\n",
    "\n",
    "print(f\"\\n‚úÖ Model saved to {output_dir}\")\n",
    "print(\"\\nüì¶ Files to download:\")\n",
    "print(\"  - config.json\")\n",
    "print(\"  - pytorch_model.bin (or model.safetensors)\")\n",
    "print(\"  - label_mapping.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Step 12: Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new examples\n",
    "test_examples = [\n",
    "    \"B·∫≠t t∆∞·ªõi khu A trong 5 ph√∫t\",\n",
    "    \"ƒê·ªô ·∫©m ·ªü khu B l√† bao nhi√™u\",\n",
    "    \"Chi ph√≠ th√°ng n√†y\",\n",
    "    \"T·∫Øt ƒë√®n khu C\",\n",
    "    \"Nhi·ªát ƒë·ªô khu 1 hi·ªán t·∫°i\",\n",
    "    \"Doanh thu qu√Ω 2\",\n",
    "    \"C√°ch tr·ªìng cam s√†nh\"\n",
    "]\n",
    "\n",
    "def predict_entities(text):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)[0]\n",
    "    \n",
    "    # Decode\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    labels = [id2label[p.item()] for p in predictions]\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token in [\"<s>\", \"</s>\", \"<pad>\"]:\n",
    "            continue\n",
    "            \n",
    "        if label.startswith(\"B-\"):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            current_entity = {\"type\": label[2:], \"text\": token.replace(\"_\", \" \").strip()}\n",
    "        elif label.startswith(\"I-\") and current_entity:\n",
    "            current_entity[\"text\"] += \" \" + token.replace(\"_\", \"\").strip()\n",
    "        elif label == \"O\" and current_entity:\n",
    "            entities.append(current_entity)\n",
    "            current_entity = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "print(\"\\nüß™ Testing model on new examples:\\n\")\n",
    "for example in test_examples:\n",
    "    entities = predict_entities(example)\n",
    "    print(f\"Text: {example}\")\n",
    "    print(f\"Entities: {entities}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Step 13: Download Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip model files for easy download\n",
    "!zip -r ner_model.zip ner_extractor_final/\n",
    "print(\"‚úÖ Model zipped as ner_model.zip\")\n",
    "print(\"\\nüì• Download ner_model.zip from Colab Files panel\")\n",
    "print(\"\\nüìã Deployment instructions:\")\n",
    "print(\"1. Extract ner_model.zip\")\n",
    "print(\"2. Copy files to: C:\\\\Users\\\\ADMIN\\\\Desktop\\\\ex\\\\apps\\\\python-ai-service\\\\models\\\\ner_extractor\\\\\")\n",
    "print(\"3. Restart Python AI service\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
