# Cáº£i Tiáº¿n NER Training Pipeline

## ğŸ¯ Váº¥n Äá» ÄÃ£ Kháº¯c Phá»¥c

### 1. **PhoBERT khÃ´ng há»— trá»£ `return_offsets_mapping`**
**Váº¥n Ä‘á»:** Code cÅ© cá»‘ gá»i `return_offsets_mapping=True` â†’ gÃ¢y lá»—i vá»›i PhoBERT tokenizer

**Giáº£i phÃ¡p:** Táº¡o hÃ m `_align_labels_with_tokens()` Ä‘á»ƒ:
- Táº¡o character-level entity map tá»« annotations
- Decode tá»«ng token PhoBERT
- Match token vá»›i vá»‹ trÃ­ trong text
- GÃ¡n label BIO chÃ­nh xÃ¡c cho má»—i token

### 2. **Label Alignment KhÃ´ng ChÃ­nh XÃ¡c**
**Váº¥n Ä‘á»:** Code cÅ© dÃ¹ng word-level split Ä‘Æ¡n giáº£n â†’ khÃ´ng khá»›p vá»›i PhoBERT tokenization

**Giáº£i phÃ¡p:** 
```python
# Táº¡o character-level entity map
char_labels = ['O'] * len(text)
for entity in entities:
    char_labels[start] = f"B-{entity_type}"
    for i in range(start + 1, end):
        char_labels[i] = f"I-{entity_type}"

# Match tá»«ng token vá»›i character positions
token_start = text.lower().find(token_clean.lower(), current_pos)
label = char_labels[token_start]
```

### 3. **Thiáº¿u Metrics & Evaluation**
**Váº¥n Ä‘á»:** Chá»‰ theo dÃµi loss, khÃ´ng biáº¿t model há»c tá»‘t nhÆ° tháº¿ nÃ o

**Giáº£i phÃ¡p:** ThÃªm `compute_metrics()` vá»›i:
- âœ… **Precision**: Äá»™ chÃ­nh xÃ¡c cá»§a predictions
- âœ… **Recall**: Tá»· lá»‡ entities Ä‘Æ°á»£c phÃ¡t hiá»‡n
- âœ… **F1 Score**: Harmonic mean cá»§a precision & recall

### 4. **Training Configuration ChÆ°a Tá»‘i Æ¯u**
**Cáº£i tiáº¿n:**
```python
# CÅ¨
num_train_epochs=20
learning_rate=3e-5
save_strategy="no"  # KhÃ´ng save checkpoints
metric_for_best_model="eval_loss"

# Má»šI
num_train_epochs=30  # TÄƒng epochs
learning_rate=2e-5  # Giáº£m LR cho stable
save_strategy="epoch"  # Save má»—i epoch
save_total_limit=3  # Giá»¯ 3 best checkpoints
metric_for_best_model="f1"  # DÃ¹ng F1 thay vÃ¬ loss
load_best_model_at_end=True  # Load best model
fp16=True  # Mixed precision (nhanh hÆ¡n 2x)
```

### 5. **ThÃªm Early Stopping**
**Lá»£i Ã­ch:** Tá»± Ä‘á»™ng dá»«ng náº¿u khÃ´ng cáº£i thiá»‡n sau 3 epochs â†’ tiáº¿t kiá»‡m thá»i gian

```python
callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
```

---

## ğŸ“Š Káº¿t Quáº£ Mong Äá»£i

### TrÆ°á»›c Khi Cáº£i Tiáº¿n:
```json
{
  "entities": [
    {"type": "crop_name", "value": "cÃ¡ch"}  âŒ Sai
  ]
}
```

### Sau Khi Cáº£i Tiáº¿n:
```json
{
  "entities": [
    {"type": "crop_name", "value": "cÃ  chua"}  âœ… ÄÃºng
  ]
}
```

---

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### 1. CÃ i Ä‘áº·t dependencies:
```powershell
cd c:\Users\ADMIN\Desktop\ex\apps\python-ai-service
.\venv\Scripts\Activate.ps1
pip install seqeval  # ThÆ° viá»‡n má»›i cáº§n thiáº¿t
```

### 2. Train model vá»›i code má»›i:
```powershell
cd train\scripts
python train_ner.py
```

### 3. Theo dÃµi training:
```
ğŸš€ Starting NER training...
Train examples: 758
Validation examples: 190
Training for 30 epochs with early stopping
Using device: cuda
------------------------------------------------------------

Epoch 1/30:
  train_loss: 0.234
  eval_loss: 0.145
  eval_precision: 0.78
  eval_recall: 0.82
  eval_f1: 0.80  â¬…ï¸ Theo dÃµi metric nÃ y

Epoch 2/30:
  ...
```

### 4. Sau khi training xong:
```powershell
# Restart FastAPI server
python src/main.py
```

### 5. Test:
```bash
POST http://localhost:8000/ner/extract
{
  "text": "cÃ¡ch trá»“ng cÃ  chua"
}

# Expected response:
{
  "entities": [
    {
      "type": "crop_name",
      "value": "cÃ  chua",
      "start": 12,
      "end": 19,
      "confidence": 0.85
    }
  ]
}
```

---

## ğŸ”§ Chi Tiáº¿t Ká»¹ Thuáº­t

### Token Alignment Logic:
```python
def _align_labels_with_tokens(text, entities, input_ids):
    # 1. Táº¡o character-level entity map
    char_labels = ['O'] * len(text)
    for entity in entities:
        char_labels[start] = f"B-{entity_type}"
        for i in range(start+1, end):
            char_labels[i] = f"I-{entity_type}"
    
    # 2. Decode tá»«ng token vÃ  tÃ¬m vá»‹ trÃ­
    for idx, token_id in enumerate(input_ids):
        token = tokenizer.decode([token_id])
        token_clean = token.replace('_', ' ').strip()
        
        # 3. TÃ¬m token trong text
        token_start = text.lower().find(token_clean.lower(), current_pos)
        
        # 4. GÃ¡n label tá»« character map
        label = char_labels[token_start]
        label_ids[idx] = label_to_id[label]
        
        current_pos = token_start + len(token_clean)
```

### VÃ­ dá»¥ cá»¥ thá»ƒ:
```
Text: "cÃ¡ch trá»“ng cÃ  chua"
Entity: {"type": "CROP_NAME", "value": "cÃ  chua", "start": 11, "end": 18}

Character-level map:
[O O O O O O O O O O O B-CROP_NAME I-CROP_NAME I-CROP_NAME ...]
 c Ã¡ c h   t r á»“ n g   c Ã            c            h            u  a

PhoBERT tokens:
[<s>, cÃ¡ch, trá»“ng, cÃ , chua, </s>]

Token labels:
[ignore, O, O, B-CROP_NAME, I-CROP_NAME, ignore]
         â†‘  â†‘  â†‘            â†‘
         Position 0-3: "cÃ¡ch" â†’ O
         Position 5-10: "trá»“ng" â†’ O  
         Position 11-12: "cÃ " â†’ B-CROP_NAME âœ…
         Position 14-17: "chua" â†’ I-CROP_NAME âœ…
```

---

## ğŸ“ˆ Performance Tips

1. **Náº¿u training quÃ¡ lÃ¢u**: Giáº£m `num_train_epochs` xuá»‘ng 20
2. **Náº¿u out of memory**: Giáº£m `per_device_train_batch_size` xuá»‘ng 4
3. **Náº¿u overfitting**: TÄƒng `weight_decay` lÃªn 0.02
4. **Náº¿u muá»‘n nhanh hÆ¡n**: Báº­t `fp16=True` (cáº§n GPU)

---

## âœ… Checklist

- [x] Fix PhoBERT offset mapping issue
- [x] Implement character-level label alignment
- [x] Add F1/Precision/Recall metrics
- [x] Optimize training hyperparameters
- [x] Add early stopping
- [x] Enable mixed precision training
- [x] Save best model based on F1 score

---

## ğŸ‰ Káº¿t Luáº­n

Training pipeline Ä‘Ã£ Ä‘Æ°á»£c **tá»‘i Æ°u hoÃ n toÃ n** cho PhoBERT vÃ  Vietnamese agricultural domain. Model sáº½ há»c Ä‘Æ°á»£c entity boundaries chÃ­nh xÃ¡c vÃ  Ä‘áº¡t F1 score cao hÆ¡n Ä‘Ã¡ng ká»ƒ!
