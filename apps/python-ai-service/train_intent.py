"""
Simple Intent Classification Fine-tuning Script
For Vietnamese Agricultural Chatbot
"""

import pandas as pd
import torch
import os
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments, 
    Trainer
)
from sklearn.model_selection import train_test_split
import numpy as np

# Intent labels mapping
INTENT_LABELS = [
    "financial_query",    # 0
    "crop_query",         # 1
    "device_control",     # 2
    "activity_query",     # 3
    "analytics_query",    # 4
    "farm_query",         # 5
    "sensor_query",       # 6
    "create_record",      # 7
    "update_record",      # 8
    "delete_record",      # 9
]

def create_sample_data():
    """Create sample training data"""
    data = [
        # Financial Query (0)
        ("doanh thu th√°ng n√†y l√† bao nhi√™u", 0),
        ("chi ph√≠ t∆∞·ªõi ti√™u th√°ng 3", 0),
        ("l·ª£i nhu·∫≠n t·ª´ c√† chua", 0),
        ("t·ªïng ti·ªÅn thu ƒë∆∞·ª£c", 0),
        ("gi√° tr·ªã s·∫£n ph·∫©m", 0),
        ("bao nhi√™u ti·ªÅn", 0),
        ("thu nh·∫≠p t·ª´ farm", 0),
        ("chi ph√≠ s·∫£n xu·∫•t", 0),
        
        # Crop Query (1)
        ("c√°ch tr·ªìng c√† chua", 1),
        ("th·ªùi gian thu ho·∫°ch rau", 1),
        ("gi·ªëng c√¢y n√†o t·ªët", 1),
        ("k·ªπ thu·∫≠t tr·ªìng l√∫a", 1),
        ("chƒÉm s√≥c c√¢y tr·ªìng", 1),
        ("h·∫°t gi·ªëng ch·∫•t l∆∞·ª£ng", 1),
        ("c√¢y tr·ªìng ph√π h·ª£p", 1),
        ("thu ho·∫°ch s·∫£n ph·∫©m", 1),
        
        # Device Control (2)
        ("b·∫≠t h·ªá th·ªëng t∆∞·ªõi", 2),
        ("t·∫Øt m√°y b∆°m n∆∞·ªõc", 2),
        ("ƒëi·ªÅu khi·ªÉn c·∫£m bi·∫øn", 2),
        ("ki·ªÉm tra thi·∫øt b·ªã", 2),
        ("b·∫≠t ƒë√®n chi·∫øu s√°ng", 2),
        ("t·∫Øt h·ªá th·ªëng", 2),
        ("ƒëi·ªÅu khi·ªÉn m√°y m√≥c", 2),
        ("ki·ªÉm tra thi·∫øt b·ªã", 2),
        
        # Activity Query (3)
        ("t∆∞·ªõi n∆∞·ªõc cho rau", 3),
        ("b√≥n ph√¢n cho c√¢y", 3),
        ("thu ho·∫°ch s·∫£n ph·∫©m", 3),
        ("chƒÉm s√≥c c√¢y tr·ªìng", 3),
        ("ho·∫°t ƒë·ªông n√¥ng nghi·ªáp", 3),
        ("t∆∞·ªõi ti√™u", 3),
        ("b√≥n ph√¢n", 3),
        ("chƒÉm s√≥c", 3),
        
        # Analytics Query (4)
        ("ph√¢n t√≠ch d·ªØ li·ªáu farm", 4),
        ("th·ªëng k√™ s·∫£n l∆∞·ª£ng", 4),
        ("b√°o c√°o t√†i ch√≠nh", 4),
        ("bi·ªÉu ƒë·ªì tƒÉng tr∆∞·ªüng", 4),
        ("analytics n√¥ng nghi·ªáp", 4),
        ("th·ªëng k√™", 4),
        ("b√°o c√°o", 4),
        ("ph√¢n t√≠ch", 4),
        
        # Farm Query (5)
        ("th√¥ng tin v·ªÅ farm", 5),
        ("d·ªØ li·ªáu trang tr·∫°i", 5),
        ("qu·∫£n l√Ω n√¥ng tr·∫°i", 5),
        ("th√¥ng tin ƒë·∫•t ƒëai", 5),
        ("d·ªØ li·ªáu m√¥i tr∆∞·ªùng", 5),
        ("th√¥ng tin farm", 5),
        ("d·ªØ li·ªáu trang tr·∫°i", 5),
        ("qu·∫£n l√Ω farm", 5),
        
        # Sensor Query (6)
        ("d·ªØ li·ªáu c·∫£m bi·∫øn", 6),
        ("th√¥ng tin nhi·ªát ƒë·ªô", 6),
        ("ƒë·ªô ·∫©m kh√¥ng kh√≠", 6),
        ("d·ªØ li·ªáu m√¥i tr∆∞·ªùng", 6),
        ("th√¥ng tin th·ªùi ti·∫øt", 6),
        ("c·∫£m bi·∫øn", 6),
        ("nhi·ªát ƒë·ªô", 6),
        ("ƒë·ªô ·∫©m", 6),
        
        # Create Record (7)
        ("t·∫°o b·∫£n ghi m·ªõi", 7),
        ("th√™m d·ªØ li·ªáu", 7),
        ("ghi nh·∫≠n ho·∫°t ƒë·ªông", 7),
        ("t·∫°o report", 7),
        ("th√™m th√¥ng tin", 7),
        ("t·∫°o m·ªõi", 7),
        ("th√™m", 7),
        ("ghi nh·∫≠n", 7),
        
        # Update Record (8)
        ("c·∫≠p nh·∫≠t d·ªØ li·ªáu", 8),
        ("s·ª≠a th√¥ng tin", 8),
        ("ch·ªânh s·ª≠a record", 8),
        ("update th√¥ng tin", 8),
        ("thay ƒë·ªïi d·ªØ li·ªáu", 8),
        ("c·∫≠p nh·∫≠t", 8),
        ("s·ª≠a", 8),
        ("ch·ªânh s·ª≠a", 8),
        
        # Delete Record (9)
        ("x√≥a b·∫£n ghi", 9),
        ("x√≥a d·ªØ li·ªáu", 9),
        ("remove record", 9),
        ("x√≥a th√¥ng tin", 9),
        ("delete data", 9),
        ("x√≥a", 9),
        ("remove", 9),
        ("delete", 9),
    ]
    
    return pd.DataFrame(data, columns=['text', 'label'])

class IntentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

def main():
    print("üöÄ Starting Intent Classification Fine-tuning")
    print("=" * 50)
    
    # 1. Create or load data
    if os.path.exists('intent_data.csv'):
        print("üìä Loading existing data...")
        df = pd.read_csv('intent_data.csv')
    else:
        print("üìä Creating sample data...")
        df = create_sample_data()
        df.to_csv('intent_data.csv', index=False)
    
    print(f"Total examples: {len(df)}")
    print(f"Intent distribution:")
    print(df['label'].value_counts().sort_index())
    
    # 2. Split data
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        df['text'].tolist(), 
        df['label'].tolist(), 
        test_size=0.2, 
        random_state=42
    )
    
    print(f"\nTrain examples: {len(train_texts)}")
    print(f"Validation examples: {len(val_texts)}")
    
    # 3. Load model and tokenizer
    print("\nü§ñ Loading PhoBERT model...")
    model_name = "vinai/phobert-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=len(INTENT_LABELS)
    )
    
    # 4. Tokenize data
    print("üî§ Tokenizing data...")
    def tokenize_function(examples):
        return tokenizer(
            examples, 
            truncation=True, 
            padding=True, 
            max_length=256
        )
    
    train_encodings = tokenize_function(train_texts)
    val_encodings = tokenize_function(val_texts)
    
    # 5. Create datasets
    train_dataset = IntentDataset(train_encodings, train_labels)
    val_dataset = IntentDataset(val_encodings, val_labels)
    
    # 6. Training arguments
    training_args = TrainingArguments(
        output_dir='./models/intent_classifier',
        num_train_epochs=3,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=50,
        save_strategy="steps",
        save_steps=50,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to=None,  # Disable wandb
    )
    
    # 7. Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
    )
    
    # 8. Train model
    print("\nüöÄ Starting training...")
    trainer.train()
    
    # 9. Save model
    print("\nüíæ Saving model...")
    trainer.save_model()
    tokenizer.save_pretrained('./models/intent_classifier')
    
    print("\n‚úÖ Training completed!")
    print("Model saved to: ./models/intent_classifier")
    
    # 10. Test model
    print("\nüß™ Testing model...")
    test_cases = [
        "doanh thu th√°ng n√†y l√† bao nhi√™u",
        "c√°ch tr·ªìng c√† chua",
        "b·∫≠t h·ªá th·ªëng t∆∞·ªõi",
        "t∆∞·ªõi n∆∞·ªõc cho rau",
        "ph√¢n t√≠ch d·ªØ li·ªáu farm"
    ]
    
    for text in test_cases:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
        
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(predictions, dim=-1).item()
            confidence = predictions[0][predicted_class].item()
        
        print(f"'{text}' ‚Üí {INTENT_LABELS[predicted_class]} ({confidence:.3f})")

if __name__ == "__main__":
    main()
