"""
Simple Intent Classification Fine-tuning Script
For Vietnamese Agricultural Chatbot

Usage:
    python train_intent.py --data intent_data_augmented.csv
    python train_intent.py --data intent_data.csv --epochs 5
    python train_intent.py --help
"""

import pandas as pd
import torch
import os
import argparse
from pathlib import Path
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments, 
    Trainer,
    EarlyStoppingCallback
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
import numpy as np

# Intent labels mapping (UPDATED: 6 intents only)
INTENT_LABELS = [
    "knowledge_query",    # 0 - H·ªèi ƒë√°p ki·∫øn th·ª©c n√¥ng nghi·ªáp
    "financial_query",    # 1 - H·ªèi v·ªÅ t√†i ch√≠nh
    "analytics_query",    # 2 - Y√™u c·∫ßu ph√¢n t√≠ch
    "device_control",     # 3 - ƒêi·ªÅu khi·ªÉn thi·∫øt b·ªã IoT
    "sensor_query",       # 4 - H·ªèi d·ªØ li·ªáu c·∫£m bi·∫øn
    "unknown",            # 5 - Kh√¥ng x√°c ƒë·ªãnh
]

def create_sample_data():
    """Create sample training data"""
    data = [
        # Financial Query (0)
        ("doanh thu th√°ng n√†y l√† bao nhi√™u", 0),
        ("chi ph√≠ t∆∞·ªõi ti√™u th√°ng 3", 0),
        ("l·ª£i nhu·∫≠n t·ª´ c√† chua", 0),
        ("t·ªïng ti·ªÅn thu ƒë∆∞·ª£c", 0),
        ("gi√° tr·ªã s·∫£n ph·∫©m", 0),
        ("bao nhi√™u ti·ªÅn", 0),
        ("thu nh·∫≠p t·ª´ farm", 0),
        ("chi ph√≠ s·∫£n xu·∫•t", 0),
        
        # Crop Query (1)
        ("c√°ch tr·ªìng c√† chua", 1),
        ("th·ªùi gian thu ho·∫°ch rau", 1),
        ("gi·ªëng c√¢y n√†o t·ªët", 1),
        ("k·ªπ thu·∫≠t tr·ªìng l√∫a", 1),
        ("chƒÉm s√≥c c√¢y tr·ªìng", 1),
        ("h·∫°t gi·ªëng ch·∫•t l∆∞·ª£ng", 1),
        ("c√¢y tr·ªìng ph√π h·ª£p", 1),
        ("thu ho·∫°ch s·∫£n ph·∫©m", 1),
        
        # Device Control (2)
        ("b·∫≠t h·ªá th·ªëng t∆∞·ªõi", 2),
        ("t·∫Øt m√°y b∆°m n∆∞·ªõc", 2),
        ("ƒëi·ªÅu khi·ªÉn c·∫£m bi·∫øn", 2),
        ("ki·ªÉm tra thi·∫øt b·ªã", 2),
        ("b·∫≠t ƒë√®n chi·∫øu s√°ng", 2),
        ("t·∫Øt h·ªá th·ªëng", 2),
        ("ƒëi·ªÅu khi·ªÉn m√°y m√≥c", 2),
        ("ki·ªÉm tra thi·∫øt b·ªã", 2),
        
        # Activity Query (3)
        ("t∆∞·ªõi n∆∞·ªõc cho rau", 3),
        ("b√≥n ph√¢n cho c√¢y", 3),
        ("thu ho·∫°ch s·∫£n ph·∫©m", 3),
        ("chƒÉm s√≥c c√¢y tr·ªìng", 3),
        ("ho·∫°t ƒë·ªông n√¥ng nghi·ªáp", 3),
        ("t∆∞·ªõi ti√™u", 3),
        ("b√≥n ph√¢n", 3),
        ("chƒÉm s√≥c", 3),
        
        # Analytics Query (4)
        ("ph√¢n t√≠ch d·ªØ li·ªáu farm", 4),
        ("th·ªëng k√™ s·∫£n l∆∞·ª£ng", 4),
        ("b√°o c√°o t√†i ch√≠nh", 4),
        ("bi·ªÉu ƒë·ªì tƒÉng tr∆∞·ªüng", 4),
        ("analytics n√¥ng nghi·ªáp", 4),
        ("th·ªëng k√™", 4),
        ("b√°o c√°o", 4),
        ("ph√¢n t√≠ch", 4),
        
        # Farm Query (5)
        ("th√¥ng tin v·ªÅ farm", 5),
        ("d·ªØ li·ªáu trang tr·∫°i", 5),
        ("qu·∫£n l√Ω n√¥ng tr·∫°i", 5),
        ("th√¥ng tin ƒë·∫•t ƒëai", 5),
        ("d·ªØ li·ªáu m√¥i tr∆∞·ªùng", 5),
        ("th√¥ng tin farm", 5),
        ("d·ªØ li·ªáu trang tr·∫°i", 5),
        ("qu·∫£n l√Ω farm", 5),
        
        # Sensor Query (6)
        ("d·ªØ li·ªáu c·∫£m bi·∫øn", 6),
        ("th√¥ng tin nhi·ªát ƒë·ªô", 6),
        ("ƒë·ªô ·∫©m kh√¥ng kh√≠", 6),
        ("d·ªØ li·ªáu m√¥i tr∆∞·ªùng", 6),
        ("th√¥ng tin th·ªùi ti·∫øt", 6),
        ("c·∫£m bi·∫øn", 6),
        ("nhi·ªát ƒë·ªô", 6),
        ("ƒë·ªô ·∫©m", 6),
        
        # Create Record (7)
        ("t·∫°o b·∫£n ghi m·ªõi", 7),
        ("th√™m d·ªØ li·ªáu", 7),
        ("ghi nh·∫≠n ho·∫°t ƒë·ªông", 7),
        ("t·∫°o report", 7),
        ("th√™m th√¥ng tin", 7),
        ("t·∫°o m·ªõi", 7),
        ("th√™m", 7),
        ("ghi nh·∫≠n", 7),
        
        # Update Record (8)
        ("c·∫≠p nh·∫≠t d·ªØ li·ªáu", 8),
        ("s·ª≠a th√¥ng tin", 8),
        ("ch·ªânh s·ª≠a record", 8),
        ("update th√¥ng tin", 8),
        ("thay ƒë·ªïi d·ªØ li·ªáu", 8),
        ("c·∫≠p nh·∫≠t", 8),
        ("s·ª≠a", 8),
        ("ch·ªânh s·ª≠a", 8),
        
        # Delete Record (9)
        ("x√≥a b·∫£n ghi", 9),
        ("x√≥a d·ªØ li·ªáu", 9),
        ("remove record", 9),
        ("x√≥a th√¥ng tin", 9),
        ("delete data", 9),
        ("x√≥a", 9),
        ("remove", 9),
        ("delete", 9),
    ]
    
    return pd.DataFrame(data, columns=['text', 'label'])

class IntentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


def compute_metrics(pred):
    """Compute accuracy and F1 score for evaluation"""
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average='weighted')
    
    return {
        'accuracy': acc,
        'f1': f1
    }


def load_data(data_path: str) -> pd.DataFrame:
    """Load training data from CSV file"""
    if not Path(data_path).exists():
        raise FileNotFoundError(f"Data file not found: {data_path}")
    
    print(f"üìä Loading data from: {data_path}")
    df = pd.read_csv(data_path)
    
    # Validate required columns
    if 'text' not in df.columns or 'label' not in df.columns:
        raise ValueError("CSV file must contain 'text' and 'label' columns")
    
    # Clean data
    initial_len = len(df)
    df = df.dropna(subset=['text', 'label'])
    df = df.drop_duplicates(subset=['text'])
    
    if len(df) < initial_len:
        print(f"   Cleaned data: {len(df)} samples (removed {initial_len - len(df)} duplicates/NaN)")
    
    return df


def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(
        description="Train Intent Classification Model",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    parser.add_argument(
        '--data',
        type=str,
        default='train/data/intent_data_augmented.csv',
        help='Path to training data CSV file (must have text and label columns)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='models/intent_classifier',
        help='Output directory for trained model'
    )
    
    parser.add_argument(
        '--epochs',
        type=int,
        default=10,
        help='Number of training epochs'
    )
    
    parser.add_argument(
        '--batch-size',
        type=int,
        default=16,
        help='Batch size for training'
    )
    
    parser.add_argument(
        '--learning-rate',
        type=float,
        default=2e-5,
        help='Learning rate'
    )
    
    parser.add_argument(
        '--test-size',
        type=float,
        default=0.2,
        help='Validation split ratio (0.0 to 1.0)'
    )
    
    parser.add_argument(
        '--seed',
        type=int,
        default=42,
        help='Random seed for reproducibility'
    )
    
    parser.add_argument(
        '--early-stopping-patience',
        type=int,
        default=3,
        help='Early stopping patience (stop if no improvement for N epochs)'
    )
    
    args = parser.parse_args()
    
    # Set random seed
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    print("üöÄ Starting Intent Classification Fine-tuning")
    print("=" * 60)
    print(f"üìÅ Data file: {args.data}")
    print(f"üìÇ Output dir: {args.output}")
    print(f"üîÑ Epochs: {args.epochs}")
    print(f"üì¶ Batch size: {args.batch_size}")
    print(f"üìà Learning rate: {args.learning_rate}")
    print(f"üõë Early stopping patience: {args.early_stopping_patience} epochs")
    print("=" * 60)
    
    # 1. Load data
    try:
        df = load_data(args.data)
    except (FileNotFoundError, ValueError) as e:
        print(f"‚ùå Error loading data: {e}")
        
        # Fallback to sample data if file not found
        if "not found" in str(e).lower():
            print("üìä Creating sample data as fallback...")
            df = create_sample_data()
            sample_path = 'intent_data_sample.csv'
            df.to_csv(sample_path, index=False)
            print(f"   Sample data saved to: {sample_path}")
        else:
            return 1
    
    print(f"‚úì Total examples: {len(df)}")
    
    # Show label distribution
    label_counts = df['label'].value_counts().sort_index()
    print(f"‚úì Intent distribution:")
    for label, count in label_counts.items():
        print(f"   {label}: {count} samples")
    
    # Map string labels to integers if needed
    if df['label'].dtype == 'object':
        unique_labels = sorted(df['label'].unique())
        label_to_id = {label: idx for idx, label in enumerate(unique_labels)}
        df['label_id'] = df['label'].map(label_to_id)
        print(f"‚úì Mapped {len(unique_labels)} string labels to integers")
        label_column = 'label_id'
        num_labels = len(unique_labels)
    else:
        label_column = 'label'
        num_labels = len(INTENT_LABELS)
    
    # 2. Split data
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        df['text'].tolist(), 
        df[label_column].tolist(), 
        test_size=args.test_size, 
        random_state=args.seed,
        stratify=df[label_column].tolist() if len(df) > 10 else None
    )
    
    print(f"\n‚úÇÔ∏è  Data split:")
    print(f"   Train examples: {len(train_texts)}")
    print(f"   Validation examples: {len(val_texts)}")
    
    # 3. Load model and tokenizer
    print(f"\nü§ñ Loading PhoBERT model...")
    model_name = "vinai/phobert-base"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForSequenceClassification.from_pretrained(
        model_name,
        num_labels=num_labels
    )
    print(f"   Model loaded with {num_labels} output labels")
    
    # 4. Tokenize data
    print("üî§ Tokenizing data...")
    def tokenize_function(examples):
        return tokenizer(
            examples, 
            truncation=True, 
            padding=True, 
            max_length=256
        )
    
    train_encodings = tokenize_function(train_texts)
    val_encodings = tokenize_function(val_texts)
    print("   Tokenization completed")
    
    # 5. Create datasets
    print("üì¶ Creating datasets...")
    train_dataset = IntentDataset(train_encodings, train_labels)
    val_dataset = IntentDataset(val_encodings, val_labels)
    print("   Datasets created")
    
    # 6. Training arguments
    print(f"\n‚öôÔ∏è  Setting up training...")
    
    # Create output directory
    Path(args.output).mkdir(parents=True, exist_ok=True)
    
    training_args = TrainingArguments(
        output_dir=args.output,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        warmup_ratio=0.1,
        weight_decay=0.01,
        
        # Logging
        logging_dir=f'{args.output}/logs',
        logging_strategy="epoch",
        
        # Evaluation
        evaluation_strategy="epoch",
        
        # Saving
        save_strategy="epoch",
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="loss",  # Theo d√µi eval_loss
        greater_is_better=False,  # Loss c√†ng th·∫•p c√†ng t·ªët
        
        # Performance
        fp16=torch.cuda.is_available(),
        dataloader_num_workers=0,
        
        # Misc
        report_to=None,  # Disable wandb
        seed=args.seed,
    )
    
    print(f"   Output directory: {args.output}")
    print(f"   Training epochs: {args.epochs}")
    print(f"   Batch size: {args.batch_size}")
    print(f"   Learning rate: {args.learning_rate}")
    print(f"   Metric for best model: eval_loss (lower is better)")
    print(f"   Early stopping patience: {args.early_stopping_patience} epochs")
    
    # 7. Create trainer with early stopping
    print("üéØ Creating trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.early_stopping_patience)],
    )
    print(f"   Trainer created with early stopping (patience={args.early_stopping_patience})")
    
    # 8. Train model
    print("\n" + "=" * 60)
    print("üöÄ STARTING TRAINING...")
    print("=" * 60)
    trainer.train()
    
    # 9. Evaluate final model
    print("\n" + "=" * 60)
    print("üìä FINAL EVALUATION")
    print("=" * 60)
    
    eval_results = trainer.evaluate()
    print(f"\n‚úì Final Results:")
    print(f"   Accuracy: {eval_results['eval_accuracy']:.4f}")
    print(f"   F1 Score: {eval_results['eval_f1']:.4f}")
    print(f"   Loss: {eval_results['eval_loss']:.4f}")
    
    # 10. Save model
    print(f"\nüíæ Saving model to: {args.output}")
    trainer.save_model()
    tokenizer.save_pretrained(args.output)
    
    # Save label mapping if we created one
    if df['label'].dtype == 'object':
        import json
        label_mapping_path = Path(args.output) / 'label_mapping.json'
        with open(label_mapping_path, 'w', encoding='utf-8') as f:
            json.dump({
                'label_to_id': label_to_id,
                'id_to_label': {v: k for k, v in label_to_id.items()},
                'num_labels': num_labels
            }, f, ensure_ascii=False, indent=2)
        print(f"   Label mapping saved to: {label_mapping_path}")
    
    print("\n‚úÖ Training completed successfully!")
    
    # 11. Test model with sample queries
    print("\nüß™ Testing model with sample queries...")
    test_cases = [
        "doanh thu th√°ng n√†y l√† bao nhi√™u",
        "c√°ch tr·ªìng c√† chua", 
        "b·∫≠t h·ªá th·ªëng t∆∞·ªõi",
        "t∆∞·ªõi n∆∞·ªõc cho rau",
        "ph√¢n t√≠ch d·ªØ li·ªáu farm"
    ]
    
    for text in test_cases:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
        
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
            predicted_class = torch.argmax(predictions, dim=-1).item()
            confidence = predictions[0][predicted_class].item()
        
        # Get label name
        if df['label'].dtype == 'object':
            predicted_label = [k for k, v in label_to_id.items() if v == predicted_class][0]
        else:
            predicted_label = INTENT_LABELS[predicted_class] if predicted_class < len(INTENT_LABELS) else f"class_{predicted_class}"
            
        print(f"   '{text}' ‚Üí {predicted_label} ({confidence:.3f})")
    
    print("\n" + "=" * 60)
    print("üéâ ALL DONE!")
    print("=" * 60)
    
    return 0


if __name__ == "__main__":
    exit(main())
