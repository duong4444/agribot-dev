"""
Test Fine-tuned Intent Classification Model
"""

import torch
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Intent labels
INTENT_LABELS = [
    "financial_query",    # 0
    "crop_query",         # 1
    "device_control",     # 2
    "activity_query",     # 3
    "analytics_query",    # 4
    "farm_query",         # 5
    "sensor_query",       # 6
    "create_record",      # 7
    "update_record",      # 8
    "delete_record",      # 9
]

def test_intent(text, model, tokenizer):
    """Test intent classification for a single text"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=256)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(predictions, dim=-1).item()
        confidence = predictions[0][predicted_class].item()
    
    return INTENT_LABELS[predicted_class], confidence

def main():
    print("ğŸ§ª Testing Fine-tuned Intent Classification Model")
    print("=" * 60)
    
    # Check if model exists
    model_path = "./models/intent_classifier"
    if not os.path.exists(model_path):
        print("âŒ Fine-tuned model not found!")
        print("Please run: python train_intent.py")
        return
    
    # Load model
    print("ğŸ¤– Loading fine-tuned model...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        print("âœ… Model loaded successfully!")
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return
    
    # Test cases
    test_cases = [
        "doanh thu thÃ¡ng nÃ y lÃ  bao nhiÃªu",
        "chi phÃ­ tÆ°á»›i tiÃªu thÃ¡ng 3",
        "cÃ¡ch trá»“ng cÃ  chua",
        "thá»i gian thu hoáº¡ch rau",
        "báº­t há»‡ thá»‘ng tÆ°á»›i",
        "táº¯t mÃ¡y bÆ¡m nÆ°á»›c",
        "tÆ°á»›i nÆ°á»›c cho rau",
        "bÃ³n phÃ¢n cho cÃ¢y",
        "phÃ¢n tÃ­ch dá»¯ liá»‡u farm",
        "thá»‘ng kÃª sáº£n lÆ°á»£ng",
        "thÃ´ng tin vá» farm",
        "dá»¯ liá»‡u trang tráº¡i",
        "dá»¯ liá»‡u cáº£m biáº¿n",
        "thÃ´ng tin nhiá»‡t Ä‘á»™",
        "táº¡o báº£n ghi má»›i",
        "thÃªm dá»¯ liá»‡u",
        "cáº­p nháº­t dá»¯ liá»‡u",
        "sá»­a thÃ´ng tin",
        "xÃ³a báº£n ghi",
        "remove record"
    ]
    
    print(f"\nğŸ§ª Testing {len(test_cases)} examples:")
    print("-" * 60)
    
    correct = 0
    total = len(test_cases)
    
    for i, text in enumerate(test_cases, 1):
        intent, confidence = test_intent(text, model, tokenizer)
        
        # Simple accuracy check (you can improve this)
        is_correct = confidence > 0.5  # Basic threshold
        if is_correct:
            correct += 1
        
        status = "âœ…" if is_correct else "âŒ"
        print(f"{i:2d}. {status} '{text}'")
        print(f"    â†’ {intent} ({confidence:.3f})")
        print()
    
    accuracy = (correct / total) * 100
    print(f"ğŸ“Š Results:")
    print(f"   Correct: {correct}/{total}")
    print(f"   Accuracy: {accuracy:.1f}%")
    
    if accuracy > 80:
        print("ğŸ‰ Great! Model is working well!")
    elif accuracy > 60:
        print("ğŸ‘ Good! Model needs more training data.")
    else:
        print("âš ï¸  Model needs improvement. Try more training data.")

if __name__ == "__main__":
    main()
