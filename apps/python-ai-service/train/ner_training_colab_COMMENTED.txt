================================================================================
NOTEBOOK HU·∫§N LUY·ªÜN NER MODEL - PHI√äN B·∫¢N C√ì COMMENT CHI TI·∫æT
================================================================================
M·ª•c ƒë√≠ch: Gi·∫£i th√≠ch B·∫¢N CH·∫§T t·ª´ng d√≤ng code ƒë·ªÉ hi·ªÉu s√¢u v·ªÅ Token Classification
================================================================================

cell code 1: C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN
==============================================================================================
# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
!pip install transformers datasets torch scikit-learn seqeval pandas -q

# B·∫¢N CH·∫§T T·ª™NG TH∆Ø VI·ªÜN:
# - transformers: Th∆∞ vi·ªán Hugging Face ch·ª©a PhoBERT v√† c√¥ng c·ª• Fine-tune
# - datasets: X·ª≠ l√Ω dataset (format ƒë·∫∑c bi·ªát c·ªßa Hugging Face)
# - torch: PyTorch - Framework Deep Learning
# - scikit-learn: Chia train/val/test
# - seqeval: Th∆∞ vi·ªán QUAN TR·ªåNG ƒë·ªÉ ƒë√°nh gi√° NER (Entity-level metrics)
#   T·∫°i sao kh√¥ng d√πng sklearn? V√¨ sklearn t√≠nh Accuracy t·ª´ng t·ª´ (sai l·ªách)
#   Seqeval t√≠nh F1 tr√™n c·∫•p ƒë·ªô th·ª±c th·ªÉ (ƒë√∫ng c·∫£ c·ª•m t·ª´ m·ªõi ƒë∆∞·ª£c t√≠nh)
# - pandas: X·ª≠ l√Ω CSV

==============================================================================================

cell code 2: UPLOAD V√Ä LOAD D·ªÆ LI·ªÜU
==============================================================================================
import json
import pandas as pd
from typing import List, Dict
from google.colab import files

# Upload CSV file (ƒë∆∞·ª£c sinh ra t·ª´ generate_ner_data_v2.py)
print("üì§ Please upload your CSV file (generated by generate_ner_data_v2.py)")
uploaded = files.upload()

# L·∫•y t√™n file
csv_filename = list(uploaded.keys())[0]
print(f"\nüìÇ Loading data from {csv_filename}...")

# ƒê·ªçc CSV
df = pd.read_csv(csv_filename)
# B·∫¢N CH·∫§T: CSV c√≥ 2 c·ªôt:
# - 'text': C√¢u vƒÉn (VD: "B·∫≠t ƒë√®n khu A")
# - 'entities': JSON string ch·ª©a danh s√°ch th·ª±c th·ªÉ
#   VD: '[{"start": 4, "end": 7, "type": "DEVICE", "value": "ƒë√®n"}]'

# Chuy·ªÉn ƒë·ªïi sang format training: [(text, [(start, end, label), ...]), ...]
training_data = []

for _, row in df.iterrows():
    text = row['text']  # L·∫•y c√¢u vƒÉn
    entities_json = json.loads(row['entities'])  # Parse JSON string -> List
    # B·∫¢N CH·∫§T: entities_json = [{"start": 4, "end": 7, "type": "DEVICE", ...}]
    
    # Chuy·ªÉn th√†nh tuple (start, end, type) - B·ªè value v√¨ kh√¥ng c·∫ßn
    entities = [(e['start'], e['end'], e['type']) for e in entities_json]
    # VD: [(4, 7, 'DEVICE'), (12, 17, 'AREA')]
    
    training_data.append((text, entities))

print(f"\n‚úÖ Loaded {len(training_data)} training examples from CSV")

# Hi·ªÉn th·ªã m·∫´u
print("\nüìù Sample data:")
for i, (text, entities) in enumerate(training_data[:5]):
    print(f"\n{i+1}. Text: {text}")
    print(f"   Entities: {entities}")
# VD Output:
# 1. Text: ng√¥ ng·ªçt b·ªã b·ªánh g√¨
#    Entities: [(0, 8, 'CROP')]  # "ng√¥ ng·ªçt" t·ª´ v·ªã tr√≠ 0 ƒë·∫øn 8

==============================================================================================

cell code 3: CHUY·ªÇN ƒê·ªîI SANG BIO FORMAT (B∆∞·ªõc kh√≥ nh·∫•t!)
==============================================================================================
def convert_to_bio_format(data: List[tuple]) -> List[Dict]:
    """
    Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu annotation sang BIO format
    B·∫¢N CH·∫§T: BIO = Beginning-Inside-Outside
    - B-CROP: T·ª´ ƒë·∫ßu ti√™n c·ªßa c√¢y tr·ªìng
    - I-CROP: T·ª´ ti·∫øp theo c·ªßa c√¢y tr·ªìng
    - O: Kh√¥ng ph·∫£i th·ª±c th·ªÉ
    """
    bio_data = []
    
    for text, entities in data:
        # ===== B∆Ø·ªöC 1: T√ÅCH T·ª™ =====
        words = text.split()  # T√°ch c√¢u th√†nh danh s√°ch t·ª´
        # VD: "ng√¥ ng·ªçt b·ªã b·ªánh" -> ["ng√¥", "ng·ªçt", "b·ªã", "b·ªánh"]
        
        labels = ['O'] * len(words)  # Kh·ªüi t·∫°o t·∫•t c·∫£ nh√£n l√† 'O'
        # VD: ['O', 'O', 'O', 'O']
        
        # ===== B∆Ø·ªöC 2: T·∫†O MAPPING T·ª™ V·ªä TR√ç K√ù T·ª∞ -> INDEX T·ª™ =====
        # B·∫¢N CH·∫§T: V√¨ entities c√≥ (start, end) theo k√Ω t·ª±, nh∆∞ng ta c·∫ßn g√°n nh√£n cho T·ª™
        # N√™n ph·∫£i t·∫°o b·∫£ng tra: K√Ω t·ª± th·ª© X thu·ªôc v·ªÅ t·ª´ th·ª© Y
        char_to_word = {}
        current_pos = 0
        
        for word_idx, word in enumerate(words):
            # T√¨m v·ªã tr√≠ b·∫Øt ƒë·∫ßu c·ªßa t·ª´ trong c√¢u g·ªëc
            word_start = text.find(word, current_pos)
            word_end = word_start + len(word)
            
            # G√°n t·∫•t c·∫£ k√Ω t·ª± trong t·ª´ n√†y v√†o word_idx
            for char_idx in range(word_start, word_end):
                char_to_word[char_idx] = word_idx
            # VD: char_to_word = {0: 0, 1: 0, 2: 0, 3: 1, 4: 1, ...}
            #     K√Ω t·ª± 0,1,2 thu·ªôc t·ª´ 0 ("ng√¥")
            #     K√Ω t·ª± 3,4 thu·ªôc t·ª´ 1 ("ng·ªçt")
            
            current_pos = word_end
        
        # ===== B∆Ø·ªöC 3: G√ÅN NH√ÉN BIO =====
        for start, end, entity_type in entities:
            # T√¨m t·∫•t c·∫£ c√°c t·ª´ n·∫±m trong kho·∫£ng [start, end)
            entity_words = set()
            for char_idx in range(start, end):
                if char_idx in char_to_word:
                    entity_words.add(char_to_word[char_idx])
            # VD: N·∫øu entity l√† "ng√¥ ng·ªçt" (start=0, end=8)
            #     -> entity_words = {0, 1} (t·ª´ th·ª© 0 v√† 1)
            
            entity_words = sorted(entity_words)  # S·∫Øp x·∫øp theo th·ª© t·ª±
            
            if entity_words:
                # T·ª´ ƒë·∫ßu ti√™n g√°n nh√£n B- (Beginning)
                labels[entity_words[0]] = f"B-{entity_type}"
                # VD: labels[0] = "B-CROP"
                
                # C√°c t·ª´ ti·∫øp theo g√°n nh√£n I- (Inside)
                for word_idx in entity_words[1:]:
                    labels[word_idx] = f"I-{entity_type}"
                # VD: labels[1] = "I-CROP"
        
        # L∆∞u k·∫øt qu·∫£
        bio_data.append({
            "tokens": words,      # ["ng√¥", "ng·ªçt", "b·ªã", "b·ªánh"]
            "ner_tags": labels    # ["B-CROP", "I-CROP", "O", "O"]
        })
    
    return bio_data

# Chuy·ªÉn ƒë·ªïi to√†n b·ªô d·ªØ li·ªáu
bio_dataset = convert_to_bio_format(training_data)

# Hi·ªÉn th·ªã v√≠ d·ª•
print("\nüìù Example BIO format:")
example = bio_dataset[0]
for token, tag in zip(example['tokens'], example['ner_tags']):
    print(f"{token:20} ‚Üí {tag}")
# Output:
# ng√¥                  ‚Üí B-CROP
# ng·ªçt                 ‚Üí I-CROP
# b·ªã                   ‚Üí O
# b·ªánh                 ‚Üí O
# g√¨                   ‚Üí O

print(f"\n‚úÖ Converted {len(bio_dataset)} examples to BIO format")

==============================================================================================

cell code 4: T·∫†O LABEL MAPPING
==============================================================================================
# Tr√≠ch xu·∫•t t·∫•t c·∫£ nh√£n duy nh·∫•t
all_labels = set()
for example in bio_dataset:
    all_labels.update(example['ner_tags'])
# B·∫¢N CH·∫§T: all_labels = {'O', 'B-CROP', 'I-CROP', 'B-DEVICE', 'I-DEVICE', ...}

# S·∫Øp x·∫øp nh√£n (O ƒë·∫ßu ti√™n, sau ƒë√≥ B-, r·ªìi I-)
label_list = sorted(all_labels, key=lambda x: (x != 'O', x))
# B·∫¢N CH·∫§T: ['O', 'B-AREA', 'B-CROP', ..., 'I-AREA', 'I-CROP', ...]

# T·∫°o mapping: Nh√£n -> S·ªë
label2id = {label: idx for idx, label in enumerate(label_list)}
# VD: {'O': 0, 'B-CROP': 1, 'I-CROP': 2, ...}

# T·∫°o mapping ng∆∞·ª£c: S·ªë -> Nh√£n
id2label = {idx: label for label, idx in label2id.items()}
# VD: {0: 'O', 1: 'B-CROP', 2: 'I-CROP', ...}

print(f"\nüè∑Ô∏è Total labels: {len(label_list)}")
print("\nLabel mapping:")
for label, idx in label2id.items():
    print(f"{idx:2d}: {label}")

# L∆∞u label mapping (QUAN TR·ªåNG cho deployment)
label_mapping = {
    "label_to_id": label2id,
    "id_to_label": id2label,
    "entity_types": list(set([label.split('-')[1] for label in label_list if '-' in label]))
    # VD: entity_types = ['CROP', 'DEVICE', 'AREA', 'DATE', ...]
}

with open('label_mapping.json', 'w', encoding='utf-8') as f:
    json.dump(label_mapping, f, ensure_ascii=False, indent=2)

print("\n‚úÖ Saved label_mapping.json")

==============================================================================================

cell code 5: CHU·∫®N B·ªä DATASET
==============================================================================================
from datasets import Dataset
from sklearn.model_selection import train_test_split

# Chuy·ªÉn ƒë·ªïi sang format Hugging Face Dataset
def prepare_dataset(bio_data, label2id):
    """
    B·∫¢N CH·∫§T: Hugging Face Dataset y√™u c·∫ßu format ƒë·∫∑c bi·ªát (dictionary of lists)
    """
    dataset_dict = {
        "tokens": [],    # List of list of strings
        "ner_tags": []   # List of list of integers
    }
    
    for example in bio_data:
        dataset_dict["tokens"].append(example["tokens"])
        
        # Chuy·ªÉn nh√£n ch·ªØ -> s·ªë
        tag_ids = [label2id[tag] for tag in example["ner_tags"]]
        # VD: ["B-CROP", "I-CROP", "O"] -> [1, 2, 0]
        dataset_dict["ner_tags"].append(tag_ids)
    
    return Dataset.from_dict(dataset_dict)

# Chia train/validation (80/20)
train_data, val_data = train_test_split(bio_dataset, test_size=0.2, random_state=42)
# B·∫¢N CH·∫§T: Kh√¥ng d√πng stratify v√¨ NER ph·ª©c t·∫°p h∆°n (m·ªói c√¢u c√≥ nhi·ªÅu nh√£n)

train_dataset = prepare_dataset(train_data, label2id)
val_dataset = prepare_dataset(val_data, label2id)

print(f"\nüìä Dataset split:")
print(f"  Training: {len(train_dataset)} examples")
print(f"  Validation: {len(val_dataset)} examples")

==============================================================================================

cell code 6: LOAD MODEL
==============================================================================================
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

model_name = "vinai/phobert-base"
num_labels = len(label_list)  # VD: 13 nh√£n (O + 6 B- + 6 I-)

print(f"Loading {model_name}...")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load model
model = AutoModelForTokenClassification.from_pretrained(
    model_name,
    num_labels=num_labels,  # S·ªë l∆∞·ª£ng nh√£n
    id2label=id2label,      # Mapping s·ªë -> nh√£n (ƒë·ªÉ debug)
    label2id=label2id       # Mapping nh√£n -> s·ªë
)
# B·∫¢N CH·∫§T: H√†m n√†y l√†m:
# 1. T·∫£i PhoBERT g·ªëc (12 l·ªõp Transformer)
# 2. C·∫ÆT B·ªé Head c≈© (Masked LM)
# 3. G·∫ÆN Head m·ªõi: Linear Layer 768 -> 13
#    Kh√°c v·ªõi Intent (1 output cho c·∫£ c√¢u), NER c√≥ 1 output cho M·ªñI TOKEN

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

print(f"‚úÖ Model loaded on {device}")
print(f"   Number of labels: {num_labels}")

==============================================================================================

cell code 7: TOKENIZATION & ALIGNMENT (Ph·∫ßn kh√≥ nh·∫•t c·ªßa NER!)
==============================================================================================
"""
QUAN TR·ªåNG: PhoBERT tokenizer kh√¥ng h·ªó tr·ª£ word_ids()
N√™n ph·∫£i t·ª± vi·∫øt logic alignment (cƒÉn ch·ªânh nh√£n)
"""

def tokenize_and_align_labels(examples):
    """
    Tokenize text v√† cƒÉn ch·ªânh nh√£n NER v·ªõi subword tokens
    
    V·∫§N ƒê·ªÄ: PhoBERT t√°ch t·ª´ th√†nh subword
    VD: "chi ph√≠" -> ["chi", "_ph√≠"] (2 tokens)
    Nh∆∞ng ta ch·ªâ c√≥ 1 nh√£n "B-METRIC" cho "chi ph√≠"
    
    GI·∫¢I PH√ÅP: G√°n nh√£n cho token ƒë·∫ßu ti√™n, c√°c token sau g√°n -100 (ignore)
    """
    tokenized_inputs = {
        "input_ids": [],
        "attention_mask": [],
        "labels": []
    }
    
    for tokens, ner_tags in zip(examples["tokens"], examples["ner_tags"]):
        # ===== B∆Ø·ªöC 1: N·ªêI T·ª™ TH√ÄNH C√ÇU =====
        text = " ".join(tokens)
        # VD: ["B·∫≠t", "ƒë√®n", "khu", "A"] -> "B·∫≠t ƒë√®n khu A"
        
        # ===== B∆Ø·ªöC 2: TOKENIZE TO√ÄN B·ªò C√ÇU =====
        encoding = tokenizer(
            text,
            truncation=True,
            max_length=128,  # NER th∆∞·ªùng d√πng c√¢u ng·∫Øn h∆°n Intent
            padding="max_length",
            return_tensors=None  # Tr·∫£ v·ªÅ list, kh√¥ng ph·∫£i Tensor
        )
        
        token_ids = encoding["input_ids"]  # VD: [0, 5, 234, 1890, 102, 1, 1, ...]
        attention_mask = encoding["attention_mask"]  # VD: [1, 1, 1, 1, 1, 0, 0, ...]
        
        # ===== B∆Ø·ªöC 3: KH·ªûI T·∫†O LABELS V·ªöI -100 =====
        labels = [-100] * len(token_ids)
        # B·∫¢N CH·∫§T: -100 l√† gi√° tr·ªã ƒë·∫∑c bi·ªát trong PyTorch CrossEntropyLoss
        # N√≥ s·∫Ω b·ªã B·ªé QUA khi t√≠nh Loss (kh√¥ng b·ªã ph·∫°t)
        # D√πng cho: Special tokens (<s>, </s>, <pad>) v√† Subword tokens
        
        # ===== B∆Ø·ªöC 4: MANUAL ALIGNMENT =====
        # V√¨ PhoBERT kh√¥ng c√≥ word_ids(), ta ph·∫£i t·ª± match
        word_idx = 0  # Index c·ªßa t·ª´ hi·ªán t·∫°i
        
        for i, token_id in enumerate(token_ids):
            # B·ªè qua special tokens
            if token_id in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.pad_token_id]:
                # B·∫¢N CH·∫§T:
                # - bos_token_id: <s> (token b·∫Øt ƒë·∫ßu)
                # - eos_token_id: </s> (token k·∫øt th√∫c)
                # - pad_token_id: <pad> (padding)
                continue
            
            # Decode token v·ªÅ ch·ªØ
            token_text = tokenizer.decode([token_id], skip_special_tokens=True).strip()
            # VD: token_id=234 -> "ƒë√®n"
            
            # X√≥a d·∫•u g·∫°ch d∆∞·ªõi c·ªßa PhoBERT
            token_clean = token_text.replace("_", " ").strip()
            # B·∫¢N CH·∫§T: PhoBERT th√™m "_" v√†o ƒë·∫ßu t·ª´ (VD: "_ƒë√®n")
            
            if not token_clean:
                continue
            
            # ===== B∆Ø·ªöC 5: MATCH TOKEN V·ªöI T·ª™ =====
            if word_idx < len(tokens):
                word = tokens[word_idx]  # T·ª´ g·ªëc (VD: "ƒë√®n")
                
                # Ki·ªÉm tra xem token c√≥ ph·∫£i l√† ph·∫ßn c·ªßa t·ª´ n√†y kh√¥ng
                if token_clean.lower() in word.lower() or word.lower().startswith(token_clean.lower()):
                    # G√°n nh√£n cho token n√†y
                    labels[i] = ner_tags[word_idx]
                    # VD: labels[i] = 1 (t∆∞∆°ng ·ª©ng "B-DEVICE")
                    
                    # Ki·ªÉm tra xem ƒë√£ h·∫øt t·ª´ n√†y ch∆∞a
                    if token_clean.lower() == word.lower():
                        word_idx += 1  # Chuy·ªÉn sang t·ª´ ti·∫øp theo
                else:
                    # Token kh√¥ng kh·ªõp -> Chuy·ªÉn sang t·ª´ ti·∫øp theo
                    word_idx += 1
                    if word_idx < len(tokens):
                        labels[i] = ner_tags[word_idx]
        
        # L∆∞u k·∫øt qu·∫£
        tokenized_inputs["input_ids"].append(encoding["input_ids"])
        tokenized_inputs["attention_mask"].append(encoding["attention_mask"])
        tokenized_inputs["labels"].append(labels)
    
    return tokenized_inputs

# Tokenize datasets
print("Tokenizing training dataset...")
tokenized_train = train_dataset.map(
    tokenize_and_align_labels, 
    batched=True,  # X·ª≠ l√Ω nhi·ªÅu m·∫´u c√πng l√∫c (nhanh h∆°n)
    remove_columns=train_dataset.column_names  # X√≥a c·ªôt c≈© (tokens, ner_tags)
)

print("Tokenizing validation dataset...")
tokenized_val = val_dataset.map(
    tokenize_and_align_labels, 
    batched=True,
    remove_columns=val_dataset.column_names
)

print("‚úÖ Datasets tokenized")
print(f"   Training samples: {len(tokenized_train)}")
print(f"   Validation samples: {len(tokenized_val)}")

==============================================================================================

cell code 8: C·∫§U H√åNH TRAINING
==============================================================================================
from transformers import TrainingArguments, Trainer
from transformers import DataCollatorForTokenClassification
import numpy as np
from seqeval.metrics import f1_score, precision_score, recall_score
import os

# T·∫Øt W&B
os.environ["WANDB_DISABLED"] = "true"

# ===== DATA COLLATOR =====
data_collator = DataCollatorForTokenClassification(tokenizer)
# B·∫¢N CH·∫§T: Data Collator t·ª± ƒë·ªông:
# 1. G·ªôp nhi·ªÅu m·∫´u th√†nh 1 batch
# 2. Th√™m padding v√†o input_ids ƒë·ªÉ b·∫±ng ƒë·ªô d√†i
# 3. Th√™m padding v√†o labels v·ªõi gi√° tr·ªã -100
# T·∫°i sao -100? ƒê·ªÉ model kh√¥ng b·ªã ph·∫°t khi d·ª± ƒëo√°n sai ·ªü v·ªã tr√≠ padding

# ===== H√ÄM T√çNH METRICS =====
def compute_metrics(p):
    """
    T√≠nh Precision, Recall, F1 s·ª≠ d·ª•ng seqeval
    B·∫¢N CH·∫§T: Seqeval ƒë√°nh gi√° tr√™n c·∫•p ƒë·ªô TH·ª∞C TH·ªÇ, kh√¥ng ph·∫£i t·ª´ng t·ª´
    """
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)
    # B·∫¢N CH·∫§T: predictions shape = [Batch, Seq_Len, Num_Labels]
    # argmax(axis=2) -> L·∫•y nh√£n c√≥ x√°c su·∫•t cao nh·∫•t
    # Output shape = [Batch, Seq_Len]
    
    # ===== B·ªé QUA INDEX -100 =====
    # Ch·ªâ gi·ªØ l·∫°i nh√£n th·∫≠t (kh√¥ng ph·∫£i special tokens, padding, subword)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    # B·∫¢N CH·∫§T: Chuy·ªÉn s·ªë -> ch·ªØ
    # VD: [1, 2, 0, -100, -100] -> ["B-CROP", "I-CROP", "O"]
    
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    
    # ===== T√çNH METRICS B·∫∞NG SEQEVAL =====
    return {
        "precision": precision_score(true_labels, true_predictions),
        "recall": recall_score(true_labels, true_predictions),
        "f1": f1_score(true_labels, true_predictions),
    }
    # B·∫¢N CH·∫§T SEQEVAL:
    # - Ch·ªâ t√≠nh ƒëi·ªÉm khi b·∫Øt ƒê√öNG v√† ƒê·ª¶ c·∫£ c·ª•m t·ª´
    # VD: "ng√¥ ng·ªçt" (B-CROP I-CROP)
    # - N·∫øu model d·ª± ƒëo√°n: "ng√¥" (B-CROP), "ng·ªçt" (O) -> SAI (kh√¥ng t√≠nh ƒëi·ªÉm)
    # - Ph·∫£i d·ª± ƒëo√°n: "ng√¥" (B-CROP), "ng·ªçt" (I-CROP) -> ƒê√öNG

# ===== TRAINING ARGUMENTS =====
training_args = TrainingArguments(
    output_dir="./ner_model",
    eval_strategy="epoch",  # ƒê√°nh gi√° sau m·ªói epoch
    save_strategy="epoch",  # L∆∞u checkpoint sau m·ªói epoch
    
    learning_rate=2e-5,  # Th·∫•p h∆°n Intent (5e-5) v√¨ NER kh√≥ h∆°n
    # B·∫¢N CH·∫§T: LR th·∫•p -> H·ªçc ch·∫≠m nh∆∞ng ·ªïn ƒë·ªãnh, tr√°nh qu√™n ki·∫øn th·ª©c PhoBERT
    
    per_device_train_batch_size=8,  # Nh·ªè h∆°n Intent (16) v√¨ NER t·ªën RAM h∆°n
    # B·∫¢N CH·∫§T: M·ªói token ƒë·ªÅu c√≥ 1 output (13 s·ªë) -> T·ªën b·ªô nh·ªõ
    
    per_device_eval_batch_size=8,
    
    num_train_epochs=10,  # Nhi·ªÅu h∆°n Intent (5) v√¨ NER kh√≥ h∆°n
    # B·∫¢N CH·∫§T: C·∫ßn nhi·ªÅu th·ªùi gian ƒë·ªÉ h·ªçc ranh gi·ªõi B- v√† I-
    
    weight_decay=0.01,  # L2 Regularization (ch·ªëng Overfitting)
    
    logging_dir="./logs",
    logging_steps=10,
    
    load_best_model_at_end=True,  # Gi·ªØ checkpoint t·ªët nh·∫•t
    metric_for_best_model="f1",   # D√πng F1 ƒë·ªÉ ch·ªçn
    
    push_to_hub=False,
    report_to="none",  # T·∫Øt W&B
)

print("‚úÖ Training arguments configured")
print(f"   Epochs: {training_args.num_train_epochs}")
print(f"   Batch size: {training_args.per_device_train_batch_size}")
print(f"   Learning rate: {training_args.learning_rate}")
print(f"   Logging: Disabled (no wandb)")

==============================================================================================

cell code 9: TRAINING
==============================================================================================
# Kh·ªüi t·∫°o Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_val,
    tokenizer=tokenizer,
    data_collator=data_collator,  # QUAN TR·ªåNG: X·ª≠ l√Ω padding cho labels
    compute_metrics=compute_metrics,  # D√πng seqeval
)

# B·∫Øt ƒë·∫ßu training
print("\nüöÄ Starting training...\n")
trainer.train()
# B·∫¢N CH·∫§T: V√≤ng l·∫∑p t∆∞∆°ng t·ª± Intent, nh∆∞ng:
# - Loss ƒë∆∞·ª£c t√≠nh cho M·ªñI TOKEN (kh√¥ng ph·∫£i c·∫£ c√¢u)
# - B·ªè qua v·ªã tr√≠ c√≥ label = -100 (special tokens, padding, subword)

print("\n‚úÖ Training completed!")

==============================================================================================

cell code 10: ƒê√ÅNH GI√Å
==============================================================================================
# ƒê√°nh gi√° tr√™n validation set
results = trainer.evaluate()

print("\nüìä Evaluation Results:")
for key, value in results.items():
    print(f"  {key}: {value:.4f}")
# VD Output:
#   eval_loss: 0.0104
#   eval_precision: 0.9965
#   eval_recall: 0.9965
#   eval_f1: 0.9965

==============================================================================================

cell code 11: L∆ØU MODEL
==============================================================================================
# L∆∞u model v√† tokenizer
output_dir = "./ner_extractor_final"
trainer.save_model(output_dir)
tokenizer.save_pretrained(output_dir)

# Copy label mapping (QUAN TR·ªåNG)
import shutil
shutil.copy('label_mapping.json', f'{output_dir}/label_mapping.json')
# B·∫¢N CH·∫§T: Backend c·∫ßn file n√†y ƒë·ªÉ bi·∫øt s·ªë 1 ·ª©ng v·ªõi nh√£n g√¨

print(f"\n‚úÖ Model saved to {output_dir}")
print("\nüì¶ Files to download:")
print("  - config.json")
print("  - pytorch_model.bin (or model.safetensors)")
print("  - label_mapping.json")

==============================================================================================

cell code 12: TEST V·ªöI C√ÇU M·∫™U
==============================================================================================
# C√°c c√¢u test
test_examples = [
    "B·∫≠t t∆∞·ªõi khu A trong 5 ph√∫t",
    "ƒê·ªô ·∫©m ·ªü khu B l√† bao nhi√™u",
    "Chi ph√≠ th√°ng n√†y",
    "T·∫Øt ƒë√®n khu C",
    "Nhi·ªát ƒë·ªô khu 1 hi·ªán t·∫°i",
    "Doanh thu qu√Ω 2",
    "C√°ch tr·ªìng cam s√†nh"
]

def predict_entities(text):
    """
    D·ª± ƒëo√°n th·ª±c th·ªÉ trong c√¢u
    B·∫¢N CH·∫§T: M√¥ ph·ªèng c√°ch API s·∫Ω d√πng model
    """
    # Tokenize
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=128)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # D·ª± ƒëo√°n
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.argmax(outputs.logits, dim=-1)[0]
        # B·∫¢N CH·∫§T: logits shape = [1, Seq_Len, 13]
        # argmax(dim=-1) -> L·∫•y nh√£n c√≥ x√°c su·∫•t cao nh·∫•t
        # Output shape = [Seq_Len]
    
    # Decode tokens
    tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])
    labels = [id2label[p.item()] for p in predictions]
    
    # ===== TR√çCH XU·∫§T TH·ª∞C TH·ªÇ =====
    entities = []
    current_entity = None
    
    for token, label in zip(tokens, labels):
        # B·ªè qua special tokens
        if token in ["<s>", "</s>", "<pad>"]:
            continue
        
        if label.startswith("B-"):  # B·∫Øt ƒë·∫ßu th·ª±c th·ªÉ m·ªõi
            if current_entity:
                entities.append(current_entity)
            current_entity = {
                "type": label[2:],  # VD: "B-DEVICE" -> "DEVICE"
                "text": token.replace("_", " ").strip()
            }
        elif label.startswith("I-") and current_entity:  # Ti·∫øp t·ª•c th·ª±c th·ªÉ
            current_entity["text"] += " " + token.replace("_", "").strip()
        elif label == "O" and current_entity:  # K·∫øt th√∫c th·ª±c th·ªÉ
            entities.append(current_entity)
            current_entity = None
    
    if current_entity:
        entities.append(current_entity)
    
    return entities

# Test
print("\nüß™ Testing model on new examples:\n")
for example in test_examples:
    entities = predict_entities(example)
    print(f"Text: {example}")
    print(f"Entities: {entities}")
    print()
# VD Output:
# Text: B·∫≠t t∆∞·ªõi khu A trong 5 ph√∫t
# Entities: [{'type': 'DEVICE', 'text': 't∆∞·ªõi'}, {'type': 'AREA', 'text': 'khu A'}, {'type': 'DURATION', 'text': '5 ph√∫t'}]

==============================================================================================

cell code 13: N√âN V√Ä DOWNLOAD
==============================================================================================
# N√©n model th√†nh file .zip
!zip -r ner_model.zip ner_extractor_final/
print("‚úÖ Model zipped as ner_model.zip")
print("\nüì• Download ner_model.zip from Colab Files panel")
print("\nüìã Deployment instructions:")
print("1. Extract ner_model.zip")
print("2. Copy files to: C:\\Users\\ADMIN\\Desktop\\ex\\apps\\python-ai-service\\models\\ner_extractor\\")
print("3. Restart Python AI service")

================================================================================
T·ªîNG K·∫æT B·∫¢N CH·∫§T
================================================================================
1. BIO TAGGING: Chuy·ªÉn annotation (start, end) -> BIO format (B-, I-, O)
2. LABEL MAPPING: Chuy·ªÉn nh√£n ch·ªØ -> s·ªë (13 nh√£n)
3. TOKEN ALIGNMENT: Gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ PhoBERT t√°ch t·ª´ th√†nh subword
   - G√°n nh√£n cho token ƒë·∫ßu ti√™n
   - C√°c token sau g√°n -100 (ignore)
4. DATA COLLATOR: T·ª± ƒë·ªông padding cho labels v·ªõi -100
5. SEQEVAL: ƒê√°nh gi√° tr√™n c·∫•p ƒë·ªô th·ª±c th·ªÉ (Entity-level F1)
6. TRAINING: Token Classification (m·ªói token c√≥ 1 output)
7. PREDICTION: Decode tokens -> Tr√≠ch xu·∫•t th·ª±c th·ªÉ (B- + I-)

================================================================================
C√ÇU H·ªéI TH∆Ø·ªúNG G·∫∂P T·ª™ H·ªòI ƒê·ªíNG
================================================================================
Q1: "T·∫°i sao NER kh√≥ h∆°n Intent?"
A1: "NER ph·∫£i ph√¢n lo·∫°i T·ª™NG T·ª™ (Token Classification), kh√¥ng ph·∫£i c·∫£ c√¢u.
    H∆°n n·ªØa, NER c√≥ 13 nh√£n BIO (ph·ª©c t·∫°p h∆°n 5 Intent). Model c·∫ßn h·ªçc ranh
    gi·ªõi gi·ªØa B- (Beginning) v√† I- (Inside), ƒë√≤i h·ªèi nhi·ªÅu th·ªùi gian h∆°n."

Q2: "Token Alignment l√† g√¨?"
A2: "PhoBERT t√°ch t·ª´ th√†nh subword (VD: 'chi ph√≠' -> ['chi', '_ph√≠']).
    Nh∆∞ng ta ch·ªâ c√≥ 1 nh√£n 'B-METRIC' cho 'chi ph√≠'. N√™n ph·∫£i cƒÉn ch·ªânh:
    - Token ƒë·∫ßu ti√™n ('chi'): G√°n nh√£n g·ªëc (B-METRIC)
    - Token ti·∫øp theo ('_ph√≠'): G√°n -100 (ignore)
    ƒê√¢y l√† th√°ch th·ª©c l·ªõn nh·∫•t c·ªßa NER v·ªõi PhoBERT."

Q3: "T·∫°i sao d√πng -100 cho labels?"
A3: "Trong PyTorch CrossEntropyLoss, gi√° tr·ªã -100 s·∫Ω b·ªã B·ªé QUA khi t√≠nh Loss.
    Ta d√πng -100 cho:
    - Special tokens (<s>, </s>, <pad>)
    - Subword tokens (ph·∫ßn ti·∫øp theo c·ªßa t·ª´)
    -> Model kh√¥ng b·ªã ph·∫°t khi d·ª± ƒëo√°n sai ·ªü nh·ªØng v·ªã tr√≠ n√†y."

Q4: "T·∫°i sao kh√¥ng d√πng Accuracy m√† d√πng Seqeval?"
A4: "V√≠ d·ª• c√¢u 10 t·ª´, ch·ªâ c√≥ 1 t·ª´ l√† Entity. N·∫øu model l∆∞·ªùi ƒëo√°n to√†n 'O'
    -> Accuracy = 90% (Sai qu√° sai!).
    Seqeval ƒë√°nh gi√° tr√™n c·∫•p ƒë·ªô th·ª±c th·ªÉ. N√≥ ch·ªâ t√≠nh ƒëi·ªÉm khi model b·∫Øt
    ƒê√öNG v√† ƒê·ª¶ c·∫£ c·ª•m t·ª´ (ƒë√∫ng v·ªã tr√≠ B-, ƒë√∫ng v·ªã tr√≠ I-, ƒë√∫ng lo·∫°i)."

Q5: "T·∫°i sao NER train 10 epochs m√† Intent ch·ªâ 5?"
A5: "NER kh√≥ h∆°n v√¨:
    - Token Classification (ph√¢n lo·∫°i t·ª´ng t·ª´)
    - 13 nh√£n BIO (nhi·ªÅu h∆°n 5 Intent)
    - Ph·∫£i h·ªçc ranh gi·ªõi B- v√† I-
    N√™n c·∫ßn nhi·ªÅu th·ªùi gian h∆°n ƒë·ªÉ model h·ªôi t·ª•."
================================================================================
