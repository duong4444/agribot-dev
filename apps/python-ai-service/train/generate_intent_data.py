"""
Script t·ª± ƒë·ªông sinh d·ªØ li·ªáu training cho Intent Classification

Techniques s·ª≠ d·ª•ng:
1. Template-based generation
2. (C·∫£i ti·∫øn) Random multi-synonym replacement
3. (C·∫£i ti·∫øn) Probabilistic paraphrasing & question variations
4. Entity substitution
5. (M·ªõi) Noise injection (filler words, politeness)
6. (M·ªõi) Case variations
7. (C·∫£i ti·∫øn) Global de-duplication at save time

Improvements:
- Chi·∫øn l∆∞·ª£c augmentation ƒëa d·∫°ng h∆°n (chaining methods).
- Th√™m c√°c k·ªπ thu·∫≠t augmentation th·ª±c t·∫ø (noise, case).
- X·ª≠ l√Ω de-duplication to√†n c·ª•c v√† ch√≠nh x√°c.
- T·ªëi ∆∞u h√≥a v√≤ng l·∫∑p generation.
- TQDM l·ªìng nhau ƒë∆∞·ª£c qu·∫£n l√Ω t·ªët h∆°n.
"""

import csv
import random
import re
import logging
import argparse
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict, Counter
from pathlib import Path
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import time

# ============================================================================
# CONFIGURATION
# ============================================================================

@dataclass
class Config:
    """Configuration for data generation"""
    input_file: str = "data/intent_data_6intents.csv"
    output_file: str = "data/intent_data_augmented_6intents.csv"
    target_samples: int = 200
    max_replacements: int = 2  # S·ªë t·ª´ ƒë·ªìng nghƒ©a t·ªëi ƒëa thay th·∫ø trong 1 c√¢u
    min_sample_length: int = 3
    max_sample_length: int = 200
    prob_noise: float = 0.3  # X√°c su·∫•t th√™m t·ª´ nhi·ªÖu
    prob_case_variation: float = 0.3  # X√°c su·∫•t ƒë·ªïi ki·ªÉu ch·ªØ
    prob_question_variation: float = 0.2 # X√°c su·∫•t bi·∫øn ƒë·ªïi c√¢u h·ªèi
    prob_use_template: float = 0.5 # X√°c su·∫•t t·∫°o m·ªõi t·ª´ template (so v·ªõi augment)
    enable_parallel: bool = False
    num_workers: int = 4
    random_seed: int = 42
    log_level: str = "INFO"


# ============================================================================
# LOGGING SETUP
# ============================================================================

def setup_logging(level: str = "INFO"):
    """Setup logging configuration"""
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format='%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    return logging.getLogger(__name__)


logger = setup_logging()


# ============================================================================
# SYNONYMS, ENTITIES & VARIATIONS
# ============================================================================

SYNONYMS = {
    # Verbs - ƒë·ªông t·ª´
    "xem": ["ki·ªÉm tra", "cho bi·∫øt", "hi·ªÉn th·ªã", "li·ªát k√™", "cho xem", "tra c·ª©u"],
    "t·ªïng": ["t·ªïng c·ªông", "t√≠nh t·ªïng", "t·ªïng k·∫øt", "to√†n b·ªô"],
    "th√™m": ["t·∫°o", "th√™m m·ªõi", "t·∫°o m·ªõi", "ghi nh·∫≠n", "l∆∞u l·∫°i"],
    "s·ª≠a": ["c·∫≠p nh·∫≠t", "ch·ªânh s·ª≠a", "thay ƒë·ªïi", "ƒëi·ªÅu ch·ªânh"],
    "x√≥a": ["xo√°", "lo·∫°i b·ªè", "h·ªßy", "g·ª° b·ªè", "x√≥a b·ªè"],
    "b·∫≠t": ["m·ªü", "k√≠ch ho·∫°t", "kh·ªüi ƒë·ªông"],
    "t·∫Øt": ["ƒë√≥ng", "ng·∫Øt", "d·ª´ng", "t·∫°m ng·ª´ng"],

    # Time - th·ªùi gian
    "th√°ng n√†y": ["th√°ng hi·ªán t·∫°i", "th√°ng nay"],
    "nƒÉm nay": ["nƒÉm n√†y", "nƒÉm hi·ªán t·∫°i"],
    "h√¥m nay": ["ng√†y h√¥m nay", "h√¥m n√†y"],
    "tu·∫ßn n√†y": ["tu·∫ßn hi·ªán t·∫°i", "tu·∫ßn nay"],

    # Quantity
    "bao nhi√™u": ["m·∫•y", "l√† bao nhi√™u", "l√† m·∫•y"],
    "m·∫•y": ["bao nhi√™u", "l√† m·∫•y", "l√† bao nhi√™u"],

    # Question words
    "l√†m sao": ["l√†m th·∫ø n√†o", "c√°ch n√†o", "b·∫±ng c√°ch n√†o"],
    "t·∫°i sao": ["v√¨ sao", "do ƒë√¢u"],
    "khi n√†o": ["l√∫c n√†o", "th·ªùi ƒëi·ªÉm n√†o"],
    "ph√π h·ª£p": ["h·ª£p", "th√≠ch h·ª£p", "n√™n", "ph√π h·ª£p nh·∫•t"],
}

# (M·ªõi) C√°c t·ª´ nhi·ªÖu th·ª±c t·∫ø
NOISE_WORDS = {
    'prefix': [
        "vui l√≤ng", "l√†m ∆°n", "xin h√£y", "cho t√¥i h·ªèi", "b·∫°n ∆°i", "gi√∫p t√¥i",
        "h√£y", "th·ª≠"
    ],
    'suffix': [
        "gi√∫p t√¥i", "v·ªõi", "nh√©", "nha", "xem n√†o", "√†", "∆°i", "ƒëi",
        "gi√πm"
    ]
}

# TEMPLATES V√Ä ENTITIES gi·ªØ nguy√™n nh∆∞ trong file c·ªßa b·∫°n
# (Gi·ªØ nguy√™n TEMPLATES v√† ENTITIES ·ªü ƒë√¢y)
# ============================================================================
# TEMPLATES CHO M·ªñI INTENT
# ============================================================================

TEMPLATES = {
    "knowledge_query": [
        # C√¢u h·ªèi c∆° b·∫£n v·ªÅ k·ªπ thu·∫≠t n√¥ng nghi·ªáp
        "C√°ch {action} {crop}?",
        "{crop} b·ªã {disease} x·ª≠ l√Ω th·∫ø n√†o?",
        "K·ªπ thu·∫≠t {technique} cho {crop}?",
        "L√†m sao ƒë·ªÉ {action} {crop}?",
        "{fertilizer} c√≥ t√°c d·ª•ng g√¨?",
        "Quy tr√¨nh {action} {crop}?",
        "Ph∆∞∆°ng ph√°p {technique} l√† g√¨?",
        "B·ªánh {disease} tr√™n {crop} nh·∫≠n bi·∫øt nh∆∞ th·∫ø n√†o?",
        "L·ª£i √≠ch c·ªßa {technique}?",
        "Ngu·ªìn g·ªëc c·ªßa {crop}",
        "Xu·∫•t x·ª© c·ªßa {crop}",
        "Ngu·ªìn g·ªëc c√¢y {crop}",
        "Ngu·ªìn g·ªëc c·ªßa c√¢y {crop}",
        "Ngu·ªìn g·ªëc {crop}"
        
        # Th·ªùi v·ª• v√† m·∫≠t ƒë·ªô tr·ªìng (th∆∞·ªùng b·ªã nh·∫ßm v·ªõi analytics)
        "Th·ªùi v·ª• {action} {crop} ·ªü {region}?",
        "M·∫≠t ƒë·ªô tr·ªìng {crop} th√≠ch h·ª£p?",
        "Th·ªùi v·ª• v√† m·∫≠t ƒë·ªô c·ªßa {crop}?",
        "M·∫≠t ƒë·ªô tr·ªìng {crop} ·ªü {region}?",
        "Kho·∫£ng c√°ch tr·ªìng {crop}?",
        "Th·ªùi ƒëi·ªÉm tr·ªìng {crop} n√†o t·ªët nh·∫•t?",
        
        # G·ª£i √Ω c√¢y tr·ªìng theo m√πa
        "{season} n√™n tr·ªìng c√¢y g√¨?",
        "G·ª£i √Ω c√¢y tr·ªìng cho {season}?",
        "C√¢y tr·ªìng n√†o ph√π h·ª£p v·ªõi {season}?",
        "{season} ·ªü {region} n√™n tr·ªìng {crop} n√†o?",
        "C√¢y g√¨ ph√π h·ª£p tr·ªìng v√†o {season}?",
        
        # Gi√° tr·ªã kinh t·∫ø v√† th√¥ng tin v·ªÅ c√¢y tr·ªìng (th∆∞·ªùng b·ªã nh·∫ßm v·ªõi financial/analytics)
        "Cho t√¥i th√¥ng tin v·ªÅ gi√° tr·ªã kinh t·∫ø c√¢y {crop}",
        "Gi√° tr·ªã kinh t·∫ø c·ªßa {crop} nh∆∞ th·∫ø n√†o?",
        "Th√¥ng tin v·ªÅ gi√° tr·ªã kinh t·∫ø {crop}",
        "{crop} c√≥ gi√° tr·ªã kinh t·∫ø cao kh√¥ng?",
        "L·ª£i √≠ch kinh t·∫ø khi tr·ªìng {crop}?",
        
        # Quy tr√¨nh canh t√°c (th∆∞·ªùng b·ªã nh·∫ßm v·ªõi sensor_query)
        "Quy tr√¨nh canh t√°c {crop} c√≥ m·∫•y giai ƒëo·∫°n?",
        "C√°c giai ƒëo·∫°n canh t√°c {crop}?",
        "{crop} tr·ªìng theo quy tr√¨nh n√†o?",
        "Chu tr√¨nh sinh tr∆∞·ªüng c·ªßa {crop}?",
        
        # So s√°nh b·ªánh v√† tri·ªáu ch·ª©ng (th∆∞·ªùng b·ªã nh·∫ßm v·ªõi analytics)
        "So s√°nh tri·ªáu ch·ª©ng b·ªánh {disease} v√† b·ªánh {disease2} tr√™n {crop}",
        "ƒêi·ªÉm kh√°c bi·ªát gi·ªØa b·ªánh {disease} v√† {disease2}",
        "Ph√¢n bi·ªát b·ªánh {disease} v·ªõi {disease2}",
        
        # T√°c d·ª•ng v√† c√¥ng d·ª•ng (th∆∞·ªùng b·ªã nh·∫ßm v·ªõi financial_query)
        "T√°c d·ª•ng c·ªßa {crop}",
        "C√¥ng d·ª•ng {crop} trong y h·ªçc",
        "{crop} c√≥ t√°c d·ª•ng g√¨?",
        "L·ª£i √≠ch s·ª©c kh·ªèe c·ªßa {crop}?",
        
        # C√¢u h·ªèi v·ªÅ v·ª• m√πa (th∆∞·ªùng b·ªã nh·∫ßm v·ªõi analytics)
        "V·ª• {crop} n√†o c√≥ nƒÉng su·∫•t cao nh·∫•t ·ªü {region}?",
        "V·ª• n√†o tr·ªìng {crop} t·ªët nh·∫•t?",
        "M√πa v·ª• {crop} ·ªü {region}?",
    ],
    
    "financial_query": [
        # Ch·ªâ c√°c c√¢u h·ªèi th·ª±c s·ª± v·ªÅ t√†i ch√≠nh c·ª• th·ªÉ
        "T·ªïng {metric} {period} l√† bao nhi√™u?",
        "Chi ph√≠ {category} {period}?",
        "Xem b√°o c√°o {report_type} {period}.",
        "Doanh thu t·ª´ {crop} {period}?",
        "L·ª£i nhu·∫≠n c·ªßa {item} {period}?",
        "T√≠nh {metric} c·ªßa {item}.",
        "Cho xem {metric} {period}.",
        "Ki·ªÉm tra chi ph√≠ {category} {period}.",
        "{period} l·ªó hay l√£i?",
        "So s√°nh doanh thu {period1} v·ªõi {period2}.",
        "B√°o c√°o t√†i ch√≠nh {period}",
        "Thu nh·∫≠p t·ª´ {crop} {period}",
        "Chi ti√™u {period} bao nhi√™u?",
    ],
    
    "device_control": [
        "B·∫≠t {device} ·ªü {area}.",
        "T·∫Øt {device}.",
        "ƒêi·ªÅu ch·ªânh {device} v·ªÅ {value}.",
        "L√™n l·ªãch {device} {time}.",
        "Ki·ªÉm tra tr·∫°ng th√°i {device}.",
        "{device} c√≥ ƒëang ho·∫°t ƒë·ªông kh√¥ng?",
        "C√†i ƒë·∫∑t {device} {parameter} l√† {value}.",
        "Cho {device} ch·∫°y {duration}.",
        "Ng∆∞ng {device} ·ªü {area}.",
        "T·ª± ƒë·ªông {action} {device} khi {condition}.",
        "Thay ƒë·ªïi ng∆∞·ª°ng {sensor} t∆∞·ªõi t·ª± ƒë·ªông c·ªßa {area} th√†nh {value}",
        "ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng {sensor} t∆∞·ªõi t·ª± ƒë·ªông c·ªßa {area} th√†nh {value}",
        "ƒê·∫∑t ng∆∞·ª°ng {sensor} t∆∞·ªõi t·ª± ƒë·ªông c·ªßa {area} th√†nh {value}",
        "Thay ƒë·ªïi ng∆∞·ª°ng √°nh s√°ng t·ª± ƒë·ªông c·ªßa {area} th√†nh {value}",
        "ƒê·∫∑t ng∆∞·ª°ng √°nh s√°ng t·ª± ƒë·ªông c·ªßa {area} th√†nh {value}",
        "Ch·ªânh ng∆∞·ª°ng s√°ng ƒë√®n t·ª± ƒë·ªông c·ªßa {area} th√†nh {value}",
        "ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng √°nh s√°ng t·ª± ƒë·ªông c·ªßa {area} th√†nh {value}",
    ],
    
    "sensor_query": [
        # Ch·ªâ c√°c c√¢u h·ªèi th·ª±c s·ª± v·ªÅ d·ªØ li·ªáu c·∫£m bi·∫øn c·ª• th·ªÉ
        "Nhi·ªát ƒë·ªô hi·ªán t·∫°i l√† bao nhi√™u?",
        "Xem s·ªë li·ªáu {sensor} {period}.",
        "{sensor} ·ªü {area} l√† m·∫•y?",
        "L·ªãch s·ª≠ {sensor} c·ªßa {area}?",
        "C·∫£nh b√°o v·ªÅ {sensor}.",
        "Gi√° tr·ªã {sensor} c√≥ b√¨nh th∆∞·ªùng kh√¥ng?",
        "So s√°nh {sensor} gi·ªØa {area1} v√† {area2}.",
        "Bi·ªÉu ƒë·ªì {sensor} {period}.",
        "{sensor} cao nh·∫•t/th·∫•p nh·∫•t {period}?",
        "Xu h∆∞·ªõng {sensor} {period}.",
        "D·ªØ li·ªáu {sensor} h√¥m nay",
        "C·∫£m bi·∫øn {sensor} b√°o g√¨?",
        "Ki·ªÉm tra {sensor} ·ªü {area}",
        "ƒêo {sensor} hi·ªán t·∫°i",
        "Th·ªëng k√™ {sensor} {period}",
        "S·ªë ƒëo {sensor} {area} bao nhi√™u?",
    ],
    
    "unknown": [
        "Xin ch√†o",
        "Hello",
        "Hi",
        "Ch√†o b·∫°n",
        "C·∫£m ∆°n",
        "Thanks",
        "OK",
        "ƒê∆∞·ª£c r·ªìi",
        "T·∫°m bi·ªát",
        "Bye",
    ],
}

# ============================================================================
# ENTITIES CHO TEMPLATE SUBSTITUTION
# ============================================================================

ENTITIES = {
    "crop": [
        "l√∫a", "ng√¥", "khoai lang", "s·∫Øn", "khoai t√¢y", "khoai s·ªç",
        "l√∫a ST25", "l√∫a n·∫øp", "ng√¥ n·∫øp", "ng√¥ lai",
        "c√† ph√™", "h·ªì ti√™u", "ƒëi·ªÅu", "cao su", "ch√®", "m√≠a", "d·ª´a",
        "c√† ph√™ Robusta", "c√† ph√™ Arabica", "d·ª´a xi√™m",
        "cam", "b∆∞·ªüi", "chanh", "qu√Ωt", "t·∫Øc", "b∆∞·ªüi da xanh", "cam s√†nh",
        "xo√†i", "xo√†i C√°t H√≤a L·ªôc", "xo√†i ƒê√†i Loan",
        "s·∫ßu ri√™ng", "s·∫ßu ri√™ng Ri6", "m√≠t", "m√≠t Th√°i",
        "nh√£n", "v·∫£i", "v·∫£i thi·ªÅu", "ch√¥m ch√¥m", "v√∫ s·ªØa", "na",
        "thanh long", "thanh long ru·ªôt ƒë·ªè",
        "·ªïi", "b∆°", "b∆° 034", "mƒÉng c·ª•t", "chu·ªëi", "chu·ªëi ti√™u", "ƒëu ƒë·ªß",
        "d∆∞a h·∫•u", "d∆∞a l∆∞·ªõi", "nho", "t√°o", "l√™",
        "c√† chua", "c√† t√≠m", "c√† ph√°o", "·ªõt", "·ªõt chu√¥ng",
        "d∆∞a chu·ªôt", "d∆∞a leo", "b√≠ ƒë·ªè", "b√≠ xanh", "b·∫ßu", "m∆∞·ªõp",
        "rau c·∫£i", "c·∫£i ng·ªçt", "c·∫£i b·∫π xanh", "c·∫£i th·∫£o", "b·∫Øp c·∫£i", "su h√†o",
        "rau mu·ªëng", "rau ng√≥t", "m·ªìng t∆°i", "rau d·ªÅn", "rau lang",
        "s√∫p l∆°", "s√∫p l∆° xanh", "c·∫ßn t√¢y",
        "ƒë·∫≠u", "ƒë·∫≠u c√¥ ve", "ƒë·∫≠u ƒë≈©a", "ƒë·∫≠u b·∫Øp", "ƒë·∫≠u t∆∞∆°ng", "l·∫°c",
        "h√†nh l√°", "h√†nh t√¢y", "t·ªèi", "g·ª´ng", "ngh·ªá", "ri·ªÅng", "s·∫£",
        "rau m√πi", "rau rƒÉm", "t√≠a t√¥", "kinh gi·ªõi", "h√∫ng qu·∫ø", "th√¨ l√†",
        "hoa c√∫c", "hoa h·ªìng", "hoa lan", "hoa hu·ªá", "hoa ƒë√†o", "hoa mai",
        "n·∫•m", "n·∫•m r∆°m", "n·∫•m m·ª°", "n·∫•m h∆∞∆°ng", "n·∫•m b√†o ng∆∞"
    ],
    
    "disease": [
        "ƒë·∫°o √¥n", "kh√¥ v·∫±n", "b·ªánh h√©o xanh", "th√°n th∆∞", "ph·∫•n tr·∫Øng",
        "r·ªâ s·∫Øt", "s∆∞∆°ng mai", "ƒë·ªëm l√°", "ƒë·ªëm n√¢u", "th·ªëi r·ªÖ", "th·ªëi th√¢n",
        "l·ªü c·ªï r·ªÖ", "n·∫•m h·ªìng", "n·ª©t th√¢n", "x√¨ m·ªß", "ch·ªïi r·ªìng",
        "xoƒÉn l√°", "virus v√†ng l√°", "Greening", "v√†ng l√° g√¢n xanh",
        "s√¢u ƒë·ª•c th√¢n", "s√¢u cu·ªën l√°", "s√¢u t∆°", "s√¢u xanh", "s√¢u khoang",
        "s√¢u ƒë·ª•c qu·∫£", "s√¢u v·∫Ω b√πa", "b·ªánh lo√©t",
        "r·∫ßy n√¢u", "r·∫ßy xanh", "r·ªáp s√°p", "r·ªáp mu·ªôi", "b·ªç trƒ©",
        "b·ªç ph·∫•n tr·∫Øng", "b·ªç x√≠t", "b·ªç nh·∫£y", "ru·ªìi v√†ng", "nh·ªán ƒë·ªè",
        "·ªëc b∆∞∆°u v√†ng", "tuy·∫øn tr√πng", "chu·ªôt"
    ],
    
    "disease2": [
        "kh√¥ v·∫±n", "th√°n th∆∞", "ph·∫•n tr·∫Øng", "r·ªâ s·∫Øt", "s∆∞∆°ng mai", 
        "ƒë·ªëm n√¢u", "th·ªëi r·ªÖ", "n·∫•m h·ªìng", "xoƒÉn l√°", "Greening",
        "b·ªánh lo√©t", "virus v√†ng l√°", "ch·ªïi r·ªìng", "ƒë·∫°o √¥n"
    ],
    
    "action": [
        "tr·ªìng", "gieo h·∫°t", "b√≥n ph√¢n", "t∆∞·ªõi n∆∞·ªõc", "thu ho·∫°ch",
        "chƒÉm s√≥c", "nh√¢n gi·ªëng", "c·∫£i t·∫°o", "x·ª≠ l√Ω", "ph√≤ng tr·ª´"
    ],
    
    "technique": [
        "t∆∞·ªõi nh·ªè gi·ªçt", "tr·ªìng xen canh", "·ªß ph√¢n compost", "nu√¥i tr√πn qu·∫ø",
        "aquaponics", "canh t√°c h·ªØu c∆°", "VAC", "t·ªâa c√†nh", "gh√©p c√†nh"
    ],
    
    "fertilizer": [
        "ph√¢n NPK", "ph√¢n l√¢n", "ph√¢n kali", "ph√¢n h·ªØu c∆°", "ph√¢n vi sinh",
        "v√¥i b·ªôt", "ph√¢n ƒë·∫°m", "ph√¢n chu·ªìng", "ph√¢n compost"
    ],
    
    "region": [
        "mi·ªÅn B·∫Øc", "mi·ªÅn Trung", "mi·ªÅn Nam", "T√¢y Nguy√™n", "ƒê·ªìng b·∫±ng s√¥ng C·ª≠u Long"
    ],

    "season": [
        "m√πa ƒë√¥ng", "m√πa h√®", "m√πa m∆∞a", "m√πa kh√¥", "m√πa xu√¢n",
        "m√πa thu", "v·ª• ƒë√¥ng", "v·ª• h√® thu", "v·ª• ƒë√¥ng xu√¢n"
    ],
    
    "metric": [
        "doanh thu", "chi ph√≠", "l·ª£i nhu·∫≠n", "thu nh·∫≠p", "chi ti√™u",
        "c√¥ng n·ª£", "ti·ªÅn l√£i", "v·ªën ƒë·∫ßu t∆∞", "s·∫£n l∆∞·ª£ng"
    ],
    
    "category": [
        "ph√¢n b√≥n", "thu·ªëc tr·ª´ s√¢u", "gi·ªëng", "nh√¢n c√¥ng", "ƒëi·ªán n∆∞·ªõc",
        "xƒÉng d·∫ßu", "v·∫≠t t∆∞", "th·ª©c ƒÉn chƒÉn nu√¥i", "s·ª≠a ch·ªØa"
    ],
    
    "period": [
        "th√°ng n√†y", "th√°ng tr∆∞·ªõc", "nƒÉm nay", "qu√Ω n√†y", "tu·∫ßn n√†y",
        "h√¥m nay", "h√¥m qua", "th√°ng 9", "nƒÉm 2024", "6 th√°ng ƒë·∫ßu nƒÉm"
    ],
    
    "report_type": [
        "t√†i ch√≠nh", "d√≤ng ti·ªÅn", "l·ªó l√£i", "thu chi", "c√¥ng n·ª£", "thu·∫ø"
    ],
    
    "item": [
        "v·ª• m√πa", "khu A", "lu·ªëng B", "v∆∞·ªùn cam", "c√¢y s·∫ßu ri√™ng",
        "1 hecta l√∫a", "khu B", "n√¥ng tr·∫°i", "gi·ªëng ST25"
    ],
    
    "area": [
        "khu A", "khu B", "khu C", "lu·ªëng 1", "lu·ªëng 2", "v∆∞·ªùn cam",
        "nh√† k√≠nh", "nh√† m√†ng", "khu v·ª±c 1", "ƒë·ªìng ru·ªông", "v∆∞·ªùn ∆∞∆°m"
    ],
    
    "activity": [
        "b√≥n ph√¢n", "t∆∞·ªõi n∆∞·ªõc", "phun thu·ªëc", "thu ho·∫°ch", "l√†m c·ªè",
        "x·ªõi ƒë·∫•t", "t·ªâa c√†nh", "gieo m·∫°", "b·∫£o tr√¨"
    ],
    
    "device": [
        "m√°y b∆°m", "h·ªá th·ªëng t∆∞·ªõi", "qu·∫°t", "ƒë√®n", "c·ª≠a s·ªï", "r√®m che",
        "m√°y phun s∆∞∆°ng", "van t∆∞·ªõi", "ƒë·ªông c∆°", "c·∫£m bi·∫øn"
    ],
    
    "sensor": [
        "nhi·ªát ƒë·ªô", "ƒë·ªô ·∫©m", "pH", "EC", "√°nh s√°ng", "ƒë·ªô ·∫©m ƒë·∫•t", "CO2"
    ],
    
    "value": [
        "25¬∞C", "70%", "m·ª©c 3", "t·ª± ƒë·ªông", "cao", "th·∫•p"
    ],
    
    "time": [
        "6h s√°ng", "18h chi·ªÅu", "v√†o bu·ªïi s√°ng", "l√∫c 8h", "m·ªói 2 ti·∫øng"
    ],
    
    "duration": [
        "30 ph√∫t", "1 gi·ªù", "2 ti·∫øng", "c·∫£ ng√†y"
    ],
    
    "condition": [
        "nhi·ªát ƒë·ªô > 30¬∞C", "ƒë·ªô ·∫©m < 50%", "tr·ªùi m∆∞a", "ban ng√†y"
    ],
    
    # Additional entities
    "parameter": ["nhi·ªát ƒë·ªô", "t·ªëc ƒë·ªô", "th·ªùi gian", "ch·∫ø ƒë·ªô"],
    "details": ["th√¥ng tin chi ti·∫øt", "d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß", "c√°c th√¥ng s·ªë"],
    "type": ["chi ti·∫øt", "t·ªïng h·ª£p", "h√†ng ng√†y"],
    "event": ["s·ª± ki·ªán", "ho·∫°t ƒë·ªông", "c√¥ng vi·ªác"],
    "identifier": ["s·ªë 1", "ID 123", "m√£ ABC", "khu A"],
    "field": ["t√™n", "gi√° tr·ªã", "tr·∫°ng th√°i", "s·ªë l∆∞·ª£ng"],
    "record": ["b·∫£n ghi", "d·ªØ li·ªáu", "th√¥ng tin", "ghi ch√∫"],
    "name": ["H√≤a Ph√°t", "An Ph√∫", "T√¢n L·ªôc"],
    "date": ["15/10", "h√¥m qua", "tu·∫ßn tr∆∞·ªõc"],
    "dimension": ["th·ªùi gian", "khu v·ª±c", "lo·∫°i c√¢y"],
    "item1": ["l√∫a", "ng√¥", "khu A"],
    "item2": ["c√† ph√™", "h·ªì ti√™u", "khu B"],
    "period1": ["th√°ng n√†y", "qu√Ω n√†y"],
    "period2": ["th√°ng tr∆∞·ªõc", "qu√Ω tr∆∞·ªõc"],
    "area1": ["khu A", "v∆∞·ªùn cam"],
    "area2": ["khu B", "v∆∞·ªùn chanh"],
}

# Bi√™n d·ªãch regex cho synonyms ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô
SYNONYM_PATTERNS = {
    word: (re.compile(r'\b' + re.escape(word) + r'\b', re.IGNORECASE), syns)
    for word, syns in SYNONYMS.items()
}


# ============================================================================
# DATA VALIDATION (ƒê∆°n gi·∫£n h√≥a)
# ============================================================================

class DataValidator:
    """Validator for generated samples (ch·ªâ ki·ªÉm tra ƒë·ªãnh d·∫°ng)"""

    def __init__(self, config: Config):
        self.config = config

    def is_valid_format(self, sample: str) -> bool:
        """Check if sample format is valid (kh√¥ng ki·ªÉm tra duplicate)"""
        if not sample or not sample.strip():
            return False

        # Check length
        if len(sample) < self.config.min_sample_length:
            return False
        if len(sample) > self.config.max_sample_length:
            return False

        # Check for unfilled placeholders
        if '{' in sample or '}' in sample:
            return False

        return True


# ============================================================================
# AUGMENTATION FUNCTIONS (C·∫£i ti·∫øn)
# ============================================================================

class DataAugmenter:
    """Class for data augmentation operations"""

    def __init__(self, config: Config):
        self.config = config
        random.seed(config.random_seed)

    def apply_synonym_replacement(self, text: str) -> str:
        """
        (C·∫£i ti·∫øn) Ng·∫´u nhi√™n thay th·∫ø 1 ho·∫∑c nhi·ªÅu t·ª´ ƒë·ªìng nghƒ©a.
        """
        words_to_replace = []
        for word, (pattern, syns) in SYNONYM_PATTERNS.items():
            if pattern.search(text):
                words_to_replace.append((word, pattern, syns))
        
        if not words_to_replace:
            return text

        random.shuffle(words_to_replace)
        
        num_replacements = random.randint(1, min(len(words_to_replace), self.config.max_replacements))
        
        for i in range(num_replacements):
            word, pattern, syns = words_to_replace[i]
            synonym = random.choice(syns)
            
            # Ch·ªâ thay th·∫ø 1 l·∫ßn ƒë·ªÉ tr√°nh l·ªói
            # v√≠ d·ª•: "m·∫•y" -> "bao nhi√™u", r·ªìi "bao nhi√™u" -> "m·∫•y"
            text = pattern.sub(synonym, text, count=1)
            
        return text

    def generate_from_template(self, template: str, count: int = 1) -> List[str]:
        """Generate samples from template v·ªõi entity substitution"""
        samples = set()
        placeholders = re.findall(r'\{(\w+)\}', template)

        # Validate template has all required entities
        missing_entities = [ph for ph in placeholders if ph not in ENTITIES]
        if missing_entities:
            logger.warning(f"Template '{template}' c√≥ entities b·ªã thi·∫øu: {missing_entities}")
            return []

        attempts = 0
        max_attempts = count * 10  # Avoid infinite loops

        while len(samples) < count and attempts < max_attempts:
            attempts += 1
            sample = template
            
            # Substitute each placeholder
            # D√πng set ƒë·ªÉ ƒë·∫£m b·∫£o thay th·∫ø {area} v√† {area1} ri√™ng bi·ªát
            for placeholder in set(placeholders): 
                entity_values = ENTITIES[placeholder]
                # ƒê·∫øm s·ªë l·∫ßn placeholder xu·∫•t hi·ªán
                num_occurrences = sample.count(f'{{{placeholder}}}')
                for _ in range(num_occurrences):
                    entity_value = random.choice(entity_values)
                    sample = sample.replace(f'{{{placeholder}}}', entity_value, 1)

            samples.add(sample)
        
        return list(samples)

    def add_question_variations(self, text: str) -> str:
        """(C·∫£i ti·∫øn) Bi·∫øn ƒë·ªïi c√¢u h·ªèi m·ªôt c√°ch ng·∫´u nhi√™n"""
        if random.random() > self.config.prob_question_variation:
             return text
        
        original_text = text
        
        # 1. Th√™m "cho t√¥i bi·∫øt" prefix
        if "?" in text and not text.lower().startswith("cho"):
            no_question = text.replace('?', '').strip().lower()
            variations = [
                f"Cho t√¥i bi·∫øt {no_question}.",
                f"H√£y cho bi·∫øt {no_question}.",
                f"Li·ªát k√™ {no_question}."
            ]
            text = random.choice(variations)

        # 2. Th√™m "cho xem"
        elif not any(word in text.lower() for word in ["cho", "h√£y", "xin", "vui l√≤ng", "l√†m ∆°n"]):
             text = f"Cho xem {text.lower()}"

        # 3. ƒê·∫£m b·∫£o c√≥ d·∫•u ch·∫•m h·ªèi n·∫øu l√† c√¢u h·ªèi
        q_words = ["bao nhi√™u", "m·∫•y", "g√¨", "th·∫ø n√†o", "·ªü ƒë√¢u", "khi n√†o", "t·∫°i sao"]
        if any(q in text.lower() for q in q_words) and not text.endswith('?'):
            text = text.strip(" .") + "?"

        return text if text != original_text else original_text


    def add_noise(self, text: str) -> str:
        """(M·ªõi) Th√™m t·ª´ ƒë·ªám/t·ª´ l·ªãch s·ª± m·ªôt c√°ch ng·∫´u nhi√™n"""
        if random.random() > self.config.prob_noise:
            return text

        # ƒê·∫£m b·∫£o kh√¥ng th√™m nhi·ªÖu v√†o c√¢u ƒëi·ªÅu khi·ªÉn
        if text.lower().startswith(("b·∫≠t", "t·∫Øt", "m·ªü", "ƒë√≥ng", "d·ª´ng")):
             if random.random() < 0.5:
                 suffix = random.choice(NOISE_WORDS['suffix'])
                 return f"{text.strip(' .?')} {suffix}"
             else:
                 return text # Kh√¥ng th√™m prefix cho c√¢u ƒëi·ªÅu khi·ªÉn

        # 50/50 th√™m prefix ho·∫∑c suffix
        if random.random() < 0.5:
            # Add prefix
            prefix = random.choice(NOISE_WORDS['prefix'])
            return f"{prefix} {text.lower().strip(' .?')}"
        else:
            # Add suffix
            suffix = random.choice(NOISE_WORDS['suffix'])
            return f"{text.strip(' .?')} {suffix}"

    def add_case_variation(self, text: str) -> str:
        """(M·ªõi) Ng·∫´u nhi√™n thay ƒë·ªïi ki·ªÉu ch·ªØ"""
        if random.random() > self.config.prob_case_variation:
            return text
        
        choice = random.choice(['lower', 'upper', 'capitalize'])
        
        if choice == 'lower':
            return text.lower()
        if choice == 'upper':
            return text.upper()
        if choice == 'capitalize':
            # Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu (c√≥ th·ªÉ kh√¥ng t·ª± nhi√™n l·∫Øm, nh∆∞ng tƒÉng ƒëa d·∫°ng)
            return text.capitalize()
        
        return text


# ============================================================================
# DATA LOADING & SAVING
# ============================================================================

def load_existing_data(filepath: str) -> Dict[str, List[str]]:
    """Load existing training data with error handling"""
    data_by_intent = defaultdict(list)
    file_path = Path(filepath)
    
    if not file_path.exists():
        logger.warning(f"File {filepath} kh√¥ng t·ªìn t·∫°i. S·∫Ω t·∫°o m·ªõi.")
        return data_by_intent
    
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            if not reader.fieldnames or 'text' not in reader.fieldnames or 'label' not in reader.fieldnames:
                logger.error(f"File {filepath} kh√¥ng c√≥ ƒë√∫ng format (c·∫ßn 'text' v√† 'label' columns)")
                return data_by_intent
            
            for row_num, row in enumerate(reader, start=2):
                try:
                    text = row.get('text', '').strip()
                    label = row.get('label', '').strip()
                    
                    if text and label:
                        data_by_intent[label].append(text)
                    else:
                        logger.warning(f"Row {row_num}: Empty text or label")
                        
                except Exception as e:
                    logger.error(f"Error reading row {row_num}: {e}")
                    continue
        
        logger.info(f"Loaded {len(data_by_intent)} intents with {sum(len(v) for v in data_by_intent.values())} total samples from {filepath}")
        
    except Exception as e:
        logger.error(f"Error loading file {filepath}: {e}")
        return defaultdict(list)
    
    return data_by_intent


def save_augmented_data(data: Dict[str, List[str]], output_filepath: str) -> bool:
    """
    (C·∫£i ti·∫øn) Save augmented data to CSV with GLOBAL DE-DUPLICATION.
    """
    try:
        # Ensure output directory exists
        output_path = Path(output_filepath)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Prepare rows
        rows = []
        for intent, samples in data.items():
            for sample in samples:
                rows.append({'text': sample.strip(), 'label': intent})
        
        if not rows:
            logger.error("No data to save")
            return False
        
        # Shuffle for better distribution
        random.shuffle(rows)
        
        # (Quan tr·ªçng) Global De-duplication
        final_rows = []
        seen_normalized = set()
        
        for row in rows:
            normalized = row['text'].lower()
            if normalized not in seen_normalized:
                final_rows.append(row)
                seen_normalized.add(normalized)
            
        # Write to file
        with open(output_filepath, 'w', encoding='utf-8', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=['text', 'label'])
            writer.writeheader()
            writer.writerows(final_rows)
        
        logger.info(f"‚úÖ Saved {len(final_rows)} UNIQUE samples to {output_filepath} (from {len(rows)} total generated)")
        
        # Print distribution
        intent_counts = Counter(row['label'] for row in final_rows)
        logger.info("\nüìä Final Distribution (Unique):")
        for intent, count in sorted(intent_counts.items()):
            logger.info(f"  {intent}: {count}")
        
        return True
        
    except Exception as e:
        logger.error(f"Error saving file {output_filepath}: {e}")
        return False


# ============================================================================
# MAIN GENERATION LOGIC (C·∫£i ti·∫øn)
# ============================================================================

def generate_for_intent(
    intent: str,
    samples: List[str],
    target_count: int,
    config: Config,
    disable_tqdm: bool = False
) -> Tuple[str, List[str]]:
    """
    (C·∫£i ti·∫øn) Generate augmented samples for a single intent.
    S·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c k·∫øt h·ª£p (chaining) augmentation.
    """
    augmenter = DataAugmenter(config)
    validator = DataValidator(config)
    
    final_samples = set()
    
    # 1. Th√™m c√°c m·∫´u g·ªëc h·ª£p l·ªá
    for s in samples:
        if validator.is_valid_format(s):
            final_samples.add(s)

    # 2. T·∫°o pool ƒë·ªÉ augment, ch·ªâ d√πng m·∫´u g·ªëc ƒë·ªÉ tr√°nh "drift"
    pool_for_augmentation = list(final_samples)
    if not pool_for_augmentation:
        logger.warning(f"  {intent}: No valid original samples to augment from.")
        
    template_list = TEMPLATES.get(intent, [])
    if not template_list:
         logger.warning(f"  {intent}: No templates found.")
         
    # 3. V√≤ng l·∫∑p generation
    current_count = len(final_samples)
    
    pbar = tqdm(total=target_count, desc=f"  {intent}", leave=False, disable=disable_tqdm)
    pbar.update(current_count)
    
    attempts = 0
    max_attempts = target_count * 10  # G·∫•p 10 l·∫ßn s·ªë m·∫´u target
    
    while current_count < target_count and attempts < max_attempts:
        attempts += 1
        new_sample = ""
        
        # 4. Quy·∫øt ƒë·ªãnh chi·∫øn l∆∞·ª£c: t·∫°o m·ªõi t·ª´ template hay augment m·∫´u c≈©
        use_template = (random.random() < config.prob_use_template and template_list)
        
        try:
            if use_template:
                # T·∫°o m·ªõi t·ª´ template
                template = random.choice(template_list)
                generated = augmenter.generate_from_template(template, 1)
                if not generated:
                    continue
                new_sample = generated[0]
            
            elif pool_for_augmentation:
                # Augment t·ª´ 1 m·∫´u g·ªëc
                base_sample = random.choice(pool_for_augmentation)
                
                # 5. (Quan tr·ªçng) √Åp d·ª•ng CHU·ªñI augmentations
                aug_sample = augmenter.apply_synonym_replacement(base_sample)
                aug_sample = augmenter.add_question_variations(aug_sample)
                aug_sample = augmenter.add_noise(aug_sample)
                aug_sample = augmenter.add_case_variation(aug_sample)
                new_sample = aug_sample
            
            else:
                 # Kh√¥ng c√≥ m·∫´u g·ªëc v√† kh√¥ng th·ªÉ d√πng template -> d·ª´ng
                 if not template_list:
                      logger.warning(f"  {intent}: No originals and no templates. Stopping.")
                      break
                 else:
                      continue # Th·ª≠ l·∫°i v·ªõi template

            # 6. Validate v√† th√™m v√†o set
            if validator.is_valid_format(new_sample):
                normalized = new_sample.lower()
                if normalized not in {s.lower() for s in final_samples}:
                    final_samples.add(new_sample)
                    current_count += 1
                    pbar.update(1)

        except Exception as e:
            logger.error(f"Error during augmentation for {intent}: {e}")
            continue # B·ªè qua m·∫´u b·ªã l·ªói
            
    pbar.close()
    
    if attempts >= max_attempts:
        logger.warning(f"  {intent}: Reached max attempts ({max_attempts}) but only generated {current_count}/{target_count} samples.")
    else:
        logger.debug(f"  {intent}: Generated {current_count} samples.")
        
    return intent, list(final_samples)


def generate_augmented_data(
    existing_data: Dict[str, List[str]],
    config: Config
) -> Dict[str, List[str]]:
    """Generate augmented data cho m·ªói intent"""
    augmented_data = {}
    
    logger.info("\nüîÑ Starting data augmentation...")
    
    # T√≠nh to√°n l·∫°i target_count v·ªõi ƒë·ªô ∆∞u ti√™n cho knowledge_query
    avg_samples = sum(len(s) for s in existing_data.values()) / len(existing_data)
    base_target_count = max(config.target_samples, int(avg_samples))
    
    # T·∫°o nhi·ªÅu m·∫´u knowledge_query h∆°n v√¨ ƒë√¢y l√† intent ch√≠nh
    def get_target_count_for_intent(intent: str) -> int:
        if intent == "knowledge_query":
            return int(base_target_count * 1.5)  # TƒÉng 50% cho knowledge_query
        elif intent in ["financial_query", "sensor_query", "device_control"]:
            return base_target_count
        else:  # unknown
            return int(base_target_count * 0.7)  # Gi·∫£m 30% cho unknown
    
    logger.info(f"Base target ~{base_target_count} samples, knowledge_query gets ~{get_target_count_for_intent('knowledge_query')} samples.")
    
    if config.enable_parallel and len(existing_data) > 1:
        # Parallel processing
        logger.info(f"Using parallel processing with {config.num_workers} workers")
        
        with ThreadPoolExecutor(max_workers=config.num_workers) as executor:
            futures = {
                executor.submit(
                    generate_for_intent,
                    intent,
                    samples,
                    get_target_count_for_intent(intent),
                    config,
                    disable_tqdm=True # T·∫Øt TQDM con khi ch·∫°y song song
                ): intent
                for intent, samples in existing_data.items()
            }
            
            for future in tqdm(as_completed(futures), total=len(futures), desc="Processing intents"):
                try:
                    intent, samples_list = future.result()
                    augmented_data[intent] = samples_list
                except Exception as e:
                    intent = futures[future]
                    logger.error(f"Error processing intent {intent}: {e}", exc_info=True)
    else:
        # Sequential processing
        logger.info("Using sequential processing")
        for intent, samples in tqdm(existing_data.items(), desc="Processing intents"):
            try:
                _, augmented_samples = generate_for_intent(
                    intent,
                    samples,
                    get_target_count_for_intent(intent),
                    config,
                    disable_tqdm=False # B·∫≠t TQDM con
                )
                augmented_data[intent] = augmented_samples
            except Exception as e:
                logger.error(f"Error processing intent {intent}: {e}", exc_info=True)
    
    return augmented_data


# ============================================================================
# STATISTICS & REPORTING
# ============================================================================

def print_statistics(
    before_data: Dict[str, List[str]],
    after_data: Dict[str, List[str]] # after_data ƒë√£ ƒë∆∞·ª£c de-duped
):
    """Print detailed statistics about the augmentation"""
    logger.info("\n" + "="*60)
    logger.info("üìà AUGMENTATION STATISTICS")
    logger.info("="*60)
    
    total_before = sum(len(samples) for samples in before_data.values())
    total_after = sum(len(samples) for samples in after_data.values())
    
    logger.info(f"\nTotal intents: {len(before_data)}")
    logger.info(f"Total samples (before): {total_before}")
    logger.info(f"Total samples (after, unique): {total_after} (+{total_after - total_before})")
    
    logger.info(f"\nPer-intent breakdown (Unique Counts):")
    logger.info("-" * 60)
    logger.info(f"{'Intent':<25} {'Before':<10} {'After':<10} {'Increase':<10}")
    logger.info("-" * 60)
    
    # C·∫ßn de-dupe data 'before' ƒë·ªÉ so s√°nh c√¥ng b·∫±ng
    before_counts = {}
    for intent, samples in before_data.items():
        before_counts[intent] = len(set(s.lower() for s in samples))
        
    after_counts = {}
    for intent, samples in after_data.items():
        after_counts[intent] = len(set(s.lower() for s in samples)) # D√π ƒë√£ de-dupe nh∆∞ng ƒë·ªÉ ch·∫Øc ch·∫Øn

    for intent in sorted(after_counts.keys()):
        before_count = before_counts.get(intent, 0)
        after_count = after_counts.get(intent, 0)
        increase = after_count - before_count
        logger.info(f"{intent:<25} {before_count:<10} {after_count:<10} +{increase:<9}")
    
    logger.info("="*60)


# ============================================================================
# MAIN
# ============================================================================

def main():
    """Main function"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Generate augmented intent data")
    parser.add_argument('--input', default="data/intent_data_6intents.csv", help="Input CSV file")
    parser.add_argument('--output', default="data/intent_data_augmented_6intents.csv", help="Output CSV file")
    parser.add_argument('--target', type=int, default=200, help="Target samples per intent (s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh n·∫øu s·ªë l∆∞·ª£ng m·∫´u g·ªëc trung b√¨nh cao h∆°n)")
    parser.add_argument('--parallel', action='store_true', help="Enable parallel processing")
    parser.add_argument('--workers', type=int, default=4, help="Number of parallel workers")
    parser.add_argument('--seed', type=int, default=42, help="Random seed")
    parser.add_argument('--log-level', default="INFO", choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])
    
    args = parser.parse_args()
    
    # Create configuration
    config = Config(
        input_file=args.input,
        output_file=args.output,
        target_samples=args.target,
        enable_parallel=args.parallel,
        num_workers=args.workers,
        random_seed=args.seed,
        log_level=args.log_level
    )
    
    # Setup logging with config level
    global logger
    logger = setup_logging(config.log_level)
    
    # Set random seed
    random.seed(config.random_seed)
    
    logger.info("üöÄ Starting Intent Data Generation...")
    logger.info(f"Config: {config}")
    
    try:
        start_time = time.time()
        
        # 1. Load existing data
        existing_data = load_existing_data(config.input_file)
        
        if not existing_data:
            logger.error("No data loaded. Exiting.")
            return 1
            
        # 2. Generate augmented data
        augmented_data = generate_augmented_data(existing_data, config)
        
        if not augmented_data:
            logger.error("No augmented data generated. Exiting.")
            return 1
            
        # 3. Save to file (h√†m n√†y ƒë√£ bao g·ªìm global de-duplication)
        success = save_augmented_data(augmented_data, config.output_file)
        
        if not success:
            logger.error("Failed to save data. Exiting.")
            return 1
            
        # 4. Print statistics
        # L·∫•y 'after_data' ƒë√£ ƒë∆∞·ª£c de-dupe b·∫±ng c√°ch load l·∫°i file v·ª´a save
        # ƒê√¢y l√† c√°ch ch√≠nh x√°c nh·∫•t ƒë·ªÉ th·ªëng k√™
        final_data = load_existing_data(config.output_file)
        print_statistics(existing_data, final_data)
        
        end_time = time.time()
        logger.info(f"\n‚ú® Done in {end_time - start_time:.2f} seconds!")
        return 0
        
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Process interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"\n‚ùå Unexpected error: {e}", exc_info=True)
        return 1


if __name__ == "__main__":
    exit(main())