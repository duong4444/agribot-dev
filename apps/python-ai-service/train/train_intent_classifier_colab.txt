cell code 1:
!pip install -q transformers datasets torch scikit-learn pandas numpy seqeval accelerate
scikit-learn tÃ­nh metric (accuracy,F1,...)
pandas: Xá»­ lÃ½ data dáº¡ng báº£ng.,seqeval: ÄÃ¡nh giÃ¡ sequence labeling.,accelerate: TÄƒng tá»‘c training.
# Táº¯t wandb Ä‘á»ƒ trÃ¡nh yÃªu cáº§u API key
import os
os.environ['WANDB_DISABLED'] = 'true'
print("âœ… ÄÃ£ táº¯t Weights & Biases tracking")
==============================================================================================
cell code 2:
from google.colab import files
import pandas as pd

# Upload file CSV
print("ğŸ“¤ Vui lÃ²ng upload file intent_data_augmented_161225.csv")
uploaded = files.upload()

# Láº¥y tÃªn file Ä‘áº§u tiÃªn Ä‘Æ°á»£c upload
data_file = list(uploaded.keys())[0]
print(f"âœ… ÄÃ£ upload file: {data_file}")

# Äá»c vÃ  kiá»ƒm tra data
Ä‘á»c file csv thÃ nh dataframe
df = pd.read_csv(data_file)
print(f"\nğŸ“Š Tá»•ng sá»‘ máº«u: {len(df)}")
print(f"\nğŸ“‹ CÃ¡c cá»™t: {df.columns.tolist()}")
print(f"\nğŸ·ï¸ PhÃ¢n bá»‘ labels:")
print(df['label'].value_counts())
print(f"\nğŸ‘€ Xem 5 máº«u Ä‘áº§u tiÃªn:")
df.head()
==============================================================================================

cell code 3:
import torch
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
import json

# Kiá»ƒm tra GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ï¸ Device: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")

# Táº¡o label mapping
unique_labels = sorted(df['label'].unique())
label_to_id = {label: idx for idx, label in enumerate(unique_labels)}
id_to_label = {idx: label for label, idx in label_to_id.items()}

print(f"\nğŸ·ï¸ Label mapping:")
for label, idx in label_to_id.items():
    print(f"   {idx}: {label}")

# ThÃªm cá»™t label_id
df['label_id'] = df['label'].map(label_to_id)

# Chia train/val/test (70/15/15)
train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

print(f"\nğŸ“Š PhÃ¢n chia dá»¯ liá»‡u:")
print(f"   Train: {len(train_df)} máº«u")
print(f"   Val:   {len(val_df)} máº«u")
print(f"   Test:  {len(test_df)} máº«u")

# Load tokenizer
model_name = "vinai/phobert-base"
print(f"\nğŸ”¤ Loading tokenizer: {model_name}")
tokenizer = AutoTokenizer.from_pretrained(model_name)
print("âœ… Tokenizer loaded")
==============================================================================================

cell code 4:
from torch.utils.data import Dataset

class IntentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Táº¡o datasets
train_dataset = IntentDataset(
    train_df['text'].tolist(),
    train_df['label_id'].tolist(),
    tokenizer
)

val_dataset = IntentDataset(
    val_df['text'].tolist(),
    val_df['label_id'].tolist(),
    tokenizer
)

test_dataset = IntentDataset(
    test_df['text'].tolist(),
    test_df['label_id'].tolist(),
    tokenizer
)

print("âœ… Datasets created")
==============================================================================================

cell code 5:
from transformers import (
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import numpy as np

# Load model
print(f"ğŸ¤– Loading model: {model_name}")
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(label_to_id)
)
model.to(device)
print("âœ… Model loaded")

# Äá»‹nh nghÄ©a metrics
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='weighted', zero_division=0
    )
    acc = accuracy_score(labels, preds)
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Training arguments
training_args = TrainingArguments(
    output_dir='./intent_classifier_output',
    num_train_epochs=5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    save_total_limit=2,
    fp16=torch.cuda.is_available(),  # Mixed precision náº¿u cÃ³ GPU
    report_to="none",  # Táº¯t wandb/tensorboard
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)

print("\nğŸ‹ï¸ Báº¯t Ä‘áº§u training...")
print("=" * 60)

# Train
trainer.train()

print("\nâœ… Training hoÃ n thÃ nh!")
==============================================================================================

cell code 6:
# Evaluate trÃªn test set
print("ğŸ“Š ÄÃ¡nh giÃ¡ trÃªn Test Set:")
print("=" * 60)

test_results = trainer.evaluate(test_dataset)
print(f"\nğŸ“ˆ Test Results:")
for key, value in test_results.items():
    print(f"   {key}: {value:.4f}")

# Detailed classification report
predictions = trainer.predict(test_dataset)
preds = predictions.predictions.argmax(-1)
labels = predictions.label_ids

print(f"\nğŸ“‹ Classification Report:")
print("=" * 60)
print(classification_report(
    labels,
    preds,
    target_names=[id_to_label[i] for i in range(len(id_to_label))],
    digits=4
))
==============================================================================================

cell code 7:
def predict_intent(text, model, tokenizer, label_mapping):
    """Dá»± Ä‘oÃ¡n intent cho má»™t cÃ¢u"""
    model.eval()
    
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=256,
        padding=True
    )
    
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        probs = torch.softmax(logits, dim=-1)
    
    # Top 3 predictions
    top_probs, top_indices = torch.topk(probs[0], k=3)
    
    print(f"\nğŸ“ Text: {text}")
    print(f"\nğŸ¯ Predictions:")
    for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):
        intent = label_mapping[idx]
        print(f"   {intent}: {prob:.4f} ({prob*100:.2f}%)")

# Test vá»›i cÃ¡c cÃ¢u máº«u
test_sentences = [
    "Máº­t Ä‘á»™ trá»“ng kinh giá»›i thÃ­ch há»£p?",
    "Dá»¯ liá»‡u cáº£m biáº¿n 1 giá» trÆ°á»›c Ä‘i",
    "LÃªn lá»‹ch mÃ¡y bÆ¡m má»—i 2 tiáº¿ng.",
    "BÃ¡o cÃ¡o chi phÃ­ theo tá»«ng háº¡ng má»¥c.",
    "Báº¡n lÃ  ai?"
]

print("\n" + "=" * 60)
print("ğŸ§ª TEST Vá»šI CÃC CÃ‚U MáºªU")
print("=" * 60)

for sentence in test_sentences:
    predict_intent(sentence, model, tokenizer, id_to_label)
    print("-" * 60)
==============================================================================================

cell code 8:
import os
import shutil

# Táº¡o thÆ° má»¥c output
output_dir = "./intent_classifier_final"
os.makedirs(output_dir, exist_ok=True)

print(f"ğŸ’¾ LÆ°u model vÃ o: {output_dir}")

# LÆ°u model vÃ  tokenizer
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

# LÆ°u label mapping
label_mapping = {
    "label_to_id": label_to_id,
    "id_to_label": id_to_label
}

with open(os.path.join(output_dir, "label_mapping.json"), "w", encoding="utf-8") as f:
    json.dump(label_mapping, f, ensure_ascii=False, indent=2)

print("âœ… Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u!")
print(f"\nğŸ“ CÃ¡c file trong {output_dir}:")
for file in os.listdir(output_dir):
    print(f"   - {file}")
==============================================================================================

cell code 9:
# NÃ©n thÆ° má»¥c model thÃ nh file zip
print("ğŸ“¦ Äang nÃ©n model...")
shutil.make_archive("intent_classifier_final", 'zip', output_dir)
print("âœ… ÄÃ£ nÃ©n xong!")

# Download file zip
print("\nğŸ“¥ Download file zip vá» mÃ¡y...")
files.download("intent_classifier_final.zip")

print("\n" + "=" * 60)
print("âœ… HOÃ€N THÃ€NH!")
print("=" * 60)
print("\nğŸ“ HÆ°á»›ng dáº«n sá»­ dá»¥ng:")
print("   1. Giáº£i nÃ©n file intent_classifier_final.zip")
print("   2. Copy toÃ n bá»™ ná»™i dung vÃ o thÆ° má»¥c:")
print("      apps/python-ai-service/models/intent_classifier/")
print("   3. Restart Python AI Service")
print("   4. Test API Ä‘á»ƒ kiá»ƒm tra model má»›i")
print("\nğŸ‰ ChÃºc má»«ng! Model Ä‘Ã£ Ä‘Æ°á»£c train thÃ nh cÃ´ng!")

================================================================================
1. Xá»­ lÃ½ dá»¯ liá»‡u
- Sá»­ dá»¥ng train_test_split vá»›i tham sá»‘ stratify
----> Ä‘á»ƒ giá»¯ nguyÃªn tá»‰ lá»‡ phÃ¢n bá»‘ cÃ¡c nhÃ£n á»Ÿ cáº£ 3 táº­p (Train,val,test) 
trÃ¡nh train nhiá»u nhÃ£n A nhÆ°ng test láº¡i toÃ n nhÃ£n B
2. Dataset & Tokenization
- Táº¡o lá»›p IntentDataset Ä‘á»ƒ chuyá»ƒn vÄƒn báº£n thÃ´ thÃ nh input_ids vÃ  attention_mask.
max_length = 256 lÃ  Ä‘á»§ bao quÃ¡t cÃ¡c cÃ¢u há»i cá»§a User
3. Huáº¥n luyá»‡n
- Sá»­ dá»¥ng mÃ´ hÃ¬nh AutoModelForSequenceClassification tá»« PhoBERT, cÃ³ EarlyStoppingCallback trÃ¡nh overfitting (há»c váº¹t)
4. ÄÃ¡nh giÃ¡ & xuáº¥t báº£n
- DÃ¹ng classification_report Ä‘á»ƒ xem chi tiáº¿t tá»«ng nhÃ£n vÃ  Ä‘Ã³ng gÃ³i model thÃ nh .zip Ä‘á»ƒ triá»ƒn khai thá»±c táº¿

CÃ¢u 3: Báº¡n cÃ³ thá»ƒ giáº£i thÃ­ch Ã½ nghÄ©a cá»§a attention_mask mÃ  mÃ´ hÃ¬nh sá»­ dá»¥ng khÃ´ng?
Tráº£ lá»i: VÃ¬ chÃºng ta dÃ¹ng padding='max_length', cÃ¡c cÃ¢u ngáº¯n sáº½ Ä‘Æ°á»£c thÃªm cÃ¡c token <pad> Ä‘á»ƒ Ä‘á»§ chiá»u dÃ i 256.
attention_mask (gá»“m cÃ¡c sá»‘ 0 vÃ  1) sáº½ bÃ¡o cho mÃ´ hÃ¬nh biáº¿t Ä‘Ã¢u lÃ  tá»« thá»±c (sá»‘ 1) vÃ  
Ä‘Ã¢u lÃ  pháº§n Ä‘á»‡m (sá»‘ 0) Ä‘á»ƒ mÃ´ hÃ¬nh khÃ´ng tÃ­nh toÃ¡n trÃªn cÃ¡c pháº§n Ä‘á»‡m Ä‘Ã³.