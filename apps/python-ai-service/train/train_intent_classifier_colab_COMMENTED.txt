================================================================================
NOTEBOOK HUáº¤N LUYá»†N INTENT CLASSIFIER - PHIÃŠN Báº¢N CÃ“ COMMENT CHI TIáº¾T
================================================================================
Má»¥c Ä‘Ã­ch: Giáº£i thÃ­ch Báº¢N CHáº¤T tá»«ng dÃ²ng code Ä‘á»ƒ hiá»ƒu sÃ¢u vá» quy trÃ¬nh Fine-tuning
================================================================================

cell code 1: CÃ€I Äáº¶T THÆ¯ VIá»†N
==============================================================================================
# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t (flag -q = quiet mode, Ã­t log hÆ¡n)
!pip install -q transformers datasets torch scikit-learn pandas numpy seqeval accelerate

# Báº¢N CHáº¤T Tá»ªNG THÆ¯ VIá»†N:
# - transformers: ThÆ° viá»‡n Hugging Face chá»©a PhoBERT vÃ  cÃ¡c cÃ´ng cá»¥ Fine-tune
# - datasets: Xá»­ lÃ½ dataset (khÃ´ng báº¯t buá»™c nhÆ°ng tiá»‡n lá»£i)
# - torch: PyTorch - Framework Deep Learning Ä‘á»ƒ train model
# - scikit-learn: TÃ­nh cÃ¡c metric (Accuracy, F1, Precision, Recall)
# - pandas: Xá»­ lÃ½ dá»¯ liá»‡u dáº¡ng báº£ng (CSV)
# - numpy: TÃ­nh toÃ¡n ma tráº­n, vector
# - seqeval: ÄÃ¡nh giÃ¡ sequence labeling (dÃ¹ng cho NER, á»Ÿ Ä‘Ã¢y cÃ i sáºµn cho Ä‘á»“ng bá»™)
# - accelerate: TÄƒng tá»‘c training trÃªn GPU/TPU, tá»± Ä‘á»™ng phÃ¡t hiá»‡n hardware

# Táº¯t Weights & Biases (W&B) - cÃ´ng cá»¥ tracking experiments
# Báº¢N CHáº¤T: W&B yÃªu cáº§u API key Ä‘á»ƒ log metrics lÃªn cloud. 
# VÃ¬ ta train trÃªn Colab miá»…n phÃ­ nÃªn táº¯t Ä‘i Ä‘á»ƒ trÃ¡nh lá»—i.
import os
os.environ['WANDB_DISABLED'] = 'true'  # Biáº¿n mÃ´i trÆ°á»ng bÃ¡o cho Trainer khÃ´ng dÃ¹ng W&B
print("âœ… ÄÃ£ táº¯t Weights & Biases tracking")

==========================================OK====================================================

cell code 2: UPLOAD VÃ€ KIá»‚M TRA Dá»® LIá»†U
==============================================================================================
from google.colab import files  # Module Ä‘á»ƒ upload/download file trÃªn Colab
import pandas as pd

# Upload file CSV tá»« mÃ¡y local lÃªn Colab
print("ğŸ“¤ Vui lÃ²ng upload file intent_data_augmented_161225.csv")
uploaded = files.upload()  
# Báº¢N CHáº¤T: Má»Ÿ cá»­a sá»• chá»n file, tráº£ vá» dictionary {filename: file_content}

# Láº¥y tÃªn file Ä‘áº§u tiÃªn (vÃ¬ chá»‰ upload 1 file)
data_file = list(uploaded.keys())[0]  # VD: "intent_data_augmented_161225.csv"
print(f"âœ… ÄÃ£ upload file: {data_file}")

# Äá»c file CSV thÃ nh DataFrame (báº£ng dá»¯ liá»‡u)
df = pd.read_csv(data_file)
# Báº¢N CHáº¤T: DataFrame cÃ³ 2 cá»™t:
#   - 'text': CÃ¢u há»i cá»§a user (VD: "Báº­t Ä‘Ã¨n")
#   - 'label': NhÃ£n Intent (VD: "device_control")

# In thá»‘ng kÃª cÆ¡ báº£n
print(f"\nğŸ“Š Tá»•ng sá»‘ máº«u: {len(df)}")  # VD: 7800 máº«u
print(f"\nğŸ“‹ CÃ¡c cá»™t: {df.columns.tolist()}")  # ['text', 'label']

# Äáº¿m sá»‘ lÆ°á»£ng máº«u cá»§a tá»«ng Intent
print(f"\nğŸ·ï¸ PhÃ¢n bá»‘ labels:")
print(df['label'].value_counts())
# Báº¢N CHáº¤T: Kiá»ƒm tra xem dá»¯ liá»‡u cÃ³ cÃ¢n báº±ng khÃ´ng (má»—i Intent cÃ³ sá»‘ lÆ°á»£ng tÆ°Æ¡ng Ä‘Æ°Æ¡ng)
# Náº¿u lá»‡ch quÃ¡ (VD: 5000 Control vs 100 Financial) -> Cáº§n xá»­ lÃ½ Imbalanced Data

print(f"\nğŸ‘€ Xem 5 máº«u Ä‘áº§u tiÃªn:")
df.head()  # Hiá»ƒn thá»‹ 5 dÃ²ng Ä‘áº§u Ä‘á»ƒ kiá»ƒm tra format

==============================================================================================

cell code 3: CHUáº¨N Bá»Š Dá»® LIá»†U & CHIA Táº¬P
==============================================================================================
import torch
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer
import json

# Kiá»ƒm tra xem cÃ³ GPU khÃ´ng
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# Báº¢N CHáº¤T: GPU nhanh hÆ¡n CPU hÃ ng chá»¥c láº§n khi train Deep Learning
# "cuda" = NVIDIA GPU, "cpu" = CPU thÆ°á»ng
print(f"ğŸ–¥ï¸ Device: {device}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")  # VD: "Tesla T4"

# ===== BÆ¯á»šC QUAN TRá»ŒNG: Táº O LABEL MAPPING =====
# Báº¢N CHáº¤T: AI chá»‰ hiá»ƒu sá»‘ (0, 1, 2...), khÃ´ng hiá»ƒu chá»¯ ("device_control")
# NÃªn ta pháº£i map: "device_control" -> 0, "sensor_query" -> 1, ...

unique_labels = sorted(df['label'].unique())  
# VD: ['device_control', 'financial_query', 'knowledge_query', 'sensor_query', 'unknown']

# Táº¡o dictionary: Chá»¯ -> Sá»‘
label_to_id = {label: idx for idx, label in enumerate(unique_labels)}
# VD: {'device_control': 0, 'financial_query': 1, ...}

# Táº¡o dictionary ngÆ°á»£c: Sá»‘ -> Chá»¯ (Ä‘á»ƒ decode káº¿t quáº£ sau nÃ y)
id_to_label = {idx: label for label, idx in label_to_id.items()}
# VD: {0: 'device_control', 1: 'financial_query', ...}

print(f"\nğŸ·ï¸ Label mapping:")
for label, idx in label_to_id.items():
    print(f"   {idx}: {label}")

# ThÃªm cá»™t 'label_id' vÃ o DataFrame (chuyá»ƒn chá»¯ -> sá»‘)
df['label_id'] = df['label'].map(label_to_id)
# Báº¢N CHáº¤T: Má»—i dÃ²ng giá» cÃ³ thÃªm cá»™t sá»‘ (0, 1, 2...) Ä‘á»ƒ model há»c

# ===== CHIA Dá»® LIá»†U: TRAIN / VALIDATION / TEST =====
# Báº¢N CHáº¤T:
# - Train (70%): Dá»¯ liá»‡u Ä‘á»ƒ model Há»ŒC
# - Validation (15%): Dá»¯ liá»‡u Ä‘á»ƒ CHá»ŒN model tá»‘t nháº¥t (Early Stopping)
# - Test (15%): Dá»¯ liá»‡u Ä‘á»ƒ ÄÃNH GIÃ cuá»‘i cÃ¹ng (hoÃ n toÃ n Ä‘á»™c láº­p)

# BÆ°á»›c 1: TÃ¡ch 70% Train, 30% cÃ²n láº¡i
train_df, temp_df = train_test_split(
    df, 
    test_size=0.3,  # 30% cho temp (sáº½ chia tiáº¿p thÃ nh Val + Test)
    random_state=42,  # Seed Ä‘á»ƒ káº¿t quáº£ láº·p láº¡i Ä‘Æ°á»£c
    stratify=df['label']  # QUAN TRá»ŒNG: Giá»¯ nguyÃªn tá»‰ lá»‡ cÃ¡c Intent
)
# Báº¢N CHáº¤T stratify: Náº¿u Train cÃ³ 40% Control, thÃ¬ Val vÃ  Test cÅ©ng pháº£i cÃ³ 40% Control
# TrÃ¡nh trÆ°á»ng há»£p Train toÃ n Control nhÆ°ng Test toÃ n Financial -> Model sáº½ fail

# BÆ°á»›c 2: Chia 30% temp thÃ nh 15% Val + 15% Test
val_df, test_df = train_test_split(
    temp_df, 
    test_size=0.5,  # 50% cá»§a 30% = 15%
    random_state=42, 
    stratify=temp_df['label']
)

print(f"\nğŸ“Š PhÃ¢n chia dá»¯ liá»‡u:")
print(f"   Train: {len(train_df)} máº«u")  # VD: 5460 máº«u
print(f"   Val:   {len(val_df)} máº«u")    # VD: 1170 máº«u
print(f"   Test:  {len(test_df)} máº«u")   # VD: 1170 máº«u

# ===== LOAD TOKENIZER =====
model_name = "vinai/phobert-base"
print(f"\nğŸ”¤ Loading tokenizer: {model_name}")
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Báº¢N CHáº¤T: Tokenizer lÃ  "tá»« Ä‘iá»ƒn" cá»§a PhoBERT
# NÃ³ biáº¿t cÃ¡ch tÃ¡ch tá»« tiáº¿ng Viá»‡t vÃ  chuyá»ƒn thÃ nh sá»‘ (Input IDs)
# VD: "Báº­t Ä‘Ã¨n" -> [5, 234, 1890] (má»—i sá»‘ á»©ng vá»›i 1 tá»« trong tá»« Ä‘iá»ƒn)
print("âœ… Tokenizer loaded")

==============================================================================================

cell code 4: Táº O DATASET CLASS
==============================================================================================
from torch.utils.data import Dataset

# Báº¢N CHáº¤T: Dataset lÃ  "cÃ¡i kho" chá»©a dá»¯ liá»‡u
# PyTorch yÃªu cáº§u ta pháº£i táº¡o class káº¿ thá»«a tá»« Dataset vÃ  implement 3 hÃ m:
# - __init__: Khá»Ÿi táº¡o (lÆ°u dá»¯ liá»‡u vÃ o bá»™ nhá»›)
# - __len__: Tráº£ vá» sá»‘ lÆ°á»£ng máº«u
# - __getitem__: Tráº£ vá» 1 máº«u cá»¥ thá»ƒ theo index

class IntentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        """
        Khá»Ÿi táº¡o Dataset
        Args:
            texts: List cÃ¡c cÃ¢u vÄƒn (VD: ["Báº­t Ä‘Ã¨n", "Táº¯t quáº¡t"])
            labels: List cÃ¡c nhÃ£n sá»‘ (VD: [0, 0])
            tokenizer: PhoBERT tokenizer
            max_length: Äá»™ dÃ i tá»‘i Ä‘a cá»§a cÃ¢u (256 tokens)
        """
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        """Tráº£ vá» tá»•ng sá»‘ máº«u"""
        return len(self.texts)
    
    def __getitem__(self, idx):
        """
        Láº¥y 1 máº«u theo index
        Báº¢N CHáº¤T: HÃ m nÃ y Ä‘Æ°á»£c gá»i má»—i khi model cáº§n 1 máº«u Ä‘á»ƒ train
        """
        text = str(self.texts[idx])  # Láº¥y cÃ¢u thá»© idx
        label = self.labels[idx]     # Láº¥y nhÃ£n thá»© idx
        
        # ===== TOKENIZATION (BÆ°á»›c quan trá»ng nháº¥t) =====
        encoding = self.tokenizer(
            text,
            truncation=True,  # Náº¿u cÃ¢u dÃ i hÆ¡n 256 -> Cáº¯t bá»›t
            max_length=self.max_length,  # Äá»™ dÃ i tá»‘i Ä‘a
            padding='max_length',  # Náº¿u cÃ¢u ngáº¯n hÆ¡n 256 -> ThÃªm <pad>
            return_tensors='pt'  # Tráº£ vá» dáº¡ng PyTorch Tensor
        )
        # Báº¢N CHáº¤T encoding:
        # - input_ids: [101, 5, 234, 1890, 102, 0, 0, ..., 0] (256 sá»‘)
        #   + 101 = <s> (token báº¯t Ä‘áº§u)
        #   + 5, 234, 1890 = "Báº­t", "Ä‘Ã¨n" (tá»« tháº­t)
        #   + 102 = </s> (token káº¿t thÃºc)
        #   + 0, 0, ... = <pad> (padding Ä‘á»ƒ Ä‘á»§ 256)
        # - attention_mask: [1, 1, 1, 1, 1, 0, 0, ..., 0] (256 sá»‘)
        #   + 1 = Tá»« tháº­t (model cáº§n chÃº Ã½)
        #   + 0 = Padding (model bá» qua)
        
        return {
            'input_ids': encoding['input_ids'].flatten(),  # Shape: [256]
            'attention_mask': encoding['attention_mask'].flatten(),  # Shape: [256]
            'labels': torch.tensor(label, dtype=torch.long)  # Shape: [] (scalar)
        }
        # Báº¢N CHáº¤T: Má»—i láº§n gá»i __getitem__, ta tráº£ vá» 1 dictionary
        # Trainer sáº½ tá»± Ä‘á»™ng gá»™p nhiá»u máº«u thÃ nh 1 batch (VD: 16 máº«u)

# Táº¡o 3 Dataset objects
train_dataset = IntentDataset(
    train_df['text'].tolist(),     # List cÃ¢u vÄƒn
    train_df['label_id'].tolist(), # List nhÃ£n sá»‘
    tokenizer
)

val_dataset = IntentDataset(
    val_df['text'].tolist(),
    val_df['label_id'].tolist(),
    tokenizer
)

test_dataset = IntentDataset(
    test_df['text'].tolist(),
    test_df['label_id'].tolist(),
    tokenizer
)

print("âœ… Datasets created")

==============================================================================================

cell code 5: TRAINING (Pháº§n cá»‘t lÃµi nháº¥t)
==============================================================================================
from transformers import (
    AutoModelForSequenceClassification,  # Model cho bÃ i toÃ¡n phÃ¢n loáº¡i chuá»—i
    TrainingArguments,  # Cáº¥u hÃ¬nh training
    Trainer,  # Class quáº£n lÃ½ toÃ n bá»™ quÃ¡ trÃ¬nh train
    EarlyStoppingCallback  # Dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report
import numpy as np

# ===== LOAD MODEL =====
print(f"ğŸ¤– Loading model: {model_name}")
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,  # "vinai/phobert-base"
    num_labels=len(label_to_id)  # 5 (sá»‘ lÆ°á»£ng Intent)
)
# Báº¢N CHáº¤T: HÃ m nÃ y lÃ m 2 viá»‡c:
# 1. Táº£i PhoBERT gá»‘c (12 lá»›p Transformer) tá»« Hugging Face
# 2. Cáº®T Bá» Head cÅ© (Masked LM) vÃ  Gáº®N Head má»›i (Linear Layer 768 -> 5)
#    Head má»›i cÃ³ cÃ´ng thá»©c: y = Wx + b (W lÃ  ma tráº­n 768x5)

model.to(device)  # Chuyá»ƒn model lÃªn GPU (náº¿u cÃ³)
print("âœ… Model loaded")

# ===== Äá»ŠNH NGHÄ¨A HÃ€M TÃNH METRICS =====
def compute_metrics(pred):
    """
    HÃ m tÃ­nh cÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡
    Báº¢N CHáº¤T: Trainer sáº½ gá»i hÃ m nÃ y sau má»—i epoch Ä‘á»ƒ tÃ­nh F1, Accuracy...
    """
    labels = pred.label_ids  # NhÃ£n tháº­t (Ground Truth)
    preds = pred.predictions.argmax(-1)  # NhÃ£n dá»± Ä‘oÃ¡n (láº¥y max cá»§a Softmax)
    # Báº¢N CHáº¤T argmax(-1):
    # - predictions shape: [Batch, 5] (5 Ä‘iá»ƒm sá»‘ cho 5 Intent)
    # - argmax(-1) = Láº¥y index cá»§a Ä‘iá»ƒm cao nháº¥t
    # VD: [0.1, 0.8, 0.05, 0.03, 0.02] -> argmax = 1 (Intent thá»© 1)
    
    # TÃ­nh Precision, Recall, F1
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, 
        average='weighted',  # Trung bÃ¬nh cÃ³ trá»ng sá»‘ (quan trá»ng khi Imbalanced)
        zero_division=0  # TrÃ¡nh chia cho 0
    )
    # Báº¢N CHáº¤T 'weighted':
    # - Náº¿u Intent A cÃ³ 1000 máº«u, Intent B cÃ³ 100 máº«u
    # - F1 tá»•ng = (F1_A * 1000 + F1_B * 100) / 1100
    # -> Æ¯u tiÃªn Intent cÃ³ nhiá»u máº«u hÆ¡n
    
    acc = accuracy_score(labels, preds)  # Tá»‰ lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng
    
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# ===== Cáº¤U HÃŒNH TRAINING =====
training_args = TrainingArguments(
    output_dir='./intent_classifier_output',  # ThÆ° má»¥c lÆ°u checkpoint
    num_train_epochs=5,  # Sá»‘ epoch (láº§n láº·p qua toÃ n bá»™ Train Set)
    # Báº¢N CHáº¤T: 1 epoch = model nhÃ¬n qua háº¿t 5460 máº«u Train 1 láº§n
    
    per_device_train_batch_size=16,  # Batch size khi train
    # Báº¢N CHáº¤T: Má»—i láº§n cáº­p nháº­t trá»ng sá»‘, model há»c tá»« 16 máº«u cÃ¹ng lÃºc
    # Táº¡i sao khÃ´ng Batch=1? -> Cháº­m vÃ  gradient "giáº­t cá»¥c"
    # Táº¡i sao khÃ´ng Batch=1000? -> TrÃ n VRAM vÃ  gradient "trÆ¡"
    
    per_device_eval_batch_size=32,  # Batch size khi Ä‘Ã¡nh giÃ¡ (cÃ³ thá»ƒ lá»›n hÆ¡n vÃ¬ khÃ´ng cáº§n tÃ­nh gradient)
    
    warmup_steps=500,  # Sá»‘ bÆ°á»›c "khá»Ÿi Ä‘á»™ng"
    # Báº¢N CHáº¤T: 500 bÆ°á»›c Ä‘áº§u, Learning Rate tÄƒng dáº§n tá»« 0 -> LR_max
    # Táº¡i sao? TrÃ¡nh model "sá»‘c" khi má»›i báº¯t Ä‘áº§u train
    
    weight_decay=0.01,  # L2 Regularization
    # Báº¢N CHáº¤T: Pháº¡t cÃ¡c trá»ng sá»‘ quÃ¡ lá»›n Ä‘á»ƒ trÃ¡nh Overfitting
    # Loss_total = Loss_classification + 0.01 * (sum of W^2)
    
    logging_dir='./logs',  # ThÆ° má»¥c lÆ°u log
    logging_steps=50,  # Log má»—i 50 bÆ°á»›c
    
    eval_strategy="epoch",  # ÄÃ¡nh giÃ¡ sau má»—i epoch
    save_strategy="epoch",  # LÆ°u checkpoint sau má»—i epoch
    
    load_best_model_at_end=True,  # QUAN TRá»ŒNG
    # Báº¢N CHáº¤T: Sau khi train xong, load láº¡i checkpoint Tá»T NHáº¤T (F1 cao nháº¥t)
    # Táº¡i sao? VÃ¬ Epoch cuá»‘i chÆ°a cháº¯c Ä‘Ã£ tá»‘t nháº¥t (cÃ³ thá»ƒ Overfitting)
    
    metric_for_best_model="f1",  # DÃ¹ng F1 Ä‘á»ƒ chá»n model tá»‘t nháº¥t
    greater_is_better=True,  # F1 cÃ ng cao cÃ ng tá»‘t
    
    save_total_limit=2,  # Chá»‰ giá»¯ 2 checkpoint gáº§n nháº¥t (tiáº¿t kiá»‡m dung lÆ°á»£ng)
    
    fp16=torch.cuda.is_available(),  # Mixed Precision Training
    # Báº¢N CHáº¤T: DÃ¹ng sá»‘ Float16 (thay vÃ¬ Float32) Ä‘á»ƒ train nhanh hÆ¡n
    # Chá»‰ hoáº¡t Ä‘á»™ng trÃªn GPU cÃ³ Tensor Cores (VD: T4, V100, A100)
    
    report_to="none",  # Táº¯t W&B/TensorBoard
)

# ===== Táº O TRAINER =====
trainer = Trainer(
    model=model,  # Model cáº§n train
    args=training_args,  # Cáº¥u hÃ¬nh training
    train_dataset=train_dataset,  # Dá»¯ liá»‡u Train
    eval_dataset=val_dataset,  # Dá»¯ liá»‡u Validation
    compute_metrics=compute_metrics,  # HÃ m tÃ­nh metrics
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
    # Báº¢N CHáº¤T EarlyStopping:
    # - Náº¿u F1 khÃ´ng cáº£i thiá»‡n sau 2 epoch liÃªn tiáº¿p -> Dá»«ng train
    # - Táº¡i sao? TrÃ¡nh lÃ£ng phÃ­ thá»i gian vÃ  Overfitting
)

print("\nğŸ‹ï¸ Báº¯t Ä‘áº§u training...")
print("=" * 60)

# ===== Báº®T Äáº¦U TRAINING =====
trainer.train()
# Báº¢N CHáº¤T: HÃ m nÃ y cháº¡y vÃ²ng láº·p:
# for epoch in range(5):
#     for batch in train_dataset:  # Má»—i batch cÃ³ 16 máº«u
#         # 1. Forward Pass: TÃ­nh output
#         outputs = model(batch)
#         logits = outputs.logits  # Shape: [16, 5]
#         
#         # 2. TÃ­nh Loss (CrossEntropyLoss)
#         loss = CrossEntropyLoss(logits, labels)
#         
#         # 3. Backward Pass: TÃ­nh gradient
#         loss.backward()
#         
#         # 4. Update trá»ng sá»‘
#         optimizer.step()  # W_new = W_old - LR * gradient
#         
#     # 5. ÄÃ¡nh giÃ¡ trÃªn Validation Set
#     val_metrics = evaluate(val_dataset)
#     
#     # 6. Kiá»ƒm tra Early Stopping
#     if val_metrics['f1'] khÃ´ng cáº£i thiá»‡n:
#         patience_counter += 1
#         if patience_counter >= 2:
#             break  # Dá»«ng train

print("\nâœ… Training hoÃ n thÃ nh!")

==============================================================================================

cell code 6: ÄÃNH GIÃ TRÃŠN TEST SET
==============================================================================================
# ÄÃ¡nh giÃ¡ trÃªn Test Set (dá»¯ liá»‡u hoÃ n toÃ n Ä‘á»™c láº­p)
print("ğŸ“Š ÄÃ¡nh giÃ¡ trÃªn Test Set:")
print("=" * 60)

test_results = trainer.evaluate(test_dataset)
# Báº¢N CHáº¤T: Cháº¡y model trÃªn Test Set, tÃ­nh Loss vÃ  Metrics
# KHÃ”NG Cáº¬P NHáº¬T trá»ng sá»‘ (chá»‰ Ä‘Ã¡nh giÃ¡)

print(f"\nğŸ“ˆ Test Results:")
for key, value in test_results.items():
    print(f"   {key}: {value:.4f}")

# ===== CLASSIFICATION REPORT CHI TIáº¾T =====
predictions = trainer.predict(test_dataset)
# Báº¢N CHáº¤T: Dá»± Ä‘oÃ¡n toÃ n bá»™ Test Set, tráº£ vá»:
# - predictions: Logits (Ä‘iá»ƒm thÃ´)
# - label_ids: NhÃ£n tháº­t

preds = predictions.predictions.argmax(-1)  # Chuyá»ƒn Logits -> NhÃ£n dá»± Ä‘oÃ¡n
labels = predictions.label_ids  # NhÃ£n tháº­t

print(f"\nğŸ“‹ Classification Report:")
print("=" * 60)
print(classification_report(
    labels,
    preds,
    target_names=[id_to_label[i] for i in range(len(id_to_label))],
    digits=4
))
# Báº¢N CHáº¤T: In ra báº£ng chi tiáº¿t cho Tá»ªNG Intent:
#                     precision    recall  f1-score   support
# device_control         0.99      0.99      0.99       234
# sensor_query           0.98      0.99      0.98       220
# ...
# GiÃºp phÃ¡t hiá»‡n Intent nÃ o model cÃ²n yáº¿u

==============================================================================================

cell code 7: TEST Vá»šI CÃ‚U MáºªU
==============================================================================================
def predict_intent(text, model, tokenizer, label_mapping):
    """
    Dá»± Ä‘oÃ¡n Intent cho 1 cÃ¢u
    Báº¢N CHáº¤T: HÃ m nÃ y mÃ´ phá»ng cÃ¡ch API sáº½ dÃ¹ng model trong thá»±c táº¿
    """
    model.eval()  # Chuyá»ƒn model sang cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡ (táº¯t Dropout)
    
    # Tokenize cÃ¢u input
    inputs = tokenizer(
        text,
        return_tensors="pt",  # Tráº£ vá» PyTorch Tensor
        truncation=True,
        max_length=256,
        padding=True
    )
    
    # Chuyá»ƒn input lÃªn GPU (náº¿u cÃ³)
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # Dá»± Ä‘oÃ¡n (khÃ´ng tÃ­nh gradient)
    with torch.no_grad():  # Táº¯t gradient Ä‘á»ƒ tiáº¿t kiá»‡m RAM
        outputs = model(**inputs)
        logits = outputs.logits  # Shape: [1, 5] (1 cÃ¢u, 5 Ä‘iá»ƒm)
        probs = torch.softmax(logits, dim=-1)  # Chuyá»ƒn Logits -> XÃ¡c suáº¥t
        # Báº¢N CHáº¤T Softmax:
        # - Input: [2.5, -1.0, 0.5, -2.0, 0.1]
        # - Output: [0.85, 0.05, 0.08, 0.01, 0.01] (tá»•ng = 1)
    
    # Láº¥y Top 3 Intent cÃ³ xÃ¡c suáº¥t cao nháº¥t
    top_probs, top_indices = torch.topk(probs[0], k=3)
    
    print(f"\nğŸ“ Text: {text}")
    print(f"\nğŸ¯ Predictions:")
    for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):
        intent = label_mapping[idx]
        print(f"   {intent}: {prob:.4f} ({prob*100:.2f}%)")

# Test vá»›i cÃ¡c cÃ¢u máº«u
test_sentences = [
    "Máº­t Ä‘á»™ trá»“ng kinh giá»›i thÃ­ch há»£p?",  # -> knowledge_query
    "Dá»¯ liá»‡u cáº£m biáº¿n 1 giá» trÆ°á»›c Ä‘i",    # -> sensor_query
    "LÃªn lá»‹ch mÃ¡y bÆ¡m má»—i 2 tiáº¿ng.",      # -> device_control
    "BÃ¡o cÃ¡o chi phÃ­ theo tá»«ng háº¡ng má»¥c.", # -> financial_query
    "Báº¡n lÃ  ai?"                           # -> unknown
]

print("\n" + "=" * 60)
print("ğŸ§ª TEST Vá»šI CÃC CÃ‚U MáºªU")
print("=" * 60)

for sentence in test_sentences:
    predict_intent(sentence, model, tokenizer, id_to_label)
    print("-" * 60)

==============================================================================================

cell code 8: LÆ¯U MODEL
==============================================================================================
import os
import shutil

# Táº¡o thÆ° má»¥c output
output_dir = "./intent_classifier_final"
os.makedirs(output_dir, exist_ok=True)  # exist_ok=True: KhÃ´ng lá»—i náº¿u thÆ° má»¥c Ä‘Ã£ tá»“n táº¡i

print(f"ğŸ’¾ LÆ°u model vÃ o: {output_dir}")

# ===== LÆ¯U MODEL VÃ€ TOKENIZER =====
model.save_pretrained(output_dir)
# Báº¢N CHáº¤T: LÆ°u 2 file:
# - pytorch_model.bin: Trá»ng sá»‘ cá»§a model (khoáº£ng 400MB)
# - config.json: Cáº¥u hÃ¬nh model (sá»‘ lá»›p, hidden size...)

tokenizer.save_pretrained(output_dir)
# Báº¢N CHáº¤T: LÆ°u:
# - vocab.txt: Tá»« Ä‘iá»ƒn (30000 tá»«)
# - tokenizer_config.json: Cáº¥u hÃ¬nh tokenizer

# ===== LÆ¯U LABEL MAPPING (QUAN TRá»ŒNG) =====
label_mapping = {
    "label_to_id": label_to_id,  # {"device_control": 0, ...}
    "id_to_label": id_to_label   # {0: "device_control", ...}
}

with open(os.path.join(output_dir, "label_mapping.json"), "w", encoding="utf-8") as f:
    json.dump(label_mapping, f, ensure_ascii=False, indent=2)
# Báº¢N CHáº¤T: Backend cáº§n file nÃ y Ä‘á»ƒ biáº¿t sá»‘ 0 á»©ng vá»›i Intent nÃ o
# Náº¿u khÃ´ng cÃ³ file nÃ y, model chá»‰ tráº£ vá» sá»‘ (0, 1, 2...) -> VÃ´ nghÄ©a

print("âœ… Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u!")
print(f"\nğŸ“ CÃ¡c file trong {output_dir}:")
for file in os.listdir(output_dir):
    print(f"   - {file}")

==============================================================================================

cell code 9: NÃ‰N VÃ€ DOWNLOAD
==============================================================================================
# NÃ©n thÆ° má»¥c model thÃ nh file .zip
print("ğŸ“¦ Äang nÃ©n model...")
shutil.make_archive("intent_classifier_final", 'zip', output_dir)
# Báº¢N CHáº¤T: Táº¡o file intent_classifier_final.zip chá»©a toÃ n bá»™ model
print("âœ… ÄÃ£ nÃ©n xong!")

# Download file .zip vá» mÃ¡y local
print("\nğŸ“¥ Download file zip vá» mÃ¡y...")
files.download("intent_classifier_final.zip")
# Báº¢N CHáº¤T: Má»Ÿ cá»­a sá»• download trÃªn trÃ¬nh duyá»‡t

print("\n" + "=" * 60)
print("âœ… HOÃ€N THÃ€NH!")
print("=" * 60)
print("\nğŸ“ HÆ°á»›ng dáº«n sá»­ dá»¥ng:")
print("   1. Giáº£i nÃ©n file intent_classifier_final.zip")
print("   2. Copy toÃ n bá»™ ná»™i dung vÃ o thÆ° má»¥c:")
print("      apps/python-ai-service/models/intent_classifier/")
print("   3. Restart Python AI Service")
print("   4. Test API Ä‘á»ƒ kiá»ƒm tra model má»›i")
print("\nğŸ‰ ChÃºc má»«ng! Model Ä‘Ã£ Ä‘Æ°á»£c train thÃ nh cÃ´ng!")

================================================================================
Tá»”NG Káº¾T Báº¢N CHáº¤T
================================================================================
1. TOKENIZATION: Chuyá»ƒn chá»¯ -> sá»‘ (input_ids) + táº¡o attention_mask
2. LABEL MAPPING: Chuyá»ƒn nhÃ£n chá»¯ -> sá»‘ Ä‘á»ƒ model há»c
3. DATASET: Tá»• chá»©c dá»¯ liá»‡u thÃ nh format PyTorch yÃªu cáº§u
4. MODEL: Load PhoBERT + Thay Head (Masked LM -> Linear Layer 768->5)
5. TRAINING: VÃ²ng láº·p Forward -> Loss -> Backward -> Update Weights
6. EARLY STOPPING: Dá»«ng sá»›m náº¿u F1 khÃ´ng cáº£i thiá»‡n (trÃ¡nh Overfitting)
7. EVALUATION: ÄÃ¡nh giÃ¡ trÃªn Test Set (dá»¯ liá»‡u Ä‘á»™c láº­p)
8. SAVE: LÆ°u model + tokenizer + label_mapping Ä‘á»ƒ deploy

================================================================================
CÃ‚U Há»I THÆ¯á»œNG Gáº¶P Tá»ª Há»˜I Äá»’NG
================================================================================
Q1: "Táº¡i sao dÃ¹ng stratify khi chia dá»¯ liá»‡u?"
A1: "Äá»ƒ giá»¯ nguyÃªn tá»‰ lá»‡ cÃ¡c Intent á»Ÿ cáº£ 3 táº­p (Train/Val/Test). TrÃ¡nh Train 
    toÃ n Control nhÆ°ng Test toÃ n Financial -> Model sáº½ fail."

Q2: "attention_mask lÃ  gÃ¬?"
A2: "VÃ¬ dÃ¹ng padding='max_length', cÃ¢u ngáº¯n Ä‘Æ°á»£c thÃªm <pad> Ä‘á»ƒ Ä‘á»§ 256 tokens.
    attention_mask (0 vÃ  1) bÃ¡o cho model biáº¿t Ä‘Ã¢u lÃ  tá»« tháº­t (1) vÃ  Ä‘Ã¢u lÃ  
    padding (0) Ä‘á»ƒ model khÃ´ng tÃ­nh toÃ¡n trÃªn padding."

Q3: "Táº¡i sao load_best_model_at_end=True?"
A3: "Trong quÃ¡ trÃ¬nh train, model lÆ°u nhiá»u checkpoint. CÃ¡i nÃ y Ä‘áº£m báº£o cuá»‘i 
    cÃ¹ng ta giá»¯ checkpoint Tá»T NHáº¤T (F1 cao nháº¥t trÃªn Val), khÃ´ng pháº£i checkpoint 
    cuá»‘i cÃ¹ng (cÃ³ thá»ƒ Overfitting)."

Q4: "LÃ m sao biáº¿t model khÃ´ng Overfitting?"
A4: "NhÃ¬n vÃ o Validation Loss. Náº¿u Training Loss giáº£m nhÆ°ng Validation Loss 
    TÄ‚NG -> Overfitting. á» Ä‘Ã¢y Validation Loss giáº£m Ä‘á»u (0.111 -> 0.051) nÃªn 
    model há»c tá»•ng quÃ¡t hÃ³a tá»‘t."

Q5: "F1-score 0.99 cÃ³ quÃ¡ cao khÃ´ng?"
A5: "KhÃ´ng. BÃ i toÃ¡n phÃ¢n loáº¡i 5 Intent vá»›i dá»¯ liá»‡u augment tá»‘t (7800 máº«u) 
    vÃ  dÃ¹ng PhoBERT pre-trained nÃªn F1=0.99 lÃ  há»£p lÃ½. Em cÅ©ng cÃ³ Test Set 
    riÃªng (15%) Ä‘á»ƒ kiá»ƒm chá»©ng."
================================================================================
