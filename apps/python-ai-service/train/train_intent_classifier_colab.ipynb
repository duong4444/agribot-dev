{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ¾ Train Intent Classifier - PhoBERT for Agricultural Chatbot\n",
    "\n",
    "Notebook nÃ y dÃ¹ng Ä‘á»ƒ train láº¡i model Intent Classification vá»›i data má»›i.\n",
    "\n",
    "**Model**: PhoBERT (vinai/phobert-base)  \n",
    "**Task**: Multi-class Intent Classification  \n",
    "**Labels**: knowledge_query, financial_query, device_control, sensor_query, unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1. CÃ i Ä‘áº·t thÆ° viá»‡n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch scikit-learn pandas numpy seqeval accelerate\n",
    "# transformers: ThÆ° viá»‡n HuggingFace (chá»©a PhoBERT).\n",
    "# datasets: Xá»­ lÃ½ dataset.\n",
    "# torch: PyTorch (framework AI).\n",
    "# scikit-learn: TÃ­nh metrics (accuracy, F1...).\n",
    "# pandas: Xá»­ lÃ½ data dáº¡ng báº£ng.\n",
    "# seqeval: ÄÃ¡nh giÃ¡ sequence labeling.\n",
    "# accelerate: TÄƒng tá»‘c training.\n",
    "\n",
    "# Táº¯t wandb Ä‘á»ƒ trÃ¡nh yÃªu cáº§u API key\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "print(\"âœ… ÄÃ£ táº¯t Weights & Biases tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ 2. Upload file data\n",
    "\n",
    "Upload file `intent_data_augmented_abcxyz.csv` tá»« mÃ¡y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import pandas as pd\n",
    "\n",
    "# Upload file CSV\n",
    "print(\" Vui lÃ²ng upload file intent_data_augmented.csv\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Láº¥y tÃªn file Ä‘áº§u tiÃªn Ä‘Æ°á»£c upload\n",
    "data_file = list(uploaded.keys())[0]\n",
    "print(f\"âœ… ÄÃ£ upload file: {data_file}\")\n",
    "\n",
    "# Äá»c vÃ  kiá»ƒm tra data\n",
    "# Ä‘á»c file csv thÃ nh dataframe\n",
    "# text\t                 |       label\n",
    "# \"Báº­t Ä‘Ã¨n khu A\"\t     |   device_control\n",
    "# \"Doanh thu thÃ¡ng nÃ y\"\t |   financial_query\n",
    "# \"CÃ¡ch trá»“ng lÃºa\"\t     |   knowledge_query\n",
    "df = pd.read_csv(data_file)\n",
    "print(f\"\\nğŸ“Š Tá»•ng sá»‘ máº«u: {len(df)}\")\n",
    "print(f\"\\nğŸ“‹ CÃ¡c cá»™t: {df.columns.tolist()}\")\n",
    "print(f\"\\nğŸ·ï¸ PhÃ¢n bá»‘ labels:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nğŸ‘€ Xem 5 máº«u Ä‘áº§u tiÃªn:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ 3. Chuáº©n bá»‹ dá»¯ liá»‡u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "# Kiá»ƒm tra GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Táº¡o label mapping\n",
    "unique_labels = sorted(df['label'].unique()) #sort theo alphabet\n",
    "# ['device_control', 'financial_query', 'knowledge_query', 'sensor_query', 'unknown']\n",
    "# Model AI chá»‰ hiá»ƒu sá»‘, ko hiá»ƒu chá»¯ ====> cáº§n map chá»¯(intent_label) -> sá»‘(index)\n",
    "label_to_id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "# {\n",
    "#   'device_control': 0,\n",
    "#   'financial_query': 1,\n",
    "#   'knowledge_query': 2,\n",
    "#   'sensor_query': 3,\n",
    "#   'unknown': 4\n",
    "# }\n",
    "id_to_label = {idx: label for label, idx in label_to_id.items()}\n",
    "# {\n",
    "#     \"0\": \"device_control\",\n",
    "#     \"1\": \"financial_query\",\n",
    "#     \"2\": \"knowledge_query\",\n",
    "#     \"3\": \"sensor_query\",\n",
    "#     \"4\": \"unknown\"\n",
    "#   }\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ Label mapping:\")\n",
    "# ğŸ·ï¸ Label mapping:\n",
    "#    0: device_control\n",
    "#    1: financial_query\n",
    "#    2: knowledge_query\n",
    "#    3: sensor_query\n",
    "#    4: unknown\n",
    "for label, idx in label_to_id.items():\n",
    "    print(f\"   {idx}: {label}\")\n",
    "\n",
    "# ThÃªm cá»™t label_id\n",
    "df['label_id'] = df['label'].map(label_to_id)\n",
    "\n",
    "# Chia train/val/test (70/15/15)\n",
    "# train 70% dÃ¹ng Ä‘á»ƒ há»c, validation 15% dÃ¹ng Ä‘á»ƒ Ä‘iá»u chá»‰nh trong quÃ¡ trÃ¬nh há»c\n",
    "# test 15% dÃ¹ng Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ cuá»‘i cÃ¹ng\n",
    "# láº¥y 30% cho dá»¯ liá»‡u cho pháº§n thá»© 2\n",
    "# stratify Giá»¯ tá»· lá»‡ cÃ¡c label giá»‘ng nhau á»Ÿ cáº£ 2 pháº§n.\n",
    "# random_state=42\n",
    "# Seed cho random number generator.\n",
    "# Äáº£m báº£o má»—i láº§n cháº¡y code, káº¿t quáº£ chia giá»‘ng nhau.\n",
    "# Sá»‘ 42 khÃ´ng cÃ³ Ã½ nghÄ©a Ä‘áº·c biá»‡t (cÃ³ thá»ƒ dÃ¹ng sá»‘ khÃ¡c).\n",
    "# Data gá»‘c (1000 máº«u)\n",
    "#     |\n",
    "#     v\n",
    "# [train_test_split] (test_size=0.3)\n",
    "#     |\n",
    "#     +---> train_df (700 máº«u - 70%)\n",
    "#     |\n",
    "#     +---> temp_df (300 máº«u - 30%)\n",
    "#               |\n",
    "#               v\n",
    "#           [train_test_split] (test_size=0.5)\n",
    "#               |\n",
    "#               +---> val_df (150 máº«u - 15%)\n",
    "#               |\n",
    "#               +---> test_df (150 máº«u - 15%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n",
    "\n",
    "print(f\"\\nğŸ“Š PhÃ¢n chia dá»¯ liá»‡u:\")\n",
    "print(f\"   Train: {len(train_df)} máº«u\")\n",
    "print(f\"   Val:   {len(val_df)} máº«u\")\n",
    "print(f\"   Test:  {len(test_df)} máº«u\")\n",
    "\n",
    "# Load tokenizer cá»§a model\n",
    "model_name = \"vinai/phobert-base\"\n",
    "print(f\"\\nğŸ”¤ Loading tokenizer: {model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(\"âœ… Tokenizer loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ 4. Táº¡o Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# Dataset lÃ  1 class Ä‘áº·c biá»‡t giÃºp lÆ°u trá»¯ dlieu(texts,labels),\n",
    "# cung cáº¥p cÃ¡ch láº¥y máº«u dá»¯ liá»‡u\n",
    "# tá»± Ä‘á»™ng tokenize khi cáº§n\n",
    "# extends class Dataset\n",
    "\"\"\"\n",
    "Táº¡o 1 khuÃ´n máº«u Ä‘á»ƒ Ä‘Ã³ng gÃ³i dá»¯ liá»‡u\n",
    "Má»—i cÃ¢u vÄƒn(text) Ä‘c chuyá»ƒn thÃ nh:\n",
    " - input_ids: Chuá»—i sá»‘ Ä‘áº¡i diá»‡n cho cÃ¢u vÄƒn\n",
    " - attention_mask: ÄÃ¡nh dáº¥u pháº§n nÃ o lÃ  vÄƒn báº£n tháº­t, pháº§n nÃ o lÃ  padding (Ä‘á»‡m)\n",
    " - labels: NhÃ£n Ã½ Ä‘á»‹nh (dáº¡ng sá»‘)\n",
    "\"\"\"\n",
    "class IntentDataset(Dataset):\n",
    "    # lÆ°u trá»¯ texts, labels, tokenizer\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts #list cÃ¡c cÃ¢u text\n",
    "        self.labels = labels#list cÃ¡c label id\n",
    "        self.tokenizer = tokenizer#chuyá»ƒn chá»¯ -> token ids\n",
    "        self.max_length = max_length#limit 256 token\n",
    "    # pytorch cáº§n biáº¿t cÃ³ bao nhiÃªu máº«u Ä‘á»ƒ táº¡o batches(chia dá»¯ liá»‡u thÃ nh nhÃ³m)\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    # tráº£ vá» sá»‘ lÆ°á»£ng máº«u trong dataset\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])# láº¥y cÃ¢u thá»© idx\n",
    "        label = self.labels[idx]#láº¥y label thá»© idx\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True, #cáº¯t náº¿u quÃ¡ dÃ i\n",
    "            max_length=self.max_length,#tá»‘i Ä‘a 256 token\n",
    "            padding='max_length',#thÃªm padding Ä‘á»ƒ Ä‘á»§ 256\n",
    "            return_tensors='pt'#tráº£ vá» pytorch tensor\n",
    "        )\n",
    "        #tráº£ vá» dict chá»©a \n",
    "        return {\n",
    "            # tokenizer tráº£ vá» shape 2D\n",
    "            # PyTorch Dataloader cáº§n shape 1D\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \"\"\"\n",
    "    Ä‘c gá»i khi láº¥y 1 máº«u dataset[0], tokenize text thÃ nh sá»‘\n",
    "    tráº£ vá» dict chá»©a \n",
    "    input_ids: Máº£ng ID cá»§a tokens.\n",
    "    attention_mask: Máº£ng Ä‘Ã¡nh dáº¥u token nÃ o lÃ  tháº­t, token nÃ o lÃ  padding.\n",
    "    labels: Label ID (0, 1, 2...).\n",
    "    \"\"\"\n",
    "# Táº¡o datasets cho train, val, test\n",
    "train_dataset = IntentDataset(\n",
    "    train_df['text'].tolist(),\n",
    "    train_df['label_id'].tolist(),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = IntentDataset(\n",
    "    val_df['text'].tolist(),\n",
    "    val_df['label_id'].tolist(),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = IntentDataset(\n",
    "    test_df['text'].tolist(),\n",
    "    test_df['label_id'].tolist(),\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Text (cÃ¢u ngÆ°á»i dÃ¹ng)\n",
    "#  â†’ Dataset\n",
    "#  â†’ DataLoader (batch)\n",
    "#  â†’ Tokenizer\n",
    "#  â†’ Model\n",
    "#  â†’ Loss / Prediction\n",
    "print(\"âœ… Datasets created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 5. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "Táº£i model PhoBERT\n",
    "num_train_epochs=5: Há»c láº·p láº¡i 5 láº§n toÃ n bá»™ dá»¯ liá»‡u\n",
    "batch_size=16: Má»—i láº§n há»c 16 cÃ¢u cÃ¹ng lÃºc\n",
    "early_stopping: Dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n sau 2 láº§n kiá»ƒm tra\n",
    "Accuracy: Tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng\n",
    "F1 Score: Äiá»ƒm cÃ¢n báº±ng giá»¯a precision vÃ  recall\n",
    "Precision: Äá»™ chÃ­nh xÃ¡c khi dá»± Ä‘oÃ¡n\n",
    "Recall: Kháº£ nÄƒng tÃ¬m ra táº¥t cáº£ cÃ¡c trÆ°á»ng há»£p Ä‘Ãºng\n",
    "\"\"\"\n",
    "# Load model\n",
    "print(f\"ğŸ¤– Loading model: {model_name}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_to_id) #5 intent\n",
    ")\n",
    "model.to(device)\n",
    "print(\"âœ… Model loaded\")\n",
    "\n",
    "# Äá»‹nh nghÄ©a metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids #label tháº­t\n",
    "    preds = pred.predictions.argmax(-1) #dá»± Ä‘oÃ¡n cá»§a model\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \"\"\"\n",
    "    CÃ¡c metrics:\n",
    "    Accuracy: Tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng.\n",
    "    Precision: Trong sá»‘ dá»± Ä‘oÃ¡n lÃ  X, bao nhiÃªu % Ä‘Ãºng lÃ  X.\n",
    "    Recall: Trong sá»‘ tháº­t lÃ  X, bao nhiÃªu % Ä‘Æ°á»£c báº¯t Ä‘Ãºng.\n",
    "    F1: Trung bÃ¬nh hÃ i hÃ²a cá»§a Precision vÃ  Recall.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./intent_classifier_output',\n",
    "    num_train_epochs=5, # train 5 láº§n qua toÃ n bá»™ data\n",
    "    per_device_train_batch_size=16,#má»—i láº§n train 16 máº«u\n",
    "    per_device_eval_batch_size=32,#má»—i láº§n eval 32 máº«u\n",
    "    warmup_steps=500,# 500 bÆ°á»›c Ä‘áº§u tÄƒng learning rate tá»« tá»«\n",
    "    weight_decay=0.01,# trÃ¡nh overfit\n",
    "    logging_dir='./logs',#\n",
    "    logging_steps=50,#\n",
    "    eval_strategy=\"epoch\",#eval sau má»—i epoch\n",
    "    save_strategy=\"epoch\",#lÆ°u model sau má»—i epoch\n",
    "    load_best_model_at_end=True,#láº¥y model tá»‘t nháº¥t\n",
    "    metric_for_best_model=\"f1\",#dá»±a vÃ o F1 score\n",
    "    greater_is_better=True,#\n",
    "    save_total_limit=2,#chá»‰ giá»¯ 2 checkpoint gáº§n nháº¥t\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision náº¿u cÃ³ GPU#\n",
    "    report_to=\"none\",  # Táº¯t wandb/tensorboard#\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "# náº¿u F1 ko cáº£i thiá»‡n sau 2 epochs -> dá»«ng training trÃ¡nh overfit\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\"\"\"\n",
    "# Trainer tá»± Ä‘á»™ng táº¡o DataLoader\n",
    "# DataLoader sáº½:\n",
    "# 1. Gá»i __getitem__(0), __getitem__(1), ..., __getitem__(15)\n",
    "# 2. Gá»™p 16 máº«u thÃ nh 1 batch\n",
    "# 3. ÄÆ°a vÃ o model Ä‘á»ƒ huáº¥n luyá»‡n\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "outputs = model(\n",
    "    input_ids=batch['input_ids'],        # (16, 256)\n",
    "    attention_mask=batch['attention_mask'], # (16, 256)\n",
    "    labels=batch['labels']                # (16,)\n",
    ")\n",
    "\n",
    "outputs = {\n",
    "    'loss': tensor(0.5234),           # Äá»™ sai lá»‡ch tá»•ng thá»ƒ\n",
    "    'logits': tensor([...]),          # Dá»± Ä‘oÃ¡n thÃ´ cho 16 máº«u\n",
    "    'hidden_states': None,            # (Optional) Tráº¡ng thÃ¡i áº©n\n",
    "    'attentions': None                # (Optional) Attention weights\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nğŸ‹ï¸ Báº¯t Ä‘áº§u training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\"\"\"\n",
    "Trong quÃ¡ trÃ¬nh train \n",
    "Láº¥y 16 máº«u tá»« train_dataset.\n",
    "ÄÆ°a qua model â†’ Dá»± Ä‘oÃ¡n.\n",
    "TÃ­nh loss (sai sá»‘).\n",
    "Backpropagation (cáº­p nháº­t trá»ng sá»‘).\n",
    "Láº·p láº¡i cho Ä‘áº¿n háº¿t data â†’ 1 epoch.\n",
    "Sau má»—i epoch â†’ Eval trÃªn val_dataset.\n",
    "Láº·p láº¡i 5 epochs (hoáº·c dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n).\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nâœ… Training hoÃ n thÃ nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 6. ÄÃ¡nh giÃ¡ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trÃªn test set\n",
    "print(\"ğŸ“Š ÄÃ¡nh giÃ¡ trÃªn Test Set:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f\"\\nğŸ“ˆ Test Results:\")\n",
    "for key, value in test_results.items():\n",
    "    print(f\"   {key}: {value:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "\n",
    "print(f\"\\nğŸ“‹ Classification Report:\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(\n",
    "    labels,\n",
    "    preds,\n",
    "    target_names=[id_to_label[i] for i in range(len(id_to_label))],\n",
    "    digits=4\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª 7. Test vá»›i cÃ¢u máº«u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(text, model, tokenizer, label_mapping):\n",
    "    \"\"\"Dá»± Ä‘oÃ¡n intent cho má»™t cÃ¢u\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Top 3 predictions\n",
    "    top_probs, top_indices = torch.topk(probs[0], k=3)\n",
    "    \n",
    "    print(f\"\\nğŸ“ Text: {text}\")\n",
    "    print(f\"\\nğŸ¯ Predictions:\")\n",
    "    for prob, idx in zip(top_probs.cpu().numpy(), top_indices.cpu().numpy()):\n",
    "        intent = label_mapping[idx]\n",
    "        print(f\"   {intent}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "# Test vá»›i cÃ¡c cÃ¢u máº«u\n",
    "test_sentences = [\n",
    "    \"Máº­t Ä‘á»™ trá»“ng kinh giá»›i thÃ­ch há»£p?\",\n",
    "    \"Dá»¯ liá»‡u cáº£m biáº¿n 1 giá» trÆ°á»›c Ä‘i\",\n",
    "    \"LÃªn lá»‹ch mÃ¡y bÆ¡m má»—i 2 tiáº¿ng.\",\n",
    "    \"BÃ¡o cÃ¡o chi phÃ­ theo tá»«ng háº¡ng má»¥c.\",\n",
    "    \"Báº¡n lÃ  ai?\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ğŸ§ª TEST Vá»šI CÃC CÃ‚U MáºªU\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    predict_intent(sentence, model, tokenizer, id_to_label)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ 8. LÆ°u Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Táº¡o thÆ° má»¥c output\n",
    "output_dir = \"./intent_classifier_final\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ’¾ LÆ°u model vÃ o: {output_dir}\")\n",
    "\n",
    "# LÆ°u model vÃ  tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# LÆ°u label mapping\n",
    "label_mapping = {\n",
    "    \"label_to_id\": label_to_id,\n",
    "    \"id_to_label\": id_to_label\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, \"label_mapping.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(label_mapping, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u!\")\n",
    "print(f\"\\nğŸ“ CÃ¡c file trong {output_dir}:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    print(f\"   - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ 9. Download Model vá» mÃ¡y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NÃ©n thÆ° má»¥c model thÃ nh file zip\n",
    "print(\"ğŸ“¦ Äang nÃ©n model...\")\n",
    "shutil.make_archive(\"intent_classifier_final\", 'zip', output_dir)\n",
    "print(\"âœ… ÄÃ£ nÃ©n xong!\")\n",
    "\n",
    "# Download file zip\n",
    "print(\"\\nğŸ“¥ Download file zip vá» mÃ¡y...\")\n",
    "files.download(\"intent_classifier_final.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ… HOÃ€N THÃ€NH!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nğŸ“ HÆ°á»›ng dáº«n sá»­ dá»¥ng:\")\n",
    "print(\"   1. Giáº£i nÃ©n file intent_classifier_final.zip\")\n",
    "print(\"   2. Copy toÃ n bá»™ ná»™i dung vÃ o thÆ° má»¥c:\")\n",
    "print(\"      apps/python-ai-service/models/intent_classifier/\")\n",
    "print(\"   3. Restart Python AI Service\")\n",
    "print(\"   4. Test API Ä‘á»ƒ kiá»ƒm tra model má»›i\")\n",
    "print(\"\\nğŸ‰ ChÃºc má»«ng! Model Ä‘Ã£ Ä‘Æ°á»£c train thÃ nh cÃ´ng!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
